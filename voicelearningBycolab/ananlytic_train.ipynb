{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "793d446c-926e-4b0b-9c60-6ae52463f96b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 KsponSpeech_01 5:5 혼합 학습\n",
      "🚀 KsponSpeech_01 5:5 혼합 학습 시작\n",
      "🎯 KsponSpeech_01 5:5 혼합 데이터셋 생성\n",
      "🔍 KsponSpeech_01 파일 수집 중...\n",
      "  📁 KsponSpeech_0001 처리 중...\n",
      "    ✅ 997개 파일 매칭됨\n",
      "  📁 KsponSpeech_0002 처리 중...\n",
      "    ✅ 160개 파일 매칭됨\n",
      "  📁 KsponSpeech_0003 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0004 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0005 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0006 처리 중...\n",
      "    ✅ 37개 파일 매칭됨\n",
      "  📁 KsponSpeech_0007 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0008 처리 중...\n",
      "    ✅ 937개 파일 매칭됨\n",
      "  📁 KsponSpeech_0009 처리 중...\n",
      "    ✅ 834개 파일 매칭됨\n",
      "  📁 KsponSpeech_0010 처리 중...\n",
      "    ✅ 376개 파일 매칭됨\n",
      "  📁 KsponSpeech_0011 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0012 처리 중...\n",
      "    ✅ 961개 파일 매칭됨\n",
      "  📁 KsponSpeech_0013 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0014 처리 중...\n",
      "    ✅ 861개 파일 매칭됨\n",
      "  📁 KsponSpeech_0015 처리 중...\n",
      "    ✅ 65개 파일 매칭됨\n",
      "  📁 KsponSpeech_0016 처리 중...\n",
      "    ✅ 1개 파일 매칭됨\n",
      "  📁 KsponSpeech_0017 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0018 처리 중...\n",
      "    ✅ 484개 파일 매칭됨\n",
      "📊 총 12713개 파일 수집 완료\n",
      "📋 원본 텍스트: 6356개, 발음 텍스트: 6357개\n",
      "📊 총 12713개 - 원본: 6356개, 발음: 6357개\n",
      "🔄 오디오 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 12713/12713 [00:06<00:00, 2074.32 examples/s]\n",
      "Filter: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 12713/12713 [00:39<00:00, 321.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 최종 데이터셋 크기: 12713개\n",
      "📊 학습: 10170개, 검증: 1271개\n",
      "🤖 모델 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  30%|█████████████████████████████▌                                                                      | 3004/10170 [10:20<24:39,  4.84 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 320\u001b[0m\n\u001b[1;32m    317\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mixed_5_5_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 학습 실패\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 255\u001b[0m, in \u001b[0;36mtrain_mixed_5_5_whisper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m     labels \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    244\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m448\u001b[39m,\n\u001b[1;32m    247\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     )\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_features,\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels\n\u001b[1;32m    253\u001b[0m     }\n\u001b[0;32m--> 255\u001b[0m processed_datasets \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[1;32m    256\u001b[0m     split: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    257\u001b[0m         prepare_dataset,\n\u001b[1;32m    258\u001b[0m         remove_columns\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split, dataset \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    261\u001b[0m })\n\u001b[1;32m    263\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorSpeechSeq2SeqWithPadding(processor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# 5. 훈련 설정\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 256\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    243\u001b[0m     labels \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    244\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m448\u001b[39m,\n\u001b[1;32m    247\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     )\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_features,\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels\n\u001b[1;32m    253\u001b[0m     }\n\u001b[1;32m    255\u001b[0m processed_datasets \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[0;32m--> 256\u001b[0m     split: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split, dataset \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    261\u001b[0m })\n\u001b[1;32m    263\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorSpeechSeq2SeqWithPadding(processor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# 5. 훈련 설정\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3428\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3426\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3428\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3324\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[22], line 233\u001b[0m, in \u001b[0;36mtrain_mixed_5_5_whisper.<locals>.prepare_dataset\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"데이터 전처리\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m audio \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 233\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_features, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    239\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(input_features)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/feature_extraction_whisper.py:306\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, device, return_token_timestamps, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m input_features \u001b[38;5;241m=\u001b[39m padded_inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    303\u001b[0m extract_fbank_features \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torch_extract_fbank_features \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_np_extract_fbank_features\n\u001b[1;32m    305\u001b[0m )\n\u001b[0;32m--> 306\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_fbank_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_features[\u001b[38;5;241m0\u001b[39m], List):\n\u001b[1;32m    309\u001b[0m     padded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(feature, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m input_features]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/feature_extraction_whisper.py:139\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor._torch_extract_fbank_features\u001b[0;34m(self, waveform, device)\u001b[0m\n\u001b[1;32m    137\u001b[0m     window \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    138\u001b[0m stft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstft(waveform, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhop_length, window\u001b[38;5;241m=\u001b[39mwindow, return_complex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 139\u001b[0m magnitudes \u001b[38;5;241m=\u001b[39m \u001b[43mstft\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    141\u001b[0m mel_filters \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_filters)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 데이터 절반씩하여 (변환 텍스트, no 변환 텍스트) -> 결과 체크\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"텍스트 파일 로드 (다양한 인코딩 지원)\"\"\"\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "def get_kspon01_files():\n",
    "    \"\"\"KsponSpeech_01 모든 파일들 가져오기\"\"\"\n",
    "    print(\"🔍 KsponSpeech_01 파일 수집 중...\")\n",
    "    \n",
    "    # 경로 설정 - 수정된 경로\n",
    "    base_audio_dir = \"PreprocessData/KsponSpeech_01\"      # NPY 파일들\n",
    "    base_text_dir = \"TrainData/unzipped_Speech/KsponSpeech_01\"  # 원본 텍스트\n",
    "    base_g2p_dir = \"PreprocessData/KsponSpeech_01\"        # G2P 텍스트 (여기에 있었음!)\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    # KsponSpeech_0001 ~ KsponSpeech_0018 폴더들 처리\n",
    "    for i in range(1, 19):\n",
    "        if i < 10:\n",
    "            folder_name = f\"KsponSpeech_000{i}\"\n",
    "        else:\n",
    "            folder_name = f\"KsponSpeech_00{i}\"\n",
    "        \n",
    "        print(f\"  📁 {folder_name} 처리 중...\")\n",
    "        \n",
    "        # 폴더 경로들\n",
    "        audio_folder = os.path.join(base_audio_dir, folder_name)\n",
    "        original_folder = os.path.join(base_text_dir, folder_name)\n",
    "        g2p_folder = os.path.join(base_g2p_dir, f\"{folder_name}_g2p\")  # PreprocessData에서 찾기\n",
    "        \n",
    "        # 폴더 존재 확인\n",
    "        if not all([os.path.exists(audio_folder), os.path.exists(original_folder), os.path.exists(g2p_folder)]):\n",
    "            print(f\"    ⚠️ 폴더 없음: 오디오={os.path.exists(audio_folder)}, 원본={os.path.exists(original_folder)}, G2P={os.path.exists(g2p_folder)}\")\n",
    "            continue\n",
    "        \n",
    "        # NPY 파일들 찾기\n",
    "        npy_files = glob.glob(os.path.join(audio_folder, \"*_combined_features.npy\"))\n",
    "        \n",
    "        folder_files = []\n",
    "        for npy_file in npy_files:\n",
    "            # 파일명에서 기본 이름 추출\n",
    "            base_name = os.path.basename(npy_file).replace('_combined_features.npy', '')\n",
    "            \n",
    "            # 대응되는 텍스트 파일들\n",
    "            original_txt = os.path.join(original_folder, f\"{base_name}.txt\")\n",
    "            g2p_txt = os.path.join(g2p_folder, f\"{base_name}.txt\")\n",
    "            \n",
    "            # 모든 파일이 존재하는지 확인\n",
    "            if all([os.path.exists(npy_file), os.path.exists(original_txt), os.path.exists(g2p_txt)]):\n",
    "                folder_files.append({\n",
    "                    'audio': npy_file,\n",
    "                    'original_txt': original_txt,\n",
    "                    'g2p_txt': g2p_txt,\n",
    "                    'base_name': base_name,\n",
    "                    'folder': folder_name\n",
    "                })\n",
    "        \n",
    "        print(f\"    ✅ {len(folder_files)}개 파일 매칭됨\")\n",
    "        all_files.extend(folder_files)\n",
    "    \n",
    "    print(f\"📊 총 {len(all_files)}개 파일 수집 완료\")\n",
    "    return all_files\n",
    "\n",
    "def create_5_5_mixed_dataset():\n",
    "    \"\"\"5:5 비율로 혼합 데이터셋 생성\"\"\"\n",
    "    print(\"🎯 KsponSpeech_01 5:5 혼합 데이터셋 생성\")\n",
    "    \n",
    "    # 모든 파일 수집\n",
    "    all_files = get_kspon01_files()\n",
    "    \n",
    "    if len(all_files) == 0:\n",
    "        print(\"❌ 파일을 찾을 수 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 파일들을 셔플링\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    # 5:5로 분할\n",
    "    split_point = len(all_files) // 2\n",
    "    original_files = all_files[:split_point]\n",
    "    g2p_files = all_files[split_point:]\n",
    "    \n",
    "    print(f\"📋 원본 텍스트: {len(original_files)}개, 발음 텍스트: {len(g2p_files)}개\")\n",
    "    \n",
    "    # 데이터셋 구성\n",
    "    data = {\"audio\": [], \"text\": [], \"text_type\": []}\n",
    "    \n",
    "    # 원본 텍스트 데이터 추가\n",
    "    for file_info in original_files:\n",
    "        try:\n",
    "            original_text = load_text(file_info['original_txt'])\n",
    "            if original_text != \"텍스트 로드 실패\":\n",
    "                data[\"audio\"].append(file_info['audio'])\n",
    "                data[\"text\"].append(original_text)\n",
    "                data[\"text_type\"].append(\"original\")\n",
    "        except Exception as e:\n",
    "            print(f\"원본 텍스트 로드 실패: {e}\")\n",
    "    \n",
    "    # 발음 텍스트 데이터 추가\n",
    "    for file_info in g2p_files:\n",
    "        try:\n",
    "            g2p_text = load_text(file_info['g2p_txt'])\n",
    "            if g2p_text != \"텍스트 로드 실패\":\n",
    "                data[\"audio\"].append(file_info['audio'])\n",
    "                data[\"text\"].append(g2p_text)\n",
    "                data[\"text_type\"].append(\"g2p\")\n",
    "        except Exception as e:\n",
    "            print(f\"발음 텍스트 로드 실패: {e}\")\n",
    "    \n",
    "    # 최종 통계\n",
    "    total_count = len(data['audio'])\n",
    "    original_count = data['text_type'].count('original')\n",
    "    g2p_count = data['text_type'].count('g2p')\n",
    "    \n",
    "    print(f\"📊 총 {total_count}개 - 원본: {original_count}개, 발음: {g2p_count}개\")\n",
    "    \n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def map_to_array(batch):\n",
    "    \"\"\"NPY 파일을 오디오 배열로 변환\"\"\"\n",
    "    arrays = []\n",
    "    rates = []\n",
    "\n",
    "    for audio_path in batch[\"audio\"]:\n",
    "        try:\n",
    "            audio_array = np.load(audio_path)\n",
    "            \n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(16000)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"오디오 로드 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "    return batch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def train_mixed_5_5_whisper():\n",
    "    \"\"\"5:5 혼합 데이터로 Whisper 모델 학습\"\"\"\n",
    "    output_dir = \"whisper_mixed_5_5_finetuned\"\n",
    "    \n",
    "    print(\"🚀 KsponSpeech_01 5:5 혼합 학습 시작\")\n",
    "    \n",
    "    # 1. 혼합 데이터셋 생성\n",
    "    dataset = create_5_5_mixed_dataset()\n",
    "    \n",
    "    if dataset is None or len(dataset) == 0:\n",
    "        print(\"❌ 데이터셋 생성 실패\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. 오디오 데이터 전처리\n",
    "    print(\"🔄 오디오 데이터 전처리 중...\")\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"audio\"]) > 0)\n",
    "    \n",
    "    print(f\"✅ 최종 데이터셋 크기: {len(dataset)}개\")\n",
    "\n",
    "    # 3. 데이터셋 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    print(f\"📊 학습: {len(datasets['train'])}개, 검증: {len(datasets['validation'])}개\")\n",
    "\n",
    "    # 4. 모델 및 프로세서 로드\n",
    "    print(\"🤖 모델 로드 중...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        \"\"\"데이터 전처리\"\"\"\n",
    "        audio = batch[\"audio\"]\n",
    "        \n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        \n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "        \n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=448,\n",
    "            padding=False\n",
    "        ).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    # 5. 훈련 설정\n",
    "    device_capability = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    supports_bf16 = device_capability >= (8, 0)\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=1e-6,\n",
    "        warmup_steps=300,\n",
    "        max_steps=4000,\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=supports_bf16,\n",
    "        fp16=False if supports_bf16 else True,\n",
    "        dataloader_pin_memory=False,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=400,\n",
    "        save_steps=400,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=3,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        eval_dataset=processed_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"🚀 학습 시작...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"💾 모델 저장 중...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 KsponSpeech_01 5:5 혼합 학습\")\n",
    "    \n",
    "    # 시드 설정\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 학습 실행\n",
    "    model, processor = train_mixed_5_5_whisper()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"❌ 학습 실패\")\n",
    "    else:\n",
    "        print(\"✅ 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "034ff3dd-697b-406c-ba58-a838b9030146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData G2P 폴더 상세 확인:\n",
      "G2P 폴더 18개:\n",
      "  KsponSpeech_0001_g2p: 1000개 파일\n",
      "  KsponSpeech_0002_g2p: 0개 파일\n",
      "  KsponSpeech_0003_g2p: 0개 파일\n",
      "  KsponSpeech_0004_g2p: 0개 파일\n",
      "  KsponSpeech_0005_g2p: 0개 파일\n",
      "  KsponSpeech_0006_g2p: 0개 파일\n",
      "  KsponSpeech_0007_g2p: 0개 파일\n",
      "  KsponSpeech_0008_g2p: 0개 파일\n",
      "  KsponSpeech_0009_g2p: 0개 파일\n",
      "  KsponSpeech_0010_g2p: 0개 파일\n",
      "  KsponSpeech_0011_g2p: 0개 파일\n",
      "  KsponSpeech_0012_g2p: 0개 파일\n",
      "  KsponSpeech_0013_g2p: 0개 파일\n",
      "  KsponSpeech_0014_g2p: 0개 파일\n",
      "  KsponSpeech_0015_g2p: 0개 파일\n",
      "  KsponSpeech_0016_g2p: 0개 파일\n",
      "  KsponSpeech_0017_g2p: 0개 파일\n",
      "  KsponSpeech_0018_g2p: 0개 파일\n",
      "\n",
      "총 G2P 텍스트 파일: 1000개\n",
      "\n",
      "매칭 가능한 데이터 확인:\n",
      "  KsponSpeech_0001: 오디오=997, 원본=1000, G2P=1000 → 매칭=997\n",
      "\n",
      "실제 매칭 가능한 전체 데이터: 997개\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0c957f-cd22-4a4f-a223-a7aa70d9c646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 100% 발음 텍스트 Whisper 학습 (구조 보존 + CER/WER)\n",
      "🚀 100% 발음 텍스트 Whisper 학습 시작\n",
      "🎯 목표: 구조 보존 + 발음 패턴 학습\n",
      "🎯 100% 발음 텍스트 데이터셋 생성 (구조 보존 강화)\n",
      "🔍 KsponSpeech_01 파일 수집 중...\n",
      "  📁 KsponSpeech_0001 처리 중...\n",
      "    ✅ 997개 파일 매칭됨\n",
      "  📁 KsponSpeech_0002 처리 중...\n",
      "    ✅ 160개 파일 매칭됨\n",
      "  📁 KsponSpeech_0003 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0004 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0005 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0006 처리 중...\n",
      "    ✅ 37개 파일 매칭됨\n",
      "  📁 KsponSpeech_0007 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0008 처리 중...\n",
      "    ✅ 937개 파일 매칭됨\n",
      "  📁 KsponSpeech_0009 처리 중...\n",
      "    ✅ 834개 파일 매칭됨\n",
      "  📁 KsponSpeech_0010 처리 중...\n",
      "    ✅ 376개 파일 매칭됨\n",
      "  📁 KsponSpeech_0011 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0012 처리 중...\n",
      "    ✅ 961개 파일 매칭됨\n",
      "  📁 KsponSpeech_0013 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0014 처리 중...\n",
      "    ✅ 861개 파일 매칭됨\n",
      "  📁 KsponSpeech_0015 처리 중...\n",
      "    ✅ 65개 파일 매칭됨\n",
      "  📁 KsponSpeech_0016 처리 중...\n",
      "    ✅ 1개 파일 매칭됨\n",
      "  📁 KsponSpeech_0017 처리 중...\n",
      "    ✅ 1000개 파일 매칭됨\n",
      "  📁 KsponSpeech_0018 처리 중...\n",
      "    ✅ 484개 파일 매칭됨\n",
      "📊 총 12713개 파일 수집 완료\n",
      "📊 총 12713개 발음 텍스트 데이터 생성\n",
      "🔄 오디오 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 12713/12713 [00:05<00:00, 2127.90 examples/s]\n",
      "Filter:  16%|███████████████                                                                                 | 2000/12713 [00:06<00:35, 301.80 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 485\u001b[0m\n\u001b[1;32m    482\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_phonetic_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ 학습 실패\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 282\u001b[0m, in \u001b[0;36mtrain_phonetic_whisper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 오디오 데이터 전처리 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(map_to_array, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m--> 282\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ 최종 데이터셋 크기: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m개\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# 3. 데이터셋 분할\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3670\u001b[0m, in \u001b[0;36mDataset.filter\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 3670\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_indices_from_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3694\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3697\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3698\u001b[0m new_dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3454\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3455\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3456\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3458\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3462\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3466\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3324\u001b[0m     }\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:6301\u001b[0m, in \u001b[0;36mget_indices_from_mask_function\u001b[0;34m(function, batched, with_indices, with_rank, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[0m\n\u001b[1;32m   6298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6299\u001b[0m     \u001b[38;5;66;03m# inputs only contains a batch of examples\u001b[39;00m\n\u001b[1;32m   6300\u001b[0m     batch: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 6301\u001b[0m     num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   6302\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[1;32m   6303\u001b[0m         example \u001b[38;5;241m=\u001b[39m {key: batch[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:279\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    277\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m--> 279\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:382\u001b[0m, in \u001b[0;36mLazyBatch.format\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:148\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/table.pxi:1335\u001b[0m, in \u001b[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/array.pxi:1650\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/scalar.pxi:793\u001b[0m, in \u001b[0;36mpyarrow.lib.StructScalar.as_py\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.8/_collections_abc.py:720\u001b[0m, in \u001b[0;36mKeysView.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "실시간 CER/WER 300스텝마다 출력\n",
    "최종 테스트 결과 자동 저장\n",
    "PT 파일 재사용을 위한 저장\n",
    "구조 보존 강화된 생성 설정\n",
    "'''\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from jiwer import wer, cer\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"텍스트 파일 로드 (다양한 인코딩 지원)\"\"\"\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "def preprocess_g2p_text(text):\n",
    "    \"\"\"구조 보존을 위한 G2P 텍스트 전처리\"\"\"\n",
    "    # 공백 정리\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # 문장 경계 명시화 (선택적)\n",
    "    # text = re.sub(r'([.!?])', r'\\1 /', text)\n",
    "    \n",
    "    # 반복되는 특수문자 정리\n",
    "    text = re.sub(r'([+*/])\\1+', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def get_kspon01_files():\n",
    "    \"\"\"KsponSpeech_01 모든 파일들 가져오기\"\"\"\n",
    "    print(\"🔍 KsponSpeech_01 파일 수집 중...\")\n",
    "    \n",
    "    base_audio_dir = \"PreprocessData/KsponSpeech_01\"\n",
    "    base_text_dir = \"TrainData/unzipped_Speech/KsponSpeech_01\"\n",
    "    base_g2p_dir = \"PreprocessData/KsponSpeech_01\"\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    for i in range(1, 19):\n",
    "        folder_name = f\"KsponSpeech_000{i}\" if i < 10 else f\"KsponSpeech_00{i}\"\n",
    "        print(f\"  📁 {folder_name} 처리 중...\")\n",
    "        \n",
    "        audio_folder = os.path.join(base_audio_dir, folder_name)\n",
    "        original_folder = os.path.join(base_text_dir, folder_name)\n",
    "        g2p_folder = os.path.join(base_g2p_dir, f\"{folder_name}_g2p\")\n",
    "        \n",
    "        if not all([os.path.exists(audio_folder), os.path.exists(original_folder), os.path.exists(g2p_folder)]):\n",
    "            print(f\"    ⚠️ 폴더 없음\")\n",
    "            continue\n",
    "        \n",
    "        npy_files = glob.glob(os.path.join(audio_folder, \"*_combined_features.npy\"))\n",
    "        \n",
    "        folder_files = []\n",
    "        for npy_file in npy_files:\n",
    "            base_name = os.path.basename(npy_file).replace('_combined_features.npy', '')\n",
    "            original_txt = os.path.join(original_folder, f\"{base_name}.txt\")\n",
    "            g2p_txt = os.path.join(g2p_folder, f\"{base_name}.txt\")\n",
    "            \n",
    "            if all([os.path.exists(npy_file), os.path.exists(original_txt), os.path.exists(g2p_txt)]):\n",
    "                folder_files.append({\n",
    "                    'audio': npy_file,\n",
    "                    'original_txt': original_txt,\n",
    "                    'g2p_txt': g2p_txt,\n",
    "                    'base_name': base_name,\n",
    "                    'folder': folder_name\n",
    "                })\n",
    "        \n",
    "        print(f\"    ✅ {len(folder_files)}개 파일 매칭됨\")\n",
    "        all_files.extend(folder_files)\n",
    "    \n",
    "    print(f\"📊 총 {len(all_files)}개 파일 수집 완료\")\n",
    "    return all_files\n",
    "\n",
    "def create_phonetic_only_dataset():\n",
    "    \"\"\"100% 발음 텍스트 데이터셋 생성\"\"\"\n",
    "    print(\"🎯 100% 발음 텍스트 데이터셋 생성 (구조 보존 강화)\")\n",
    "    \n",
    "    all_files = get_kspon01_files()\n",
    "    if len(all_files) == 0:\n",
    "        return None\n",
    "    \n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    data = {\"audio\": [], \"text\": [], \"original_text\": []}\n",
    "    \n",
    "    # 100% G2P 텍스트만 사용\n",
    "    for file_info in all_files:\n",
    "        try:\n",
    "            g2p_text = load_text(file_info['g2p_txt'])\n",
    "            original_text = load_text(file_info['original_txt'])\n",
    "            \n",
    "            if g2p_text != \"텍스트 로드 실패\" and original_text != \"텍스트 로드 실패\":\n",
    "                # G2P 텍스트 전처리 (구조 보존)\n",
    "                processed_g2p = preprocess_g2p_text(g2p_text)\n",
    "                \n",
    "                data[\"audio\"].append(file_info['audio'])\n",
    "                data[\"text\"].append(processed_g2p)  # 학습용 (발음 텍스트)\n",
    "                data[\"original_text\"].append(original_text)  # 평가용 (원본 텍스트)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"텍스트 로드 실패: {e}\")\n",
    "    \n",
    "    total_count = len(data['audio'])\n",
    "    print(f\"📊 총 {total_count}개 발음 텍스트 데이터 생성\")\n",
    "    \n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def map_to_array(batch):\n",
    "    \"\"\"NPY 파일을 오디오 배열로 변환\"\"\"\n",
    "    arrays = []\n",
    "    rates = []\n",
    "\n",
    "    for audio_path in batch[\"audio\"]:\n",
    "        try:\n",
    "            audio_array = np.load(audio_path)\n",
    "            \n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(16000)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"오디오 로드 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "    return batch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def compute_metrics(eval_pred, processor, original_texts):\n",
    "    \"\"\"CER/WER 계산\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # 라벨에서 -100을 제거\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    # 디코딩\n",
    "    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # CER/WER 계산 (오류 처리 포함)\n",
    "    try:\n",
    "        # 빈 예측 처리\n",
    "        filtered_preds = []\n",
    "        filtered_labels = []\n",
    "        \n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            if pred.strip() and label.strip():\n",
    "                filtered_preds.append(pred.strip())\n",
    "                filtered_labels.append(label.strip())\n",
    "        \n",
    "        if len(filtered_preds) > 0:\n",
    "            cer_score = cer(filtered_labels, filtered_preds) * 100\n",
    "            wer_score = wer(filtered_labels, filtered_preds) * 100\n",
    "        else:\n",
    "            cer_score = 100.0\n",
    "            wer_score = 100.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"메트릭 계산 오류: {e}\")\n",
    "        cer_score = 100.0\n",
    "        wer_score = 100.0\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer_score,\n",
    "        \"wer\": wer_score\n",
    "    }\n",
    "\n",
    "class PhoneticWhisperTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"CER/WER 평가가 포함된 커스텀 트레이너\"\"\"\n",
    "    \n",
    "    def __init__(self, original_texts=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.original_texts = original_texts\n",
    "        \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"평가 시 CER/WER 계산\"\"\"\n",
    "        print(\"📊 CER/WER 평가 중...\")\n",
    "        \n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        \n",
    "        # 기본 평가\n",
    "        output = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # CER/WER 계산을 위한 샘플 예측\n",
    "        try:\n",
    "            eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "            sample_batch = next(iter(eval_dataloader))\n",
    "            \n",
    "            # GPU로 이동\n",
    "            if torch.cuda.is_available():\n",
    "                sample_batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v \n",
    "                              for k, v in sample_batch.items()}\n",
    "            \n",
    "            # 예측 생성\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    sample_batch[\"input_features\"],\n",
    "                    max_length=448,\n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.5,  # 구조 보존 강화\n",
    "                    no_repeat_ngram_size=4,\n",
    "                    length_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "            \n",
    "            # 메트릭 계산\n",
    "            metrics = compute_metrics(\n",
    "                (generated_ids.cpu().numpy(), sample_batch[\"labels\"].cpu().numpy()),\n",
    "                self.tokenizer,\n",
    "                self.original_texts\n",
    "            )\n",
    "            \n",
    "            # 결과 추가\n",
    "            output.update({f\"{metric_key_prefix}_{k}\": v for k, v in metrics.items()})\n",
    "            \n",
    "            print(f\"📈 CER: {metrics['cer']:.2f}%, WER: {metrics['wer']:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ CER/WER 계산 오류: {e}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train_phonetic_whisper():\n",
    "    \"\"\"100% 발음 텍스트로 Whisper 모델 학습\"\"\"\n",
    "    output_dir = \"whisper_phonetic_only_finetuned\"\n",
    "    pt_save_path = \"whisper_phonetic_model.pt\"\n",
    "    \n",
    "    print(\"🚀 100% 발음 텍스트 Whisper 학습 시작\")\n",
    "    print(\"🎯 목표: 구조 보존 + 발음 패턴 학습\")\n",
    "    \n",
    "    # 1. 발음 전용 데이터셋 생성\n",
    "    dataset = create_phonetic_only_dataset()\n",
    "    \n",
    "    if dataset is None or len(dataset) == 0:\n",
    "        print(\"❌ 데이터셋 생성 실패\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. 오디오 데이터 전처리\n",
    "    print(\"🔄 오디오 데이터 전처리 중...\")\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"audio\"]) > 0)\n",
    "    \n",
    "    print(f\"✅ 최종 데이터셋 크기: {len(dataset)}개\")\n",
    "\n",
    "    # 3. 데이터셋 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    print(f\"📊 학습: {len(datasets['train'])}개, 검증: {len(datasets['validation'])}개, 테스트: {len(datasets['test'])}개\")\n",
    "\n",
    "    # 4. 모델 및 프로세서 로드\n",
    "    print(\"🤖 모델 로드 중...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        \"\"\"데이터 전처리\"\"\"\n",
    "        audio = batch[\"audio\"]\n",
    "        \n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        \n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "        \n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # 발음 텍스트로 라벨 생성\n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=448,\n",
    "            padding=False\n",
    "        ).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    # 5. 훈련 설정 (구조 보존 강화)\n",
    "    device_capability = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    supports_bf16 = device_capability >= (8, 0)\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-7,              # 매우 낮은 학습률 (구조 보존)\n",
    "        warmup_steps=500,                # 긴 워밍업\n",
    "        max_steps=3000,                  # 적절한 스텝 수\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=supports_bf16,\n",
    "        fp16=False if supports_bf16 else True,\n",
    "        dataloader_pin_memory=False,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=300,                  # 300스텝마다 평가 (CER/WER 계산)\n",
    "        save_steps=300,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_cer\", # CER 기준으로 최적 모델 선택\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=3,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False,\n",
    "        # 구조 보존을 위한 생성 설정\n",
    "        generation_max_length=448,\n",
    "        generation_num_beams=2,\n",
    "        predict_with_generate=True,\n",
    "    )\n",
    "\n",
    "    # 6. 커스텀 트레이너 (CER/WER 포함)\n",
    "    trainer = PhoneticWhisperTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        eval_dataset=processed_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        original_texts=[item[\"original_text\"] for item in datasets[\"validation\"]],\n",
    "    )\n",
    "\n",
    "    print(\"🚀 발음 전용 학습 시작...\")\n",
    "    print(\"📊 평가 주기: 300스텝마다 CER/WER 계산\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"💾 모델 저장 중...\")\n",
    "    \n",
    "    # 1. Hugging Face 형식으로 저장\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    # 2. PT 파일로 저장 (재사용 용이)\n",
    "    print(f\"💾 PT 파일 저장 중: {pt_save_path}\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'processor_config': processor.to_dict() if hasattr(processor, 'to_dict') else None,\n",
    "        'model_config': model.config.to_dict(),\n",
    "        'training_args': training_args.to_dict(),\n",
    "        'vocab_size': len(processor.tokenizer),\n",
    "        'training_type': 'phonetic_only_structure_preserved'\n",
    "    }, pt_save_path)\n",
    "    \n",
    "    # 3. 최종 테스트 (CER/WER)\n",
    "    print(\"🧪 최종 테스트 중...\")\n",
    "    test_results = trainer.evaluate(processed_datasets[\"test\"], metric_key_prefix=\"test\")\n",
    "    \n",
    "    # 4. 결과 저장\n",
    "    final_stats = {\n",
    "        \"training_type\": \"phonetic_only_structure_preserved\",\n",
    "        \"total_samples\": len(dataset),\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"max_steps\": training_args.max_steps,\n",
    "        \"final_test_cer\": test_results.get(\"test_cer\", \"N/A\"),\n",
    "        \"final_test_wer\": test_results.get(\"test_wer\", \"N/A\"),\n",
    "        \"model_path\": output_dir,\n",
    "        \"pt_path\": pt_save_path\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"phonetic_training_results.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(final_stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"✅ 발음 전용 학습 완료!\")\n",
    "    print(f\"📊 최종 결과:\")\n",
    "    print(f\"   CER: {final_stats['final_test_cer']}\")\n",
    "    print(f\"   WER: {final_stats['final_test_wer']}\")\n",
    "    print(f\"   HF 모델: {output_dir}\")\n",
    "    print(f\"   PT 파일: {pt_save_path}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def load_phonetic_model(pt_path=\"whisper_phonetic_model.pt\"):\n",
    "    \"\"\"저장된 PT 파일에서 모델 로드\"\"\"\n",
    "    print(f\"📁 PT 파일에서 모델 로드: {pt_path}\")\n",
    "    \n",
    "    if not os.path.exists(pt_path):\n",
    "        print(f\"❌ PT 파일이 없습니다: {pt_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # PT 파일 로드\n",
    "    checkpoint = torch.load(pt_path, map_location='cpu')\n",
    "    \n",
    "    # 모델 초기화\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "    \n",
    "    # 상태 로드\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    print(\"✅ PT 파일에서 모델 로드 완료!\")\n",
    "    print(f\"   훈련 타입: {checkpoint.get('training_type', 'unknown')}\")\n",
    "    print(f\"   학습률: {checkpoint.get('training_args', {}).get('learning_rate', 'unknown')}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 100% 발음 텍스트 Whisper 학습 (구조 보존 + CER/WER)\")\n",
    "    \n",
    "    # 시드 설정\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 학습 실행\n",
    "    model, processor = train_phonetic_whisper()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"❌ 학습 실패\")\n",
    "    else:\n",
    "        print(\"✅ 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13d43d-b30f-4aa2-a0b4-a4c328465f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
