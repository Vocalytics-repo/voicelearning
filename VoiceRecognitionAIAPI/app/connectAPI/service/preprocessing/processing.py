import numpy as np
from typing import Dict, Any

# =============================================================================
# íŠ¹ì„± ê°ì§€ ë° ë¶„ì„ í•¨ìˆ˜ë“¤
# =============================================================================

def detect_feature_type(features: np.ndarray) -> Dict[str, Any]:
    """ì…ë ¥ íŠ¹ì„±ì˜ íƒ€ì…ê³¼ êµ¬ì¡°ë¥¼ ìë™ ê°ì§€"""
    try:
        n_features, n_frames = features.shape

        feature_info = {
            'shape': features.shape,
            'type': 'unknown',
            'components': {},
            'confidence': 0.0
        }

        # ì°¨ì› ê¸°ë°˜ íƒ€ì… ê°ì§€
        if n_features == 13:
            feature_info['type'] = 'mfcc_only'
            feature_info['components']['mfcc'] = (0, 13)
            feature_info['confidence'] = 0.9

        elif n_features == 26:
            feature_info['type'] = 'mfcc_delta'
            feature_info['components']['mfcc'] = (0, 13)
            feature_info['components']['delta'] = (13, 26)
            feature_info['confidence'] = 0.85

        elif n_features == 39:
            feature_info['type'] = 'mfcc_full'
            feature_info['components']['mfcc'] = (0, 13)
            feature_info['components']['delta'] = (13, 26)
            feature_info['components']['delta2'] = (26, 39)
            feature_info['confidence'] = 0.95

        # í†µê³„ì  ê²€ì¦ìœ¼ë¡œ ì‹ ë¢°ë„ ì¡°ì •
        confidence_adjustment = _validate_feature_statistics(features, feature_info['type'])
        feature_info['confidence'] *= confidence_adjustment

        print(f"ğŸ” íŠ¹ì„± íƒ€ì… ê°ì§€: {feature_info['type']} (ì‹ ë¢°ë„: {feature_info['confidence']:.2f})")
        return feature_info

    except Exception as e:
        print(f"âŒ íŠ¹ì„± íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
        return {
            'shape': features.shape,
            'type': 'unknown',
            'components': {},
            'confidence': 0.0
        }


def _validate_feature_statistics(features: np.ndarray, detected_type: str) -> float:
    """í†µê³„ì  íŠ¹ì„± ê²€ì¦"""
    try:
        confidence = 1.0

        # MFCC íŠ¹ì„± ê²€ì¦
        if detected_type in ['mfcc_only', 'mfcc_delta', 'mfcc_full']:
            mfcc_part = features[:13, :]

            # MFCC ì²« ë²ˆì§¸ ê³„ìˆ˜ (ì—ë„ˆì§€)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ë¥¸ ê³„ìˆ˜ë³´ë‹¤ í° ê°’
            if np.mean(np.abs(mfcc_part[0, :])) > np.mean(np.abs(mfcc_part[1:, :])):
                confidence *= 1.1
            else:
                confidence *= 0.9

            # MFCC ê³„ìˆ˜ë“¤ì˜ ë¶„ì‚° íŒ¨í„´ ê²€ì¦
            variances = np.var(mfcc_part, axis=1)
            if np.argmax(variances) == 0:  # ì²« ë²ˆì§¸ ê³„ìˆ˜ê°€ ê°€ì¥ í° ë¶„ì‚°
                confidence *= 1.05

        # Delta íŠ¹ì„± ê²€ì¦
        if detected_type in ['mfcc_delta', 'mfcc_full']:
            delta_part = features[13:26, :]

            # DeltaëŠ” ì¼ë°˜ì ìœ¼ë¡œ MFCCë³´ë‹¤ ì‘ì€ ê°’
            mfcc_mean = np.mean(np.abs(features[:13, :]))
            delta_mean = np.mean(np.abs(delta_part))

            if delta_mean < mfcc_mean:
                confidence *= 1.05
            else:
                confidence *= 0.95

        # Delta2 íŠ¹ì„± ê²€ì¦
        if detected_type == 'mfcc_full':
            delta2_part = features[26:39, :]
            delta_part = features[13:26, :]

            # Delta2ëŠ” ì¼ë°˜ì ìœ¼ë¡œ Deltaë³´ë‹¤ ì‘ì€ ê°’
            delta_mean = np.mean(np.abs(delta_part))
            delta2_mean = np.mean(np.abs(delta2_part))

            if delta2_mean < delta_mean:
                confidence *= 1.05
            else:
                confidence *= 0.95

        return min(confidence, 1.0)

    except Exception as e:
        print(f"âš ï¸ í†µê³„ì  ê²€ì¦ ì‹¤íŒ¨: {e}")
        return 0.8


async def adaptive_feature_to_audio(features: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """ì ì‘í˜• íŠ¹ì„±-ì˜¤ë””ì˜¤ ë³€í™˜"""
    try:
        # íŠ¹ì„± íƒ€ì… ìë™ ê°ì§€
        feature_info = detect_feature_type(features)

        print(f"ğŸ”„ ì ì‘í˜• ë³€í™˜ ì‹œì‘: {feature_info['type']}")

        # ì‹ ë¢°ë„ ê¸°ë°˜ ì²˜ë¦¬ ë°©ë²• ì„ íƒ
        if feature_info['confidence'] > 0.8:
            # ë†’ì€ ì‹ ë¢°ë„: íŠ¹ì„±ì— ë§ëŠ” ìµœì  ë°©ë²• ì‚¬ìš©
            if feature_info['type'] == 'mfcc_only':
                return await _process_mfcc_only(features, sample_rate)
            elif feature_info['type'] == 'mfcc_delta':
                return await _process_mfcc_delta(features, sample_rate)
            elif feature_info['type'] == 'mfcc_full':
                return await _process_mfcc_full(features, sample_rate)

        # ë‚®ì€ ì‹ ë¢°ë„ ë˜ëŠ” ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…: ì•ˆì „í•œ ë°©ë²• ì‚¬ìš©
        print("âš ï¸ ë‚®ì€ ì‹ ë¢°ë„ - ì•ˆì „í•œ ë³€í™˜ ë°©ë²• ì‚¬ìš©")
        return await _safe_feature_conversion(features, sample_rate)

    except Exception as e:
        print(f"âŒ ì ì‘í˜• ë³€í™˜ ì‹¤íŒ¨: {e}")
        return np.random.randn(sample_rate) * 0.05


async def _process_mfcc_only(features: np.ndarray, sample_rate: int) -> np.ndarray:
    """ìˆœìˆ˜ MFCC ìµœì  ì²˜ë¦¬"""
    print("ğŸ“Š ìˆœìˆ˜ MFCC ìµœì  ì²˜ë¦¬")
    return _mfcc_basic_reconstruction(features, sample_rate)


async def _process_mfcc_delta(features: np.ndarray, sample_rate: int) -> np.ndarray:
    """MFCC + Delta ìµœì  ì²˜ë¦¬"""
    print("ğŸ“Š MFCC + Delta ìµœì  ì²˜ë¦¬")
    mfcc = features[:13, :]
    delta = features[13:26, :]
    return _mfcc_delta_reconstruction(mfcc, delta, sample_rate)


async def _process_mfcc_full(features: np.ndarray, sample_rate: int) -> np.ndarray:
    """MFCC + Delta + Delta2 ìµœì  ì²˜ë¦¬"""
    print("ğŸ“Š MFCC + Delta + Delta2 ìµœì  ì²˜ë¦¬")
    mfcc = features[:13, :]
    delta = features[13:26, :]
    delta2 = features[26:39, :]
    return _mfcc_full_reconstruction(mfcc, delta, delta2, sample_rate)


async def _safe_feature_conversion(features: np.ndarray, sample_rate: int) -> np.ndarray:
    """ì•ˆì „í•œ íŠ¹ì„± ë³€í™˜ (íƒ€ì… ë¶ˆëª… ì‹œ)"""
    try:
        print("ğŸ›¡ï¸ ì•ˆì „í•œ íŠ¹ì„± ë³€í™˜")

        # ê°€ì¥ ì•ˆì „í•œ ë°©ë²•: ì²« 13ê°œ íŠ¹ì„±ë§Œ MFCCë¡œ ê°„ì£¼
        safe_features = features[:min(13, features.shape[0]), :]

        # ê¸°ë³¸ ë³µì› ì‹œë„
        try:
            return _mfcc_basic_reconstruction(safe_features, sample_rate)
        except Exception:
            # ë³µì› ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ë°©ë²•
            return _manual_mfcc_reconstruction(safe_features, sample_rate)

    except Exception as e:
        print(f"âŒ ì•ˆì „í•œ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return np.random.randn(sample_rate) * 0.05


# =============================================================================
# ê³ ê¸‰ ì‹ í˜¸ ì²˜ë¦¬ í•¨ìˆ˜ë“¤
# =============================================================================

def enhance_audio_quality(audio: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """ë³µì›ëœ ì˜¤ë””ì˜¤ í’ˆì§ˆ í–¥ìƒ"""
    try:
        print("ğŸ”§ ì˜¤ë””ì˜¤ í’ˆì§ˆ í–¥ìƒ ì‹œì‘")

        enhanced = np.copy(audio)

        # 1. DC ì„±ë¶„ ì œê±°
        enhanced = enhanced - np.mean(enhanced)

        # 2. ë¶€ë“œëŸ¬ìš´ ìœˆë„ìš° ì ìš© (í´ë¦­ ë…¸ì´ì¦ˆ ì œê±°)
        window_size = min(1024, len(enhanced) // 10)
        if window_size > 0:
            window = np.hanning(window_size)
            enhanced[:window_size // 2] *= window[:window_size // 2]
            enhanced[-window_size // 2:] *= window[-window_size // 2:]

        # 3. ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ì œê±°
        try:
            # ê°„ë‹¨í•œ ì €ì—­í†µê³¼ í•„í„°
            from scipy import signal
            nyquist = sample_rate / 2
            cutoff = min(8000, nyquist * 0.9)
            b, a = signal.butter(4, cutoff / nyquist, btype='low')
            enhanced = signal.filtfilt(b, a, enhanced)
        except ImportError:
            # scipy ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì´ë™í‰ê· 
            kernel_size = 3
            kernel = np.ones(kernel_size) / kernel_size
            enhanced = np.convolve(enhanced, kernel, mode='same')

        # 4. ë™ì  ë²”ìœ„ ì •ê·œí™”
        if np.max(np.abs(enhanced)) > 0:
            enhanced = enhanced / np.max(np.abs(enhanced)) * 0.8

        print("âœ… ì˜¤ë””ì˜¤ í’ˆì§ˆ í–¥ìƒ ì™„ë£Œ")
        return enhanced

    except Exception as e:
        print(f"âš ï¸ í’ˆì§ˆ í–¥ìƒ ì‹¤íŒ¨: {e}")
        return audio


def validate_reconstructed_audio(audio: np.ndarray, sample_rate: int = 16000) -> Dict[str, Any]:
    """ë³µì›ëœ ì˜¤ë””ì˜¤ í’ˆì§ˆ ê²€ì¦"""
    try:
        metrics = {
            'length_seconds': len(audio) / sample_rate,
            'max_amplitude': np.max(np.abs(audio)),
            'rms': np.sqrt(np.mean(audio ** 2)),
            'zero_crossing_rate': np.mean(np.abs(np.diff(np.sign(audio)))) / 2,
            'is_valid': True,
            'quality_score': 0.0
        }

        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        score = 0.0

        # ê¸¸ì´ ê²€ì¦ (ìµœì†Œ 0.1ì´ˆ)
        if metrics['length_seconds'] >= 0.1:
            score += 0.25

        # ì§„í­ ê²€ì¦ (í´ë¦¬í•‘ ì—†ìŒ)
        if 0.01 <= metrics['max_amplitude'] <= 0.99:
            score += 0.25

        # RMS ê²€ì¦ (ì ì ˆí•œ ì—ë„ˆì§€)
        if 0.001 <= metrics['rms'] <= 0.5:
            score += 0.25

        # ì œë¡œ í¬ë¡œì‹± ê²€ì¦ (ìŒì„± ì‹ í˜¸ íŠ¹ì„±)
        if 0.01 <= metrics['zero_crossing_rate'] <= 0.5:
            score += 0.25

        metrics['quality_score'] = score
        metrics['is_valid'] = score >= 0.5

        return metrics

    except Exception as e:
        print(f"âš ï¸ ì˜¤ë””ì˜¤ ê²€ì¦ ì‹¤íŒ¨: {e}")
        return {
            'length_seconds': 0,
            'max_amplitude': 0,
            'rms': 0,
            'zero_crossing_rate': 0,
            'is_valid': False,
            'quality_score': 0.0
        }


# =============================================================================
# í†µí•© ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜
# =============================================================================

async def intelligent_feature_processing(input_data: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """ì§€ëŠ¥í˜• íŠ¹ì„± ì²˜ë¦¬ - ëª¨ë“  íƒ€ì… ìë™ ì²˜ë¦¬"""
    try:
        print(f"ğŸ§  ì§€ëŠ¥í˜• íŠ¹ì„± ì²˜ë¦¬ ì‹œì‘: {input_data.shape}")

        # 1ì°¨ì›ì¸ ê²½ìš° ì´ë¯¸ ì˜¤ë””ì˜¤ë¡œ ê°„ì£¼
        if len(input_data.shape) == 1:
            print("ğŸ“» 1D ì˜¤ë””ì˜¤ ë°ì´í„° - ì§ì ‘ ë°˜í™˜")
            return input_data

        # 2ì°¨ì›ì¸ ê²½ìš° íŠ¹ì„±ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ ë³€í™˜
        if len(input_data.shape) == 2:
            # ì ì‘í˜• íŠ¹ì„±-ì˜¤ë””ì˜¤ ë³€í™˜
            audio = await adaptive_feature_to_audio(input_data, sample_rate)

            # í’ˆì§ˆ í–¥ìƒ
            enhanced_audio = enhance_audio_quality(audio, sample_rate)

            # í’ˆì§ˆ ê²€ì¦
            validation = validate_reconstructed_audio(enhanced_audio, sample_rate)

            if validation['is_valid']:
                print(f"âœ… ì§€ëŠ¥í˜• ì²˜ë¦¬ ì™„ë£Œ - í’ˆì§ˆ ì ìˆ˜: {validation['quality_score']:.2f}")
                return enhanced_audio
            else:
                print("âš ï¸ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨ - ì›ë³¸ ì˜¤ë””ì˜¤ ë°˜í™˜")
                return audio

        # ì˜ˆìƒì¹˜ ëª»í•œ ì°¨ì›
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•íƒœ: {input_data.shape}")
        return np.random.randn(sample_rate) * 0.05

    except Exception as e:
        print(f"âŒ ì§€ëŠ¥í˜• ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return np.random.randn(sample_rate) * 0.05


import librosa
import time
from typing import Optional, Tuple
import scipy.signal
import scipy.ndimage


async def convert_preprocessing(audio_data: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """
    ê¸°ë³¸ ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ í•¨ìˆ˜
    Args:
        audio_data: ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„° (numpy array)
        sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸ (ê¸°ë³¸ê°’: 16000)
    Returns:
        ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    try:
        print(f"ğŸ”§ ê¸°ë³¸ ì „ì²˜ë¦¬ ì‹œì‘ - ì…ë ¥ ê¸¸ì´: {len(audio_data)}")

        # 1. ì •ê·œí™”
        if np.max(np.abs(audio_data)) > 0:
            audio_data = audio_data / np.max(np.abs(audio_data))

        # 2. ë…¸ì´ì¦ˆ ì œê±°
        audio_data = _remove_noise(audio_data, sample_rate)

        # 3. í”„ë¦¬ì— í¼ì‹œìŠ¤
        audio_data = _apply_preemphasis(audio_data)

        print(f"âœ… ê¸°ë³¸ ì „ì²˜ë¦¬ ì™„ë£Œ - ì¶œë ¥ ê¸¸ì´: {len(audio_data)}")
        return audio_data

    except Exception as e:
        print(f"âŒ ê¸°ë³¸ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return audio_data


async def convert_preprocessing_for_microphone_stable(audio_data: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """
    ë§ˆì´í¬ ì…ë ¥ìš© ì•ˆì •í™”ëœ ì „ì²˜ë¦¬
    Args:
        audio_data: ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°
        sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
    Returns:
        ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    try:
        print(f"ğŸ¤ ë§ˆì´í¬ ì•ˆì •í™” ì „ì²˜ë¦¬ ì‹œì‘ - ì…ë ¥ ê¸¸ì´: {len(audio_data)}")

        # 1. DC ì˜¤í”„ì…‹ ì œê±°
        audio_data = audio_data - np.mean(audio_data)

        # 2. ì •ê·œí™”
        if np.max(np.abs(audio_data)) > 0:
            audio_data = audio_data / np.max(np.abs(audio_data))

        # 3. ê³ ì£¼íŒŒ ë…¸ì´ì¦ˆ ì œê±° (ì €ì—­í†µê³¼ í•„í„°)
        audio_data = _apply_lowpass_filter(audio_data, sample_rate, cutoff=8000)

        # 4. VAD (Voice Activity Detection)
        audio_data = _apply_vad(audio_data, sample_rate)

        # 5. í”„ë¦¬ì— í¼ì‹œìŠ¤
        audio_data = _apply_preemphasis(audio_data)

        print(f"âœ… ë§ˆì´í¬ ì•ˆì •í™” ì „ì²˜ë¦¬ ì™„ë£Œ - ì¶œë ¥ ê¸¸ì´: {len(audio_data)}")
        return audio_data

    except Exception as e:
        print(f"âŒ ë§ˆì´í¬ ì•ˆì •í™” ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return audio_data


async def convert_preprocessing_for_microphone_advanced(audio_data: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """
    ë§ˆì´í¬ ì…ë ¥ìš© ê³ ê¸‰ ì „ì²˜ë¦¬
    Args:
        audio_data: ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°
        sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
    Returns:
        ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    try:
        print(f"ğŸ”¬ ë§ˆì´í¬ ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹œì‘ - ì…ë ¥ ê¸¸ì´: {len(audio_data)}")

        # 1. DC ì˜¤í”„ì…‹ ì œê±°
        audio_data = audio_data - np.mean(audio_data)

        # 2. ìŠ¤í™íŠ¸ëŸ¼ ë…¸ì´ì¦ˆ ê°ì†Œ
        audio_data = _spectral_noise_reduction(audio_data, sample_rate)

        # 3. ë™ì  ë²”ìœ„ ì••ì¶•
        audio_data = _dynamic_range_compression(audio_data)

        # 4. ê³ ê¸‰ VAD
        audio_data = _advanced_vad(audio_data, sample_rate)

        # 5. ì ì‘í˜• í•„í„°ë§
        audio_data = _adaptive_filtering(audio_data, sample_rate)

        # 6. í”„ë¦¬ì— í¼ì‹œìŠ¤
        audio_data = _apply_preemphasis(audio_data)

        print(f"âœ… ë§ˆì´í¬ ê³ ê¸‰ ì „ì²˜ë¦¬ ì™„ë£Œ - ì¶œë ¥ ê¸¸ì´: {len(audio_data)}")
        return audio_data

    except Exception as e:
        print(f"âŒ ë§ˆì´í¬ ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return audio_data


async def convert_preprocessing_for_microphone_chunked(audio_data: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """
    ë§ˆì´í¬ ì…ë ¥ìš© ì²­í‚¹ ì „ì²˜ë¦¬ (ê¸´ ì˜¤ë””ì˜¤ìš©)
    Args:
        audio_data: ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°
        sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
    Returns:
        ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    try:
        print(f"ğŸ“Š ì²­í‚¹ ì „ì²˜ë¦¬ ì‹œì‘ - ì…ë ¥ ê¸¸ì´: {len(audio_data)}")

        # ì˜¤ë””ì˜¤ ê¸¸ì´ í™•ì¸
        duration = len(audio_data) / sample_rate

        if duration <= 30.0:
            # 30ì´ˆ ì´í•˜ë©´ ì¼ë°˜ ì „ì²˜ë¦¬
            return await convert_preprocessing_for_microphone_stable(audio_data, sample_rate)

        # 30ì´ˆ ì´ìƒì´ë©´ ì²­í‚¹ ì²˜ë¦¬
        chunk_size = 25 * sample_rate  # 25ì´ˆ ì²­í¬
        overlap_size = 2 * sample_rate  # 2ì´ˆ ì˜¤ë²„ë©

        processed_chunks = []
        start_idx = 0

        while start_idx < len(audio_data):
            end_idx = min(start_idx + chunk_size, len(audio_data))
            chunk = audio_data[start_idx:end_idx]

            # ê° ì²­í¬ ì „ì²˜ë¦¬
            processed_chunk = await convert_preprocessing_for_microphone_stable(chunk, sample_rate)
            processed_chunks.append(processed_chunk)

            start_idx += (chunk_size - overlap_size)

        # ì²­í¬ë“¤ ê²°í•©
        result = np.concatenate(processed_chunks)

        print(f"âœ… ì²­í‚¹ ì „ì²˜ë¦¬ ì™„ë£Œ - ì¶œë ¥ ê¸¸ì´: {len(result)}")
        return result

    except Exception as e:
        print(f"âŒ ì²­í‚¹ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return audio_data


async def convert_preprocessing_simple(audio_data: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """
    ê°„ë‹¨í•œ ì „ì²˜ë¦¬ (ìµœì†Œí•œì˜ ì²˜ë¦¬)
    Args:
        audio_data: ì…ë ¥ ì˜¤ë””ì˜¤ ë°ì´í„°
        sample_rate: ìƒ˜í”Œë§ ë ˆì´íŠ¸
    Returns:
        ì „ì²˜ë¦¬ëœ ì˜¤ë””ì˜¤ ë°ì´í„°
    """
    try:
        print(f"âš¡ ê°„ë‹¨ ì „ì²˜ë¦¬ ì‹œì‘ - ì…ë ¥ ê¸¸ì´: {len(audio_data)}")

        # 1. ì •ê·œí™”ë§Œ ìˆ˜í–‰
        if np.max(np.abs(audio_data)) > 0:
            audio_data = audio_data / np.max(np.abs(audio_data))

        # 2. í´ë¦¬í•‘ ë°©ì§€
        audio_data = np.clip(audio_data, -1.0, 1.0)

        print(f"âœ… ê°„ë‹¨ ì „ì²˜ë¦¬ ì™„ë£Œ - ì¶œë ¥ ê¸¸ì´: {len(audio_data)}")
        return audio_data

    except Exception as e:
        print(f"âŒ ê°„ë‹¨ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return audio_data


# =============================================================================
# ë³´ì¡° í•¨ìˆ˜ë“¤
# =============================================================================

def _remove_noise(audio_data: np.ndarray, sample_rate: int, noise_threshold: float = 0.01) -> np.ndarray:
    """ê¸°ë³¸ ë…¸ì´ì¦ˆ ì œê±°"""
    try:
        # ê°„ë‹¨í•œ threshold ê¸°ë°˜ ë…¸ì´ì¦ˆ ì œê±°
        mask = np.abs(audio_data) > noise_threshold
        audio_data = audio_data * mask
        return audio_data
    except Exception:
        return audio_data


def _apply_preemphasis(audio_data: np.ndarray, coef: float = 0.97) -> np.ndarray:
    """í”„ë¦¬ì— í¼ì‹œìŠ¤ í•„í„° ì ìš©"""
    try:
        if len(audio_data) > 1:
            return np.append(audio_data[0], audio_data[1:] - coef * audio_data[:-1])
        return audio_data
    except Exception:
        return audio_data


def _apply_lowpass_filter(audio_data: np.ndarray, sample_rate: int, cutoff: float = 8000) -> np.ndarray:
    """ì €ì—­í†µê³¼ í•„í„° ì ìš©"""
    try:
        nyquist = sample_rate / 2
        normalized_cutoff = cutoff / nyquist

        if normalized_cutoff >= 1.0:
            return audio_data

        b, a = scipy.signal.butter(4, normalized_cutoff, btype='low')
        filtered = scipy.signal.filtfilt(b, a, audio_data)
        return filtered
    except Exception:
        return audio_data


def _apply_vad(audio_data: np.ndarray, sample_rate: int, energy_threshold: float = 0.01) -> np.ndarray:
    """ê¸°ë³¸ Voice Activity Detection"""
    try:
        frame_length = int(0.025 * sample_rate)  # 25ms
        frame_step = int(0.01 * sample_rate)  # 10ms

        # í”„ë ˆì„ ë‹¨ìœ„ë¡œ ì—ë„ˆì§€ ê³„ì‚°
        frames = []
        for i in range(0, len(audio_data) - frame_length, frame_step):
            frame = audio_data[i:i + frame_length]
            energy = np.mean(frame ** 2)
            frames.append((i, i + frame_length, energy))

        if not frames:
            return audio_data

        # ì—ë„ˆì§€ ì„ê³„ê°’ ê¸°ë°˜ ìŒì„± êµ¬ê°„ ê²€ì¶œ
        energies = [f[2] for f in frames]
        max_energy = max(energies) if energies else 0

        if max_energy == 0:
            return audio_data

        threshold = max_energy * energy_threshold

        # ìŒì„± êµ¬ê°„ë§Œ ìœ ì§€
        speech_mask = np.zeros_like(audio_data, dtype=bool)
        for start, end, energy in frames:
            if energy > threshold:
                speech_mask[start:end] = True

        # ìŒì„± êµ¬ê°„ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        if np.sum(speech_mask) < len(audio_data) * 0.1:
            return audio_data

        return audio_data * speech_mask.astype(float)

    except Exception:
        return audio_data


def _spectral_noise_reduction(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
    """ìŠ¤í™íŠ¸ëŸ¼ ê¸°ë°˜ ë…¸ì´ì¦ˆ ê°ì†Œ"""
    try:
        # STFT
        stft = librosa.stft(audio_data, hop_length=512, n_fft=2048)
        magnitude = np.abs(stft)
        phase = np.angle(stft)

        # ë…¸ì´ì¦ˆ ì¶”ì • (ì²« 10í”„ë ˆì„ì˜ í‰ê· )
        noise_profile = np.mean(magnitude[:, :10], axis=1, keepdims=True)

        # ìŠ¤í™íŠ¸ëŸ¼ ì°¨ê°
        enhanced_magnitude = magnitude - 0.5 * noise_profile
        enhanced_magnitude = np.maximum(enhanced_magnitude, 0.1 * magnitude)

        # ì—­ë³€í™˜
        enhanced_stft = enhanced_magnitude * np.exp(1j * phase)
        enhanced_audio = librosa.istft(enhanced_stft, hop_length=512)

        return enhanced_audio[:len(audio_data)]

    except Exception:
        return audio_data


def _dynamic_range_compression(audio_data: np.ndarray, ratio: float = 4.0, threshold: float = 0.5) -> np.ndarray:
    """ë™ì  ë²”ìœ„ ì••ì¶•"""
    try:
        # ê°„ë‹¨í•œ ì»´í”„ë ˆì„œ
        compressed = np.copy(audio_data)

        # ì„ê³„ê°’ì„ ë„˜ëŠ” ë¶€ë¶„ì— ì••ì¶• ì ìš©
        mask = np.abs(compressed) > threshold
        over_threshold = compressed[mask]

        # ì••ì¶• ì ìš©
        sign = np.sign(over_threshold)
        abs_vals = np.abs(over_threshold)
        compressed_vals = threshold + (abs_vals - threshold) / ratio

        compressed[mask] = sign * compressed_vals

        return compressed

    except Exception:
        return audio_data


def _advanced_vad(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
    """ê³ ê¸‰ Voice Activity Detection"""
    try:
        # ì—¬ëŸ¬ íŠ¹ì„±ì„ ì¡°í•©í•œ VAD
        frame_length = int(0.025 * sample_rate)
        frame_step = int(0.01 * sample_rate)

        features = []
        for i in range(0, len(audio_data) - frame_length, frame_step):
            frame = audio_data[i:i + frame_length]

            # ì—ë„ˆì§€
            energy = np.mean(frame ** 2)

            # ì œë¡œ í¬ë¡œì‹± ë¹„ìœ¨
            zcr = np.mean(np.abs(np.diff(np.sign(frame)))) / 2

            # ìŠ¤í™íŠ¸ëŸ¼ ì¤‘ì‹¬
            try:
                spectrum = np.abs(np.fft.fft(frame))
                freqs = np.fft.fftfreq(len(frame), 1 / sample_rate)
                spectral_centroid = np.sum(freqs[:len(freqs) // 2] * spectrum[:len(spectrum) // 2]) / np.sum(
                    spectrum[:len(spectrum) // 2])
            except:
                spectral_centroid = 0

            features.append((i, i + frame_length, energy, zcr, spectral_centroid))

        if not features:
            return audio_data

        # ì„ê³„ê°’ ê¸°ë°˜ ë¶„ë¥˜
        energies = [f[2] for f in features]
        zcrs = [f[3] for f in features]

        energy_threshold = np.percentile(energies, 30)
        zcr_threshold = np.percentile(zcrs, 70)

        # ìŒì„± êµ¬ê°„ ë§ˆìŠ¤í¬ ìƒì„±
        speech_mask = np.zeros_like(audio_data, dtype=bool)
        for start, end, energy, zcr, _ in features:
            if energy > energy_threshold and zcr < zcr_threshold:
                speech_mask[start:end] = True

        # ìŒì„± êµ¬ê°„ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
        if np.sum(speech_mask) < len(audio_data) * 0.1:
            return audio_data

        return audio_data * speech_mask.astype(float)

    except Exception:
        return audio_data


def _adaptive_filtering(audio_data: np.ndarray, sample_rate: int) -> np.ndarray:
    """ì ì‘í˜• í•„í„°ë§"""
    try:
        # ê°„ë‹¨í•œ ì ì‘í˜• ìœ„ë„ˆ í•„í„°
        # ë…¸ì´ì¦ˆ ì¶”ì •
        noise_estimate = np.var(audio_data[:int(0.1 * sample_rate)])  # ì²« 0.1ì´ˆë¥¼ ë…¸ì´ì¦ˆë¡œ ê°€ì •
        signal_power = np.var(audio_data)

        if signal_power == 0:
            return audio_data

        # ìœ„ë„ˆ í•„í„° ê³„ìˆ˜
        snr = signal_power / (noise_estimate + 1e-10)
        wiener_filter = snr / (snr + 1)

        return audio_data * wiener_filter

    except Exception:
        return audio_data


# =============================================================================
# MFCC ê´€ë ¨ í•¨ìˆ˜ë“¤ (í˜¸í™˜ì„± ìœ ì§€)
# =============================================================================

async def extract_mfcc_features(audio_data: np.ndarray, sample_rate: int = 16000, n_mfcc: int = 13) -> np.ndarray:
    """MFCC íŠ¹ì„± ì¶”ì¶œ"""
    try:
        mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=n_mfcc)
        delta = librosa.feature.delta(mfcc)
        delta2 = librosa.feature.delta(mfcc, order=2)

        # ëª¨ë“  íŠ¹ì„± ê²°í•©
        combined_features = np.vstack([mfcc, delta, delta2])
        return combined_features

    except Exception as e:
        print(f"âŒ MFCC ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        # ë”ë¯¸ íŠ¹ì„± ë°˜í™˜
        return np.random.randn(39, 100)


async def convert_mfcc_to_audio_compatible(mfcc_features: np.ndarray, sample_rate: int = 16000) -> np.ndarray:
    """MFCC + Delta + Delta2 íŠ¹ì„±ì„ Whisper í˜¸í™˜ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜"""
    try:
        n_features, n_frames = mfcc_features.shape
        print(f"ğŸ“ MFCC íŠ¹ì„± ë³€í™˜: {mfcc_features.shape}")

        if n_features == 13:
            # ìˆœìˆ˜ MFCC
            print("ğŸ“Š ìˆœìˆ˜ MFCC (13ì°¨ì›) ì²˜ë¦¬")
            mfcc_only = mfcc_features
            audio_approx = _mfcc_basic_reconstruction(mfcc_only, sample_rate)

        elif n_features == 26:
            # MFCC + Delta
            print("ğŸ“Š MFCC + Delta (26ì°¨ì›) ì²˜ë¦¬")
            mfcc_only = mfcc_features[:13, :]
            delta = mfcc_features[13:26, :]
            audio_approx = _mfcc_delta_reconstruction(mfcc_only, delta, sample_rate)

        elif n_features == 39:
            # MFCC + Delta + Delta2 (ì™„ì „í•œ íŠ¹ì„±)
            print("ğŸ“Š MFCC + Delta + Delta2 (39ì°¨ì›) ì²˜ë¦¬")
            mfcc_only = mfcc_features[:13, :]
            delta = mfcc_features[13:26, :]
            delta2 = mfcc_features[26:39, :]
            audio_approx = _mfcc_full_reconstruction(mfcc_only, delta, delta2, sample_rate)

        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” ì°¨ì› - ì²« 13ê°œë§Œ ì‚¬ìš©
            print(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íŠ¹ì„± ì°¨ì›: {n_features} - ì²« 13ê°œë§Œ ì‚¬ìš©")
            mfcc_like = mfcc_features[:min(13, n_features), :]
            audio_approx = _mfcc_basic_reconstruction(mfcc_like, sample_rate)

        # ì •ê·œí™”
        if np.max(np.abs(audio_approx)) > 0:
            audio_approx = audio_approx / np.max(np.abs(audio_approx))

        print(f"âœ… MFCC ë³€í™˜ ì™„ë£Œ: {len(audio_approx)} samples")
        return audio_approx

    except Exception as e:
        print(f"âŒ MFCC ë³€í™˜ ì‹¤íŒ¨: {e}")
        return np.random.randn(sample_rate) * 0.1


def _mfcc_basic_reconstruction(mfcc: np.ndarray, sample_rate: int) -> np.ndarray:
    """ê¸°ë³¸ MFCC ë³µì›"""
    try:
        # LibROSA ì—­ë³€í™˜ ì‚¬ìš©
        audio_approx = librosa.feature.inverse.mfcc_to_audio(
            mfcc,
            sr=sample_rate,
            n_fft=2048,
            hop_length=512
        )
        return audio_approx

    except Exception as e:
        print(f"âš ï¸ LibROSA ì—­ë³€í™˜ ì‹¤íŒ¨: {e} - ëŒ€ì²´ ë°©ë²• ì‚¬ìš©")
        return _manual_mfcc_reconstruction(mfcc, sample_rate)


def _mfcc_delta_reconstruction(mfcc: np.ndarray, delta: np.ndarray, sample_rate: int) -> np.ndarray:
    """MFCC + Delta ë³µì›"""
    try:
        print("ğŸ”„ MFCC + Delta í˜‘ë ¥ ë³µì›")

        # Delta ì •ë³´ë¡œ MFCC ê¶¤ì  ê°œì„ 
        enhanced_mfcc = _improve_mfcc_with_delta(mfcc, delta)

        # ê°œì„ ëœ MFCCë¡œ ì˜¤ë””ì˜¤ ë³µì›
        audio = _mfcc_basic_reconstruction(enhanced_mfcc, sample_rate)

        # Delta ê¸°ë°˜ ì‹œê°„ì  ë³€ì¡° ì ìš©
        modulated_audio = _apply_delta_temporal_modulation(audio, delta, sample_rate)

        return modulated_audio

    except Exception as e:
        print(f"âš ï¸ MFCC+Delta ë³µì› ì‹¤íŒ¨: {e}")
        return _mfcc_basic_reconstruction(mfcc, sample_rate)


def _mfcc_full_reconstruction(mfcc: np.ndarray, delta: np.ndarray, delta2: np.ndarray, sample_rate: int) -> np.ndarray:
    """MFCC + Delta + Delta2 ì™„ì „ ë³µì›"""
    try:
        print("ğŸ”„ MFCC + Delta + Delta2 ì™„ì „ ë³µì›")

        # 1ë‹¨ê³„: Delta2ë¡œ Delta ê¶¤ì  ê°œì„ 
        improved_delta = _improve_delta_with_delta2(delta, delta2)

        # 2ë‹¨ê³„: ê°œì„ ëœ Deltaë¡œ MFCC ê¶¤ì  ê°œì„ 
        enhanced_mfcc = _improve_mfcc_with_delta(mfcc, improved_delta)

        # 3ë‹¨ê³„: ê°œì„ ëœ MFCCë¡œ ê¸°ë³¸ ì˜¤ë””ì˜¤ ë³µì›
        audio = _mfcc_basic_reconstruction(enhanced_mfcc, sample_rate)

        # 4ë‹¨ê³„: Delta ë³€ì¡° ì ìš©
        audio = _apply_delta_temporal_modulation(audio, improved_delta, sample_rate)

        # 5ë‹¨ê³„: Delta2 ê¸°ë°˜ ê³ ì°¨ ë³€ì¡° ì ìš©
        audio = _apply_delta2_spectral_modulation(audio, delta2, sample_rate)

        return audio

    except Exception as e:
        print(f"âš ï¸ ì™„ì „ ë³µì› ì‹¤íŒ¨: {e}")
        return _mfcc_delta_reconstruction(mfcc, delta, sample_rate)


def _improve_mfcc_with_delta(mfcc: np.ndarray, delta: np.ndarray) -> np.ndarray:
    """Delta ì •ë³´ë¡œ MFCC ê¶¤ì  ê°œì„ """
    try:
        enhanced_mfcc = np.copy(mfcc)

        # DeltaëŠ” ì‹œê°„ì  ë³€í™”ìœ¨ì´ë¯€ë¡œ ì´ë¥¼ ì ë¶„í•˜ì—¬ ë¶€ë“œëŸ¬ìš´ ê¶¤ì  ìƒì„±
        for coeff_idx in range(min(mfcc.shape[0], delta.shape[0])):
            for time_idx in range(1, min(mfcc.shape[1], delta.shape[1])):
                # Delta ì •ë³´ë¥¼ ì‚¬ìš©í•œ ì‹œê°„ì  ì—°ì†ì„± ê°œì„ 
                delta_contribution = 0.1 * delta[coeff_idx, time_idx]
                enhanced_mfcc[coeff_idx, time_idx] += delta_contribution

        # ë¶€ë“œëŸ¬ìš´ ì „í™˜ì„ ìœ„í•œ ê°€ìš°ì‹œì•ˆ í•„í„° ì ìš©
        try:
            from scipy import ndimage
            enhanced_mfcc = ndimage.gaussian_filter1d(enhanced_mfcc, sigma=0.5, axis=1)
        except ImportError:
            pass  # scipyê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ

        return enhanced_mfcc

    except Exception as e:
        print(f"âš ï¸ MFCC ê¶¤ì  ê°œì„  ì‹¤íŒ¨: {e}")
        return mfcc


def _improve_delta_with_delta2(delta: np.ndarray, delta2: np.ndarray) -> np.ndarray:
    """Delta2 ì •ë³´ë¡œ Delta ê¶¤ì  ê°œì„ """
    try:
        improved_delta = np.copy(delta)

        # Delta2ëŠ” ê°€ì†ë„ì´ë¯€ë¡œ ì´ë¥¼ ì ë¶„í•˜ì—¬ Delta ê°œì„ 
        for coeff_idx in range(min(delta.shape[0], delta2.shape[0])):
            for time_idx in range(1, min(delta.shape[1], delta2.shape[1])):
                # Delta2ë¥¼ ì ë¶„í•˜ì—¬ Deltaì˜ ë¶€ë“œëŸ¬ìš´ ë³€í™” ìƒì„±
                acceleration = delta2[coeff_idx, time_idx]
                velocity_change = 0.05 * acceleration
                improved_delta[coeff_idx, time_idx] += velocity_change

        return improved_delta

    except Exception as e:
        print(f"âš ï¸ Delta ê¶¤ì  ê°œì„  ì‹¤íŒ¨: {e}")
        return delta


def _apply_delta_temporal_modulation(audio: np.ndarray, delta: np.ndarray, sample_rate: int) -> np.ndarray:
    """Delta ê¸°ë°˜ ì‹œê°„ì  ë³€ì¡°"""
    try:
        # Deltaì˜ ì—ë„ˆì§€ë¥¼ ì‹œê°„ì  ë³€ì¡°ì— í™œìš©
        delta_energy = np.mean(np.abs(delta), axis=0)

        if len(delta_energy) == 0:
            return audio

        # ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ê²Œ Delta ì—ë„ˆì§€ ë³´ê°„
        audio_times = np.linspace(0, len(delta_energy) - 1, len(audio))
        delta_interp = np.interp(audio_times, np.arange(len(delta_energy)), delta_energy)

        # Delta ì—ë„ˆì§€ ê¸°ë°˜ ì§„í­ ë³€ì¡°
        modulation = 1.0 + 0.2 * (delta_interp - np.mean(delta_interp)) / (np.std(delta_interp) + 1e-10)
        modulated_audio = audio * modulation

        return modulated_audio

    except Exception as e:
        print(f"âš ï¸ Delta ì‹œê°„ ë³€ì¡° ì‹¤íŒ¨: {e}")
        return audio


def _apply_delta2_spectral_modulation(audio: np.ndarray, delta2: np.ndarray, sample_rate: int) -> np.ndarray:
    """Delta2 ê¸°ë°˜ ìŠ¤í™íŠ¸ëŸ¼ ë³€ì¡°"""
    try:
        # Delta2ì˜ ë³€í™”ë¥¼ ì£¼íŒŒìˆ˜ ë³€ì¡°ë¡œ ë³€í™˜
        delta2_energy = np.mean(np.abs(delta2), axis=0)

        if len(delta2_energy) == 0:
            return audio

        # ì˜¤ë””ì˜¤ ê¸¸ì´ì— ë§ê²Œ ë³´ê°„
        audio_times = np.linspace(0, len(delta2_energy) - 1, len(audio))
        delta2_interp = np.interp(audio_times, np.arange(len(delta2_energy)), delta2_energy)

        # Delta2 ê¸°ë°˜ ë¯¸ì„¸í•œ ì£¼íŒŒìˆ˜ ì‹œí”„íŠ¸ ì‹œë®¬ë ˆì´ì…˜
        time_vec = np.arange(len(audio)) / sample_rate
        phase_modulation = 0.1 * np.cumsum(delta2_interp) / sample_rate

        # ìœ„ìƒ ë³€ì¡° ì ìš©
        modulated_audio = audio * np.cos(2 * np.pi * phase_modulation)

        return modulated_audio

    except Exception as e:
        print(f"âš ï¸ Delta2 ìŠ¤í™íŠ¸ëŸ¼ ë³€ì¡° ì‹¤íŒ¨: {e}")
        return audio


def _manual_mfcc_reconstruction(mfcc: np.ndarray, sample_rate: int) -> np.ndarray:
    """ìˆ˜ë™ MFCC ë³µì› (LibROSA ì‹¤íŒ¨ ì‹œ ëŒ€ì²´ ë°©ë²•)"""
    try:
        print("ğŸ”§ ìˆ˜ë™ MFCC ë³µì› ì‹œë„")

        # MFCC ê³„ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•œ ì‚¬ì¸íŒŒ í•©ì„±
        n_frames = mfcc.shape[1]
        hop_length = 512
        audio_length = n_frames * hop_length

        # ê¸°ë³¸ ì£¼íŒŒìˆ˜ ì„¤ì •
        base_freq = 200  # Hz

        # ê° MFCC ê³„ìˆ˜ë¥¼ ì„œë¡œ ë‹¤ë¥¸ ì£¼íŒŒìˆ˜ ì„±ë¶„ìœ¼ë¡œ ë³€í™˜
        audio = np.zeros(audio_length)

        for coeff_idx in range(min(8, mfcc.shape[0])):  # ì²« 8ê°œ ê³„ìˆ˜ë§Œ ì‚¬ìš©
            for frame_idx in range(n_frames):
                start_sample = frame_idx * hop_length
                end_sample = min(start_sample + hop_length, audio_length)

                # MFCC ê³„ìˆ˜ë¥¼ ì£¼íŒŒìˆ˜ì™€ ì§„í­ìœ¼ë¡œ ë³€í™˜
                freq = base_freq * (coeff_idx + 1)
                amplitude = abs(mfcc[coeff_idx, frame_idx]) * 0.1

                # í•´ë‹¹ í”„ë ˆì„ì— ì‚¬ì¸íŒŒ ì¶”ê°€
                t = np.arange(end_sample - start_sample) / sample_rate
                sine_wave = amplitude * np.sin(2 * np.pi * freq * t)
                audio[start_sample:end_sample] += sine_wave

        # ì •ê·œí™”
        if np.max(np.abs(audio)) > 0:
            audio = audio / np.max(np.abs(audio)) * 0.5

        print(f"âœ… ìˆ˜ë™ ë³µì› ì™„ë£Œ: {len(audio)} samples")
        return audio

    except Exception as e:
        print(f"âŒ ìˆ˜ë™ ë³µì› ì‹¤íŒ¨: {e}")
        # ìµœí›„ì˜ ìˆ˜ë‹¨: í™”ì´íŠ¸ ë…¸ì´ì¦ˆ
        return np.random.randn(sample_rate) * 0.05