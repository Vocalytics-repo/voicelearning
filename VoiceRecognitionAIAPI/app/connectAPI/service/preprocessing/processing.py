import numpy as np
import librosa
import time


# =============================================================================
# í•™ìŠµê³¼ 100% ë™ì¼í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
# =============================================================================

async def convert_preprocessing(np_pcm, sample_rate=16000, use_simple=None, output_type='auto') -> np.ndarray:
    """
    í•™ìŠµê³¼ 100% ë™ì¼í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
    - í•™ìŠµ ì‹œ ì‚¬ìš©í•œ preprocess_and_save_pcm_features_only í•¨ìˆ˜ì™€ ë™ì¼í•œ ë¡œì§
    """
    try:
        start_total = time.time()
        print(f"ğŸ” í•™ìŠµìš© ì „ì²˜ë¦¬ ì‹œì‘ - ë°ì´í„° ê¸¸ì´: {len(np_pcm)}")

        # ì…ë ¥ ê²€ì¦
        if len(np_pcm) == 0:
            print("âŒ ë¹ˆ ì˜¤ë””ì˜¤ ë°ì´í„°")
            return np.zeros((39, 1))  # ê¸°ë³¸ combined_features í˜•íƒœ

        # ğŸ¯ í•™ìŠµê³¼ 100% ë™ì¼í•œ ì „ì²˜ë¦¬ ìˆœì„œ
        print("ğŸ”¬ í•™ìŠµê³¼ ë™ì¼í•œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")

        # 1. ë°ì´í„°ê°€ ì´ë¯¸ float32 í˜•íƒœë¼ê³  ê°€ì • (preprocessingì—ì„œ ë³€í™˜ë¨)
        audio_data = np_pcm
        print(f"  1ï¸âƒ£ ì…ë ¥ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")

        # 2. í”„ë¦¬ì— í¼ì‹œìŠ¤ ì ìš© (í•™ìŠµê³¼ ë™ì¼)
        start = time.time()
        emphasized = apply_preemphasis(audio_data)
        print(f"  2ï¸âƒ£ í”„ë¦¬ì— í¼ì‹œìŠ¤: {time.time() - start:.3f}ì´ˆ")

        # 3. ë…¸ì´ì¦ˆ ì œê±° (í•™ìŠµê³¼ ë™ì¼)
        start = time.time()
        denoised = simple_noise_reduction(emphasized, sample_rate)
        print(f"  3ï¸âƒ£ ë…¸ì´ì¦ˆ ì œê±°: {time.time() - start:.3f}ì´ˆ")

        # 4. ì •ê·œí™” (í•™ìŠµê³¼ ë™ì¼: 'peak' ë°©ì‹)
        start = time.time()
        normalized = normalize_audio(denoised, 'peak')
        print(f"  4ï¸âƒ£ ì •ê·œí™”: {time.time() - start:.3f}ì´ˆ")

        # 5. VAD ì ìš© (í•™ìŠµê³¼ ë™ì¼)
        start = time.time()
        vad_mask, vad_frames = simple_vad(normalized, sample_rate)
        speech_only = normalized * vad_mask
        print(f"  5ï¸âƒ£ VAD: {time.time() - start:.3f}ì´ˆ")

        # 6. MFCC ì¶”ì¶œ (í•™ìŠµê³¼ ë™ì¼)
        start = time.time()
        mfcc_features = compute_mfcc(speech_only, sample_rate)
        print(f"  6ï¸âƒ£ MFCC: {time.time() - start:.3f}ì´ˆ")

        # 7. ë¸íƒ€ ë° ë¸íƒ€-ë¸íƒ€ ê³„ì‚° (í•™ìŠµê³¼ ë™ì¼)
        start = time.time()
        delta_features, delta2_features = compute_deltas(mfcc_features)
        print(f"  7ï¸âƒ£ ë¸íƒ€: {time.time() - start:.3f}ì´ˆ")

        # 8. ëª¨ë“  íŠ¹ì„±ì„ ê²°í•© (í•™ìŠµê³¼ ì •í™•íˆ ë™ì¼)
        start = time.time()
        combined_features = np.vstack([mfcc_features, delta_features, delta2_features])
        print(f"  8ï¸âƒ£ íŠ¹ì„± ê²°í•©: {time.time() - start:.3f}ì´ˆ")

        total_time = time.time() - start_total
        print(f"ğŸ“ í•™ìŠµìš© ì „ì²˜ë¦¬ ì™„ë£Œ - ì´ ì‹œê°„: {total_time:.3f}ì´ˆ")
        print(f"ğŸ“Š ì¶œë ¥ í˜•íƒœ: {combined_features.shape} (í•™ìŠµê³¼ ë™ì¼)")

        return combined_features

    except Exception as e:
        print(f"âŒ í•™ìŠµìš© ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
        # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ í˜•íƒœ ë°˜í™˜
        return np.zeros((39, 100))


# =============================================================================
# ë³´ì¡° í•¨ìˆ˜ë“¤ (í•™ìŠµê³¼ ë™ì¼)
# =============================================================================

def apply_preemphasis(audio_data, coef=0.97):
    """Apply pre-emphasis filter to audio data"""
    return np.append(audio_data[0], audio_data[1:] - coef * audio_data[:-1])


def frame_signal(audio_data, sample_rate, frame_size=0.025, frame_stride=0.01, window='hamming'):
    """Divide the audio signal into overlapping frames and apply window function"""
    frame_length = int(frame_size * sample_rate)
    frame_step = int(frame_stride * sample_rate)

    signal_length = len(audio_data)
    num_frames = 1 + int(np.ceil((signal_length - frame_length) / frame_step))

    pad_length = (num_frames - 1) * frame_step + frame_length
    padded_signal = np.append(audio_data, np.zeros(pad_length - signal_length))

    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + \
              np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T
    frames = padded_signal[indices.astype(np.int32)]

    if window == 'hamming':
        frames = frames * np.hamming(frame_length)
    elif window == 'hanning':
        frames = frames * np.hanning(frame_length)

    return frames


def simple_noise_reduction(audio_data, sample_rate, noise_threshold=0.005):
    """Apply simple noise reduction by thresholding"""
    denoised = np.copy(audio_data)
    denoised[np.abs(denoised) < noise_threshold] = 0
    return denoised


def normalize_audio(audio_data, method='peak'):
    """Normalize audio data"""
    if method == 'peak':
        # í”¼í¬ ê¸°ë°˜ ì •ê·œí™” (-1 ~ 1 ë²”ìœ„)
        if np.max(np.abs(audio_data)) > 0:
            return audio_data / np.max(np.abs(audio_data))
        else:
            return audio_data
    elif method == 'rms':
        # RMS ê¸°ë°˜ ì •ê·œí™”
        rms = np.sqrt(np.mean(audio_data ** 2))
        if rms > 0:
            return audio_data / (rms * 10)  # -0.1 ~ 0.1 ë²”ìœ„
        else:
            return audio_data
    else:
        return audio_data


def simple_vad(audio_data, sample_rate, frame_size=0.025, frame_stride=0.01, energy_threshold=0.1):
    """Simple energy-based Voice Activity Detection (í•™ìŠµê³¼ ë™ì¼)"""
    try:
        # í”„ë ˆì„ ê¸¸ì´ì™€ ê°„ê²©ì„ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ë³€í™˜
        frame_length = int(frame_size * sample_rate)
        frame_step = int(frame_stride * sample_rate)

        # í”„ë ˆì„ ë¶„í• 
        frames = frame_signal(audio_data, sample_rate, frame_size, frame_stride)

        # ê° í”„ë ˆì„ì˜ ì—ë„ˆì§€ ê³„ì‚°
        energy = np.sum(frames ** 2, axis=1)

        # ì—ë„ˆì§€ ì •ê·œí™”
        if np.max(energy) > 0:
            energy = energy / np.max(energy)

        # ì„ê³„ê°’ ì ìš©
        speech_frames = energy > energy_threshold

        # í”„ë ˆì„ ë‹¨ìœ„ ê²°ì •ì„ ìƒ˜í”Œ ë‹¨ìœ„ë¡œ ë³€í™˜
        speech_mask = np.zeros_like(audio_data)

        for i, is_speech in enumerate(speech_frames):
            start_idx = i * frame_step
            end_idx = min(start_idx + frame_length, len(audio_data))
            speech_mask[start_idx:end_idx] = 1 if is_speech else 0

        # ì •í™•íˆ ì›ë³¸ ì˜¤ë””ì˜¤ ê¸¸ì´ì™€ ì¼ì¹˜í•˜ë„ë¡ í•¨
        speech_mask = speech_mask[:len(audio_data)]

        return speech_mask, speech_frames

    except Exception as e:
        print(f"âš ï¸ VAD ì‹¤íŒ¨: {str(e)} - ì „ì²´ë¥¼ ìŒì„±ìœ¼ë¡œ ì²˜ë¦¬")
        return np.ones_like(audio_data), np.array([True])


def compute_mfcc(audio_data, sample_rate, num_cepstral=13):
    """Compute MFCC features from an audio signal using librosa (í•™ìŠµê³¼ ë™ì¼)"""
    try:
        mfcc_features = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=num_cepstral)
        return mfcc_features
    except Exception as e:
        print(f"âš ï¸ MFCC ê³„ì‚° ì‹¤íŒ¨: {str(e)} - ë”ë¯¸ ë°ì´í„° ë°˜í™˜")
        return np.random.randn(num_cepstral, 100)


def compute_deltas(features):
    """Compute delta features using librosa (í•™ìŠµê³¼ ë™ì¼)"""
    try:
        delta_features = librosa.feature.delta(features)
        delta2_features = librosa.feature.delta(features, order=2)
        return delta_features, delta2_features
    except Exception as e:
        print(f"âš ï¸ ë¸íƒ€ ê³„ì‚° ì‹¤íŒ¨: {str(e)} - ì œë¡œ ë¸íƒ€ ë°˜í™˜")
        return np.zeros_like(features), np.zeros_like(features)


# =============================================================================
# í˜¸í™˜ì„±ì„ ìœ„í•œ ë‹¨ìˆœ ë²„ì „ (í•„ìš”ì‹œ ì‚¬ìš©)
# =============================================================================

async def convert_preprocessing_for_microphone_advanced(np_pcm, sample_rate=16000) -> np.ndarray:
    """ë§ˆì´í¬ ì…ë ¥ìš© ê³ ê¸‰ ì „ì²˜ë¦¬ (VAD í¬í•¨)"""
    start = time.time()

    if len(np_pcm) == 0:
        return np.array([0.0])

    print("ğŸ¤ ë§ˆì´í¬ ì…ë ¥ìš© ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹œì‘")

    try:
        # 1. í”„ë¦¬ì— í¼ì‹œìŠ¤ ì ìš©
        emphasized = apply_preemphasis(np_pcm)

        # 2. ê°•í™”ëœ ë…¸ì´ì¦ˆ ì œê±°
        denoised = simple_noise_reduction(emphasized, sample_rate, noise_threshold=0.01)

        # 3. ì •ê·œí™”
        normalized = normalize_audio(denoised, 'peak')

        # 4. VAD ì ìš© (ìŒì„± êµ¬ê°„ë§Œ ì¶”ì¶œ)
        vad_mask, vad_frames = simple_vad(normalized, sample_rate, energy_threshold=0.05)  # ì„ê³„ê°’ ë‚®ì¶¤
        speech_only = normalized * vad_mask

        # 5. ìŒì„± êµ¬ê°„ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
        speech_ratio = np.sum(vad_mask) / len(vad_mask)
        if speech_ratio < 0.1:  # 10% ë¯¸ë§Œì´ë©´
            print("  âš ï¸ ìŒì„± êµ¬ê°„ ë¶€ì¡± - ì›ë³¸ ì‚¬ìš©")
            speech_only = normalized
        else:
            print(f"  âœ… ìŒì„± êµ¬ê°„ {speech_ratio * 100:.1f}% ì¶”ì¶œ")

        # 6. ë³¼ë¥¨ ë¶€ìŠ¤íŠ¸
        boost_factor = 1.2
        speech_only = speech_only * boost_factor
        speech_only = np.clip(speech_only, -1.0, 1.0)

        # 7. ìµœì¢… ì •ê·œí™”
        if np.max(np.abs(speech_only)) > 0:
            speech_only = speech_only / np.max(np.abs(speech_only))

        print(f"âœ… ë§ˆì´í¬ìš© ê³ ê¸‰ ì „ì²˜ë¦¬: {time.time() - start:.3f}ì´ˆ")
        print(f"ğŸ“Š ì¶œë ¥: 1D ì˜¤ë””ì˜¤ ë°°ì—´, ê¸¸ì´ {len(speech_only)}")

        return speech_only

    except Exception as e:
        print(f"âŒ ê³ ê¸‰ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)} - ê¸°ë³¸ ë§ˆì´í¬ ì „ì²˜ë¦¬ë¡œ fallback")
        return await convert_preprocessing_for_microphone_stable(np_pcm, sample_rate)


async def convert_preprocessing_for_microphone_stable(np_pcm, sample_rate=16000) -> np.ndarray:
    """ë§ˆì´í¬ ì…ë ¥ìš© ì•ˆì •ì ì¸ ì „ì²˜ë¦¬ (ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì )"""
    start = time.time()

    if len(np_pcm) == 0:
        return np.array([0.0])

    print("ğŸ¤ ë§ˆì´í¬ ì…ë ¥ìš© ì•ˆì •ì  ì „ì²˜ë¦¬ ì‹œì‘")

    # 1. ê¸°ë³¸ ì •ê·œí™”
    if np.max(np.abs(np_pcm)) > 0:
        normalized = np_pcm / np.max(np.abs(np_pcm))
    else:
        normalized = np_pcm

    # 2. ì ë‹¹í•œ ë…¸ì´ì¦ˆ ì œê±° (ë„ˆë¬´ ê°•í•˜ì§€ ì•Šê²Œ)
    noise_threshold = 0.015  # 0.01ê³¼ 0.02 ì‚¬ì´
    normalized[np.abs(normalized) < noise_threshold] = 0

    # 3. ë³¼ë¥¨ ë¶€ìŠ¤íŠ¸ (ì ë‹¹íˆ)
    boost_factor = 1.3
    normalized = normalized * boost_factor
    normalized = np.clip(normalized, -1.0, 1.0)

    # 4. ìµœì¢… ì •ê·œí™”
    if np.max(np.abs(normalized)) > 0:
        normalized = normalized / np.max(np.abs(normalized))

    print(f"âœ… ì•ˆì •ì  ë§ˆì´í¬ ì „ì²˜ë¦¬: {time.time() - start:.3f}ì´ˆ")
    print(f"ğŸ“Š ì¶œë ¥: 1D ì˜¤ë””ì˜¤ ë°°ì—´, ê¸¸ì´ {len(normalized)}")

    return normalized


async def convert_preprocessing_for_microphone_chunked(np_pcm, sample_rate=16000, max_chunk_duration=10) -> np.ndarray:
    """ë§ˆì´í¬ ì…ë ¥ìš© ì²­í¬ ë¶„í•  ì „ì²˜ë¦¬ (ê¸´ ì˜¤ë””ì˜¤ ëŒ€ì‘)"""
    start = time.time()

    if len(np_pcm) == 0:
        return np.array([0.0])

    # ì˜¤ë””ì˜¤ ê¸¸ì´ ê³„ì‚° (ì´ˆ)
    duration = len(np_pcm) / sample_rate

    print(f"ğŸ¤ ë§ˆì´í¬ ì…ë ¥ ë¶„ì„: {duration:.1f}ì´ˆ")

    # ì§§ì€ ì˜¤ë””ì˜¤ëŠ” ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
    if duration <= max_chunk_duration:
        return await convert_preprocessing_for_microphone_advanced(np_pcm, sample_rate)

    print(f"âš ï¸ ê¸´ ì˜¤ë””ì˜¤ ê°ì§€ ({duration:.1f}ì´ˆ) - ì²­í¬ ë¶„í•  ì²˜ë¦¬")

    # ì²­í¬ í¬ê¸° ê³„ì‚°
    chunk_size = int(max_chunk_duration * sample_rate)
    chunks = []

    # ì˜¤ë””ì˜¤ë¥¼ ì²­í¬ë¡œ ë¶„í• 
    for i in range(0, len(np_pcm), chunk_size):
        chunk = np_pcm[i:i + chunk_size]

        # ê° ì²­í¬ì— ê³ ê¸‰ ì „ì²˜ë¦¬ ì ìš©
        processed_chunk = await convert_preprocessing_for_microphone_advanced(chunk, sample_rate)
        chunks.append(processed_chunk)

    # ì²­í¬ë“¤ì„ í•©ì¹˜ê¸°
    result = np.concatenate(chunks)

    print(f"âœ… ì²­í¬ ë¶„í•  ì „ì²˜ë¦¬ ì™„ë£Œ: {time.time() - start:.3f}ì´ˆ")
    print(f"ğŸ“Š ì¶œë ¥: 1D ì˜¤ë””ì˜¤ ë°°ì—´, ê¸¸ì´ {len(result)} ({len(chunks)}ê°œ ì²­í¬)")

    return result


async def convert_preprocessing_simple(np_pcm) -> np.ndarray:
    """ê¸°ì¡´ ë‹¨ìˆœ ì •ê·œí™” ë²„ì „ (í˜¸í™˜ì„±ìš©) - 1D ì˜¤ë””ì˜¤ ì¶œë ¥"""
    start = time.time()

    if len(np_pcm) == 0:
        return np.array([0.0])

    # ë‹¨ìˆœ ì •ê·œí™”ë§Œ ìˆ˜í–‰
    if np.max(np.abs(np_pcm)) > 0:
        normalized = np_pcm / np.max(np.abs(np_pcm))
    else:
        normalized = np_pcm

    # ê°„ë‹¨í•œ ë…¸ì´ì¦ˆ ì œê±° (ì„ íƒì )
    noise_threshold = 0.01
    normalized[np.abs(normalized) < noise_threshold] = 0

    print(f"âœ… ë‹¨ìˆœ ì „ì²˜ë¦¬: {time.time() - start:.3f}ì´ˆ")
    print(f"ğŸ“Š ì¶œë ¥: 1D ì˜¤ë””ì˜¤ ë°°ì—´, ê¸¸ì´ {len(normalized)}")

    return normalized