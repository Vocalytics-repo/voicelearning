{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19c5fa2c-ef4b-4c27-bc09-784188973615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCM 및 TXT 파일 구조:\n",
      "KsponSpeech_0450/KsponSpeech_449656.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449581.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449371.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449256.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449309.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449379.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449333.txt\n",
      "KsponSpeech_0450/KsponSpeech_449584.txt\n",
      "KsponSpeech_0450/KsponSpeech_449610.txt\n",
      "KsponSpeech_0450/KsponSpeech_449085.txt\n",
      "KsponSpeech_0450/KsponSpeech_449322.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449530.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449625.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449031.txt\n",
      "KsponSpeech_0450/KsponSpeech_449514.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449454.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449733.txt\n",
      "KsponSpeech_0450/KsponSpeech_449800.txt\n",
      "KsponSpeech_0450/KsponSpeech_449219.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449532.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449102.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449248.txt\n",
      "KsponSpeech_0450/KsponSpeech_449700.txt\n",
      "KsponSpeech_0450/KsponSpeech_449175.txt\n",
      "KsponSpeech_0450/KsponSpeech_449564.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449818.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449362.txt\n",
      "KsponSpeech_0450/KsponSpeech_449926.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449811.txt\n",
      "KsponSpeech_0450/KsponSpeech_449363.txt\n",
      "KsponSpeech_0450/KsponSpeech_449646.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449817.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449228.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449130.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449293.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449441.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449087.txt\n",
      "KsponSpeech_0450/KsponSpeech_449378.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449007.txt\n",
      "KsponSpeech_0450/KsponSpeech_449843.txt\n",
      "KsponSpeech_0450/KsponSpeech_449776.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449861.txt\n",
      "KsponSpeech_0450/KsponSpeech_449705.txt\n",
      "KsponSpeech_0450/KsponSpeech_449717.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449101.txt\n",
      "KsponSpeech_0450/KsponSpeech_449490.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449281.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449846.txt\n",
      "KsponSpeech_0450/KsponSpeech_449005.txt\n",
      "KsponSpeech_0450/KsponSpeech_449317.txt\n",
      "KsponSpeech_0450/KsponSpeech_449324.txt\n",
      "KsponSpeech_0450/KsponSpeech_449017.txt\n",
      "KsponSpeech_0450/KsponSpeech_449420.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449301.txt\n",
      "KsponSpeech_0450/KsponSpeech_449669.txt\n",
      "KsponSpeech_0450/KsponSpeech_449427.txt\n",
      "KsponSpeech_0450/KsponSpeech_449569.txt\n",
      "KsponSpeech_0450/KsponSpeech_449602.txt\n",
      "KsponSpeech_0450/KsponSpeech_449445.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449394.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449605.txt\n",
      "KsponSpeech_0450/KsponSpeech_449014.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449030.txt\n",
      "KsponSpeech_0450/KsponSpeech_449103.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449055.txt\n",
      "KsponSpeech_0450/KsponSpeech_449401.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449869.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449675.txt\n",
      "KsponSpeech_0450/KsponSpeech_449486.txt\n",
      "KsponSpeech_0450/KsponSpeech_449957.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449780.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449878.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449534.txt\n",
      "KsponSpeech_0450/KsponSpeech_449135.txt\n",
      "KsponSpeech_0450/KsponSpeech_449298.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449772.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449589.txt\n",
      "KsponSpeech_0450/KsponSpeech_449095.txt\n",
      "KsponSpeech_0450/KsponSpeech_449702.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449752.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449100.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449486.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449468.txt\n",
      "KsponSpeech_0450/KsponSpeech_449609.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449555.txt\n",
      "KsponSpeech_0450/KsponSpeech_449521.txt\n",
      "KsponSpeech_0450/KsponSpeech_449533.txt\n",
      "KsponSpeech_0450/KsponSpeech_449401.txt\n",
      "KsponSpeech_0450/KsponSpeech_449830.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449403.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449448.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449330.txt\n",
      "KsponSpeech_0450/KsponSpeech_449234.txt\n",
      "KsponSpeech_0450/KsponSpeech_449364.txt\n",
      "KsponSpeech_0450/KsponSpeech_449547.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449567.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449658.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449497.txt\n",
      "KsponSpeech_0450/KsponSpeech_449136.pcm\n",
      "KsponSpeech_0450/KsponSpeech_449346.pcm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_files_with_pattern(root_path, extensions=['.pcm', '.txt'], max_files=50):\n",
    "    found_files = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in extensions):\n",
    "                full_path = os.path.join(root, file)\n",
    "                # 상대 경로로 표시\n",
    "                rel_path = os.path.relpath(full_path, root_path)\n",
    "                found_files.append(rel_path)\n",
    "                \n",
    "                if len(found_files) >= max_files:\n",
    "                    return found_files\n",
    "    \n",
    "    return found_files\n",
    "\n",
    "# PCM과 TXT 파일 찾기\n",
    "print(\"PCM 및 TXT 파일 구조:\")\n",
    "files = find_files_with_pattern(\".\", ['.pcm', '.txt'], max_files=100)\n",
    "for file in files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffef7d8-f96c-45d0-b145-44cc52188683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Traindata_2/KsponSpeech_04\n"
     ]
    }
   ],
   "source": [
    "cd KsponSpeech_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521bee0-623c-4026-afbe-f7d589431762",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2855ec07-49c9-4fff-aa9f-f2650bcc80b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KsponSpeech_04 데이터 전처리를 시작합니다...\n",
      "처리할 디렉토리: ./KsponSpeech_0450\n",
      "오디오 특성 저장 디렉토리: ./PreprocessData_04/audio_features\n",
      "G2P 텍스트 저장 디렉토리: ./PreprocessData_04/g2p_texts\n",
      "발견된 PCM 파일: 1000개\n",
      "발견된 TXT 파일: 1000개\n",
      "\n",
      "=== PCM 파일 처리 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 파일 처리: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [18:24<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 파일 처리 결과:\n",
      "  성공: 1000개\n",
      "  건너뜀 (이미 처리됨): 0개\n",
      "  오류: 0개\n",
      "\n",
      "=== TXT 파일 처리 시작 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT 파일 처리: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:27<00:00, 36.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 파일 처리 결과:\n",
      "  성공: 1000개\n",
      "  건너뜀 (이미 처리됨): 0개\n",
      "  오류: 0개\n",
      "\n",
      "=== 전체 처리 완료 ===\n",
      "전처리된 데이터 저장 위치: ./PreprocessData_04\n",
      "  - 오디오 특성 (NPZ): ./PreprocessData_04/audio_features\n",
      "  - G2P 텍스트: ./PreprocessData_04/g2p_texts\n",
      "\n",
      "=== 최종 통계 ===\n",
      "처리된 오디오 파일: 1000개\n",
      "처리된 텍스트 파일: 1000개\n",
      "✅ 오디오와 텍스트 파일 수가 일치합니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from g2pk import G2p\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "# G2p 초기화\n",
    "g2p = G2p()\n",
    "\n",
    "# 기본 디렉토리 경로 설정\n",
    "base_dir = \".\"  # 현재 디렉토리 (KsponSpeech_0450이 있는 곳)\n",
    "target_base_dir = \"./PreprocessData_04\"  # 전처리된 데이터를 저장할 기본 디렉토리\n",
    "\n",
    "# PCM 파일을 읽고 오디오 데이터로 변환하는 함수\n",
    "def read_pcm_file(pcm_file, sample_rate=16000, channels=1, bit_depth=16):\n",
    "    \"\"\"\n",
    "    PCM 파일을 읽어서 numpy 배열로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        pcm_file: PCM 파일 경로\n",
    "        sample_rate: 샘플링 레이트 (기본값: 16000)\n",
    "        channels: 채널 수 (기본값: 1, 모노)\n",
    "        bit_depth: 비트 깊이 (기본값: 16)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: 오디오 데이터\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(pcm_file, 'rb') as f:\n",
    "            # PCM 데이터 읽기\n",
    "            raw_data = f.read()\n",
    "        \n",
    "        # 16비트 PCM 데이터를 numpy 배열로 변환\n",
    "        if bit_depth == 16:\n",
    "            audio_data = np.frombuffer(raw_data, dtype=np.int16)\n",
    "        elif bit_depth == 32:\n",
    "            audio_data = np.frombuffer(raw_data, dtype=np.int32)\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 비트 깊이: {bit_depth}\")\n",
    "        \n",
    "        # 정규화 (-1.0 ~ 1.0 범위로)\n",
    "        if bit_depth == 16:\n",
    "            audio_data = audio_data.astype(np.float32) / 32768.0\n",
    "        elif bit_depth == 32:\n",
    "            audio_data = audio_data.astype(np.float32) / 2147483648.0\n",
    "        \n",
    "        return audio_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"PCM 파일 읽기 오류 ({pcm_file}): {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 오디오 특성 추출 함수\n",
    "def extract_audio_features(audio_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    오디오 데이터에서 다양한 특성을 추출합니다.\n",
    "    \n",
    "    Args:\n",
    "        audio_data: 오디오 데이터 (numpy array)\n",
    "        sample_rate: 샘플링 레이트\n",
    "    \n",
    "    Returns:\n",
    "        dict: 추출된 특성들\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. MFCC 특성 추출\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
    "        \n",
    "        # 2. 멜 스펙트로그램\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_mels=80)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "        \n",
    "        # 3. 스펙트럴 센트로이드 (음색의 밝기)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 4. 스펙트럴 롤오프\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 5. 제로 크로싱 레이트\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio_data)\n",
    "        \n",
    "        # 6. 크로마 특성\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 7. 피치(F0) 추출\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(audio_data, \n",
    "                                                   fmin=librosa.note_to_hz('C2'), \n",
    "                                                   fmax=librosa.note_to_hz('C7'))\n",
    "        \n",
    "        features = {\n",
    "            'mfcc': mfcc,\n",
    "            'mel_spectrogram': log_mel_spec,\n",
    "            'spectral_centroids': spectral_centroids,\n",
    "            'spectral_rolloff': spectral_rolloff,\n",
    "            'zero_crossing_rate': zcr,\n",
    "            'chroma': chroma,\n",
    "            'f0': f0,\n",
    "            'voiced_flag': voiced_flag,\n",
    "            'voiced_probs': voiced_probs,\n",
    "            'sample_rate': sample_rate,\n",
    "            'duration': len(audio_data) / sample_rate\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"특성 추출 오류: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 텍스트 파일 로드 함수 (다양한 인코딩 시도)\n",
    "def load_text(text_file):\n",
    "    \"\"\"다양한 인코딩을 시도하여 텍스트 파일을 로드합니다.\"\"\"\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(text_file, 'r', encoding=encoding) as f:\n",
    "                return f.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    # 모든 인코딩이 실패하면 바이너리 모드로 읽고 강제 디코딩\n",
    "    try:\n",
    "        with open(text_file, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            return raw_data.decode('latin1').encode('utf-8').decode('utf-8')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"모든 인코딩 시도 실패: {str(e)}\")\n",
    "\n",
    "# 디렉토리 내의 PCM 및 TXT 파일 경로 가져오기\n",
    "def get_file_paths(directory):\n",
    "    \"\"\"디렉토리 내의 모든 PCM 및 TXT 파일 경로를 반환합니다.\"\"\"\n",
    "    pcm_files = sorted(glob.glob(os.path.join(directory, \"*.pcm\")))\n",
    "    txt_files = sorted(glob.glob(os.path.join(directory, \"*.txt\")))\n",
    "    return pcm_files, txt_files\n",
    "\n",
    "# 메인 전처리 함수\n",
    "def preprocess_kspon_data():\n",
    "    \"\"\"KsponSpeech_04 데이터를 전처리합니다.\"\"\"\n",
    "    \n",
    "    # KsponSpeech_0450 디렉토리 처리\n",
    "    source_dir = \"./KsponSpeech_0450\"\n",
    "    \n",
    "    # 저장할 디렉토리들 생성\n",
    "    audio_target_dir = f\"{target_base_dir}/audio_features\"  # NPZ 파일들\n",
    "    text_target_dir = f\"{target_base_dir}/g2p_texts\"       # G2P 변환된 텍스트들\n",
    "    \n",
    "    print(f\"처리할 디렉토리: {source_dir}\")\n",
    "    print(f\"오디오 특성 저장 디렉토리: {audio_target_dir}\")\n",
    "    print(f\"G2P 텍스트 저장 디렉토리: {text_target_dir}\")\n",
    "    \n",
    "    # 소스 디렉토리가 존재하는지 확인\n",
    "    if not os.path.exists(source_dir):\n",
    "        print(f\"소스 디렉토리가 존재하지 않습니다: {source_dir}\")\n",
    "        return\n",
    "    \n",
    "    # 대상 디렉토리들 생성\n",
    "    os.makedirs(audio_target_dir, exist_ok=True)\n",
    "    os.makedirs(text_target_dir, exist_ok=True)\n",
    "    \n",
    "    # 파일 경로 가져오기\n",
    "    pcm_files, txt_files = get_file_paths(source_dir)\n",
    "    \n",
    "    print(f\"발견된 PCM 파일: {len(pcm_files)}개\")\n",
    "    print(f\"발견된 TXT 파일: {len(txt_files)}개\")\n",
    "    \n",
    "    # 통계 변수\n",
    "    audio_success = 0\n",
    "    audio_skip = 0\n",
    "    audio_error = 0\n",
    "    \n",
    "    text_success = 0\n",
    "    text_skip = 0\n",
    "    text_error = 0\n",
    "    \n",
    "    print(\"\\n=== PCM 파일 처리 시작 ===\")\n",
    "    \n",
    "    # PCM 파일들 처리 (오디오 특성 추출)\n",
    "    for pcm_file in tqdm(pcm_files, desc=\"PCM 파일 처리\"):\n",
    "        # 파일명 추출 (예: KsponSpeech_449656)\n",
    "        base_name = os.path.basename(pcm_file).split('.')[0]\n",
    "        target_file = os.path.join(audio_target_dir, f\"{base_name}.npz\")\n",
    "        \n",
    "        # 이미 처리된 파일인지 확인\n",
    "        if os.path.exists(target_file):\n",
    "            audio_skip += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # PCM 파일 읽기\n",
    "            audio_data = read_pcm_file(pcm_file)\n",
    "            \n",
    "            if audio_data is None:\n",
    "                audio_error += 1\n",
    "                continue\n",
    "            \n",
    "            # 오디오 특성 추출\n",
    "            features = extract_audio_features(audio_data)\n",
    "            \n",
    "            if features is None:\n",
    "                audio_error += 1\n",
    "                continue\n",
    "            \n",
    "            # NPZ 파일로 저장\n",
    "            np.savez_compressed(target_file, **features)\n",
    "            \n",
    "            audio_success += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            audio_error += 1\n",
    "            print(f\"오디오 처리 오류 ({base_name}): {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nPCM 파일 처리 결과:\")\n",
    "    print(f\"  성공: {audio_success}개\")\n",
    "    print(f\"  건너뜀 (이미 처리됨): {audio_skip}개\")\n",
    "    print(f\"  오류: {audio_error}개\")\n",
    "    \n",
    "    print(\"\\n=== TXT 파일 처리 시작 ===\")\n",
    "    \n",
    "    # TXT 파일들 처리 (G2P 변환)\n",
    "    for txt_file in tqdm(txt_files, desc=\"TXT 파일 처리\"):\n",
    "        # 파일명 추출 (예: KsponSpeech_449656)\n",
    "        base_name = os.path.basename(txt_file).split('.')[0]\n",
    "        target_file = os.path.join(text_target_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        # 이미 처리된 파일인지 확인\n",
    "        if os.path.exists(target_file):\n",
    "            text_skip += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 텍스트 로드\n",
    "            original_text = load_text(txt_file)\n",
    "            \n",
    "            # G2P 변환 적용\n",
    "            g2p_text = g2p(original_text)\n",
    "            \n",
    "            # 결과 저장\n",
    "            with open(target_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(g2p_text)\n",
    "            \n",
    "            text_success += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            text_error += 1\n",
    "            print(f\"텍스트 처리 오류 ({base_name}): {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nTXT 파일 처리 결과:\")\n",
    "    print(f\"  성공: {text_success}개\")\n",
    "    print(f\"  건너뜀 (이미 처리됨): {text_skip}개\")\n",
    "    print(f\"  오류: {text_error}개\")\n",
    "    \n",
    "    print(f\"\\n=== 전체 처리 완료 ===\")\n",
    "    print(f\"전처리된 데이터 저장 위치: {target_base_dir}\")\n",
    "    print(f\"  - 오디오 특성 (NPZ): {audio_target_dir}\")\n",
    "    print(f\"  - G2P 텍스트: {text_target_dir}\")\n",
    "    \n",
    "    # 매칭 확인\n",
    "    processed_audio_files = len([f for f in os.listdir(audio_target_dir) if f.endswith('.npz')])\n",
    "    processed_text_files = len([f for f in os.listdir(text_target_dir) if f.endswith('.txt')])\n",
    "    \n",
    "    print(f\"\\n=== 최종 통계 ===\")\n",
    "    print(f\"처리된 오디오 파일: {processed_audio_files}개\")\n",
    "    print(f\"처리된 텍스트 파일: {processed_text_files}개\")\n",
    "    \n",
    "    if processed_audio_files == processed_text_files:\n",
    "        print(\"✅ 오디오와 텍스트 파일 수가 일치합니다!\")\n",
    "    else:\n",
    "        print(\"⚠️ 오디오와 텍스트 파일 수가 일치하지 않습니다.\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"KsponSpeech_04 데이터 전처리를 시작합니다...\")\n",
    "    preprocess_kspon_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27e0d475-4322-42d9-8b55-8b341c6e24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mKsponSpeech_0373\u001b[0m/  \u001b[01;34mKsponSpeech_0405\u001b[0m/  \u001b[01;34mKsponSpeech_0437\u001b[0m/  \u001b[01;34mKsponSpeech_0469\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0374\u001b[0m/  \u001b[01;34mKsponSpeech_0406\u001b[0m/  \u001b[01;34mKsponSpeech_0438\u001b[0m/  \u001b[01;34mKsponSpeech_0470\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0375\u001b[0m/  \u001b[01;34mKsponSpeech_0407\u001b[0m/  \u001b[01;34mKsponSpeech_0439\u001b[0m/  \u001b[01;34mKsponSpeech_0471\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0376\u001b[0m/  \u001b[01;34mKsponSpeech_0408\u001b[0m/  \u001b[01;34mKsponSpeech_0440\u001b[0m/  \u001b[01;34mKsponSpeech_0472\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0377\u001b[0m/  \u001b[01;34mKsponSpeech_0409\u001b[0m/  \u001b[01;34mKsponSpeech_0441\u001b[0m/  \u001b[01;34mKsponSpeech_0473\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0378\u001b[0m/  \u001b[01;34mKsponSpeech_0410\u001b[0m/  \u001b[01;34mKsponSpeech_0442\u001b[0m/  \u001b[01;34mKsponSpeech_0474\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0379\u001b[0m/  \u001b[01;34mKsponSpeech_0411\u001b[0m/  \u001b[01;34mKsponSpeech_0443\u001b[0m/  \u001b[01;34mKsponSpeech_0475\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0380\u001b[0m/  \u001b[01;34mKsponSpeech_0412\u001b[0m/  \u001b[01;34mKsponSpeech_0444\u001b[0m/  \u001b[01;34mKsponSpeech_0476\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0381\u001b[0m/  \u001b[01;34mKsponSpeech_0413\u001b[0m/  \u001b[01;34mKsponSpeech_0445\u001b[0m/  \u001b[01;34mKsponSpeech_0477\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0382\u001b[0m/  \u001b[01;34mKsponSpeech_0414\u001b[0m/  \u001b[01;34mKsponSpeech_0446\u001b[0m/  \u001b[01;34mKsponSpeech_0478\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0383\u001b[0m/  \u001b[01;34mKsponSpeech_0415\u001b[0m/  \u001b[01;34mKsponSpeech_0447\u001b[0m/  \u001b[01;34mKsponSpeech_0479\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0384\u001b[0m/  \u001b[01;34mKsponSpeech_0416\u001b[0m/  \u001b[01;34mKsponSpeech_0448\u001b[0m/  \u001b[01;34mKsponSpeech_0480\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0385\u001b[0m/  \u001b[01;34mKsponSpeech_0417\u001b[0m/  \u001b[01;34mKsponSpeech_0449\u001b[0m/  \u001b[01;34mKsponSpeech_0481\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0386\u001b[0m/  \u001b[01;34mKsponSpeech_0418\u001b[0m/  \u001b[01;34mKsponSpeech_0450\u001b[0m/  \u001b[01;34mKsponSpeech_0482\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0387\u001b[0m/  \u001b[01;34mKsponSpeech_0419\u001b[0m/  \u001b[01;34mKsponSpeech_0451\u001b[0m/  \u001b[01;34mKsponSpeech_0483\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0388\u001b[0m/  \u001b[01;34mKsponSpeech_0420\u001b[0m/  \u001b[01;34mKsponSpeech_0452\u001b[0m/  \u001b[01;34mKsponSpeech_0484\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0389\u001b[0m/  \u001b[01;34mKsponSpeech_0421\u001b[0m/  \u001b[01;34mKsponSpeech_0453\u001b[0m/  \u001b[01;34mKsponSpeech_0485\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0390\u001b[0m/  \u001b[01;34mKsponSpeech_0422\u001b[0m/  \u001b[01;34mKsponSpeech_0454\u001b[0m/  \u001b[01;34mKsponSpeech_0486\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0391\u001b[0m/  \u001b[01;34mKsponSpeech_0423\u001b[0m/  \u001b[01;34mKsponSpeech_0455\u001b[0m/  \u001b[01;34mKsponSpeech_0487\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0392\u001b[0m/  \u001b[01;34mKsponSpeech_0424\u001b[0m/  \u001b[01;34mKsponSpeech_0456\u001b[0m/  \u001b[01;34mKsponSpeech_0488\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0393\u001b[0m/  \u001b[01;34mKsponSpeech_0425\u001b[0m/  \u001b[01;34mKsponSpeech_0457\u001b[0m/  \u001b[01;34mKsponSpeech_0489\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0394\u001b[0m/  \u001b[01;34mKsponSpeech_0426\u001b[0m/  \u001b[01;34mKsponSpeech_0458\u001b[0m/  \u001b[01;34mKsponSpeech_0490\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0395\u001b[0m/  \u001b[01;34mKsponSpeech_0427\u001b[0m/  \u001b[01;34mKsponSpeech_0459\u001b[0m/  \u001b[01;34mKsponSpeech_0491\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0396\u001b[0m/  \u001b[01;34mKsponSpeech_0428\u001b[0m/  \u001b[01;34mKsponSpeech_0460\u001b[0m/  \u001b[01;34mKsponSpeech_0492\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0397\u001b[0m/  \u001b[01;34mKsponSpeech_0429\u001b[0m/  \u001b[01;34mKsponSpeech_0461\u001b[0m/  \u001b[01;34mKsponSpeech_0493\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0398\u001b[0m/  \u001b[01;34mKsponSpeech_0430\u001b[0m/  \u001b[01;34mKsponSpeech_0462\u001b[0m/  \u001b[01;34mKsponSpeech_0494\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0399\u001b[0m/  \u001b[01;34mKsponSpeech_0431\u001b[0m/  \u001b[01;34mKsponSpeech_0463\u001b[0m/  \u001b[01;34mKsponSpeech_0495\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0400\u001b[0m/  \u001b[01;34mKsponSpeech_0432\u001b[0m/  \u001b[01;34mKsponSpeech_0464\u001b[0m/  \u001b[01;34mKsponSpeech_0496\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0401\u001b[0m/  \u001b[01;34mKsponSpeech_0433\u001b[0m/  \u001b[01;34mKsponSpeech_0465\u001b[0m/  \u001b[01;34mPreprocessData_04\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0402\u001b[0m/  \u001b[01;34mKsponSpeech_0434\u001b[0m/  \u001b[01;34mKsponSpeech_0466\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0403\u001b[0m/  \u001b[01;34mKsponSpeech_0435\u001b[0m/  \u001b[01;34mKsponSpeech_0467\u001b[0m/\n",
      "\u001b[01;34mKsponSpeech_0404\u001b[0m/  \u001b[01;34mKsponSpeech_0436\u001b[0m/  \u001b[01;34mKsponSpeech_0468\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e999e875-7125-4336-8376-334411267e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KsponSpeech_04 데이터 전처리를 시작합니다...\n",
      "발견된 KsponSpeech 폴더들: 123개\n",
      "  - KsponSpeech_0373\n",
      "  - KsponSpeech_0374\n",
      "  - KsponSpeech_0375\n",
      "  - KsponSpeech_0376\n",
      "  - KsponSpeech_0377\n",
      "  - KsponSpeech_0378\n",
      "  - KsponSpeech_0379\n",
      "  - KsponSpeech_0380\n",
      "  - KsponSpeech_0381\n",
      "  - KsponSpeech_0382\n",
      "  - KsponSpeech_0383\n",
      "  - KsponSpeech_0384\n",
      "  - KsponSpeech_0385\n",
      "  - KsponSpeech_0386\n",
      "  - KsponSpeech_0387\n",
      "  - KsponSpeech_0388\n",
      "  - KsponSpeech_0389\n",
      "  - KsponSpeech_0390\n",
      "  - KsponSpeech_0391\n",
      "  - KsponSpeech_0392\n",
      "  - KsponSpeech_0393\n",
      "  - KsponSpeech_0394\n",
      "  - KsponSpeech_0395\n",
      "  - KsponSpeech_0396\n",
      "  - KsponSpeech_0397\n",
      "  - KsponSpeech_0398\n",
      "  - KsponSpeech_0399\n",
      "  - KsponSpeech_0400\n",
      "  - KsponSpeech_0401\n",
      "  - KsponSpeech_0402\n",
      "  - KsponSpeech_0403\n",
      "  - KsponSpeech_0404\n",
      "  - KsponSpeech_0405\n",
      "  - KsponSpeech_0406\n",
      "  - KsponSpeech_0407\n",
      "  - KsponSpeech_0408\n",
      "  - KsponSpeech_0409\n",
      "  - KsponSpeech_0410\n",
      "  - KsponSpeech_0411\n",
      "  - KsponSpeech_0412\n",
      "  - KsponSpeech_0413\n",
      "  - KsponSpeech_0414\n",
      "  - KsponSpeech_0415\n",
      "  - KsponSpeech_0416\n",
      "  - KsponSpeech_0417\n",
      "  - KsponSpeech_0418\n",
      "  - KsponSpeech_0419\n",
      "  - KsponSpeech_0420\n",
      "  - KsponSpeech_0421\n",
      "  - KsponSpeech_0422\n",
      "  - KsponSpeech_0423\n",
      "  - KsponSpeech_0424\n",
      "  - KsponSpeech_0425\n",
      "  - KsponSpeech_0426\n",
      "  - KsponSpeech_0427\n",
      "  - KsponSpeech_0428\n",
      "  - KsponSpeech_0429\n",
      "  - KsponSpeech_0430\n",
      "  - KsponSpeech_0431\n",
      "  - KsponSpeech_0432\n",
      "  - KsponSpeech_0433\n",
      "  - KsponSpeech_0434\n",
      "  - KsponSpeech_0435\n",
      "  - KsponSpeech_0436\n",
      "  - KsponSpeech_0437\n",
      "  - KsponSpeech_0438\n",
      "  - KsponSpeech_0439\n",
      "  - KsponSpeech_0440\n",
      "  - KsponSpeech_0441\n",
      "  - KsponSpeech_0442\n",
      "  - KsponSpeech_0443\n",
      "  - KsponSpeech_0444\n",
      "  - KsponSpeech_0445\n",
      "  - KsponSpeech_0446\n",
      "  - KsponSpeech_0447\n",
      "  - KsponSpeech_0448\n",
      "  - KsponSpeech_0449\n",
      "  - KsponSpeech_0451\n",
      "  - KsponSpeech_0452\n",
      "  - KsponSpeech_0453\n",
      "  - KsponSpeech_0454\n",
      "  - KsponSpeech_0455\n",
      "  - KsponSpeech_0456\n",
      "  - KsponSpeech_0457\n",
      "  - KsponSpeech_0458\n",
      "  - KsponSpeech_0459\n",
      "  - KsponSpeech_0460\n",
      "  - KsponSpeech_0461\n",
      "  - KsponSpeech_0462\n",
      "  - KsponSpeech_0463\n",
      "  - KsponSpeech_0464\n",
      "  - KsponSpeech_0465\n",
      "  - KsponSpeech_0466\n",
      "  - KsponSpeech_0467\n",
      "  - KsponSpeech_0468\n",
      "  - KsponSpeech_0469\n",
      "  - KsponSpeech_0470\n",
      "  - KsponSpeech_0471\n",
      "  - KsponSpeech_0472\n",
      "  - KsponSpeech_0473\n",
      "  - KsponSpeech_0474\n",
      "  - KsponSpeech_0475\n",
      "  - KsponSpeech_0476\n",
      "  - KsponSpeech_0477\n",
      "  - KsponSpeech_0478\n",
      "  - KsponSpeech_0479\n",
      "  - KsponSpeech_0480\n",
      "  - KsponSpeech_0481\n",
      "  - KsponSpeech_0482\n",
      "  - KsponSpeech_0483\n",
      "  - KsponSpeech_0484\n",
      "  - KsponSpeech_0485\n",
      "  - KsponSpeech_0486\n",
      "  - KsponSpeech_0487\n",
      "  - KsponSpeech_0488\n",
      "  - KsponSpeech_0489\n",
      "  - KsponSpeech_0490\n",
      "  - KsponSpeech_0491\n",
      "  - KsponSpeech_0492\n",
      "  - KsponSpeech_0493\n",
      "  - KsponSpeech_0494\n",
      "  - KsponSpeech_0495\n",
      "  - KsponSpeech_0496\n",
      "\n",
      "전처리된 데이터 저장 위치: ./PreprocessData_04\n",
      "  - 오디오 특성 (NPZ): ./PreprocessData_04/audio_features\n",
      "  - G2P 텍스트: ./PreprocessData_04/g2p_texts\n",
      "\n",
      "================================================================================\n",
      "KsponSpeech_04 전체 폴더 처리 시작\n",
      "================================================================================\n",
      "\n",
      "[1/123] KsponSpeech_0373 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 파일 처리 (KsponSpeech_0373): 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [17:40<00:00,  1.06s/it]\n",
      "TXT 파일 처리 (KsponSpeech_0373): 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:27<00:00, 36.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  KsponSpeech_0373 완료:\n",
      "    파일 수: PCM 1000개, TXT 1000개\n",
      "    오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "\n",
      "[2/123] KsponSpeech_0374 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 파일 처리 (KsponSpeech_0374): 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [17:08<00:00,  1.03s/it]\n",
      "TXT 파일 처리 (KsponSpeech_0374): 100%|███████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:27<00:00, 36.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  KsponSpeech_0374 완료:\n",
      "    파일 수: PCM 1000개, TXT 1000개\n",
      "    오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "\n",
      "[3/123] KsponSpeech_0375 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 파일 처리 (KsponSpeech_0375):  15%|████████████▏                                                                   | 152/1000 [02:29<13:55,  1.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 332\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKsponSpeech_04 데이터 전처리를 시작합니다...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 332\u001b[0m     \u001b[43mpreprocess_kspon_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 279\u001b[0m, in \u001b[0;36mpreprocess_kspon_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# 단일 폴더 처리\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_single_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_target_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_target_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m total_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# 통계 누적\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 176\u001b[0m, in \u001b[0;36mprocess_single_folder\u001b[0;34m(source_dir, audio_target_dir, text_target_dir)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# 오디오 특성 추출\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_audio_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     audio_error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 73\u001b[0m, in \u001b[0;36mextract_audio_features\u001b[0;34m(audio_data, sample_rate)\u001b[0m\n\u001b[1;32m     70\u001b[0m mfcc \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mmfcc(y\u001b[38;5;241m=\u001b[39maudio_data, sr\u001b[38;5;241m=\u001b[39msample_rate, n_mfcc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# 2. 멜 스펙트로그램\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m mel_spec \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmelspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_mels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m log_mel_spec \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpower_to_db(mel_spec)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 3. 스펙트럴 센트로이드 (음색의 밝기)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/feature/spectral.py:2135\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmelspectrogram\u001b[39m(\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   2015\u001b[0m     y: Optional[np\u001b[38;5;241m.\u001b[39mndarray] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2025\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   2026\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m   2027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute a mel-scaled spectrogram.\u001b[39;00m\n\u001b[1;32m   2028\u001b[0m \n\u001b[1;32m   2029\u001b[0m \u001b[38;5;124;03m    If a spectrogram input ``S`` is provided, then it is mapped directly onto\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;124;03m    >>> ax.set(title='Mel-frequency spectrogram')\u001b[39;00m\n\u001b[1;32m   2134\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2135\u001b[0m     S, n_fft \u001b[38;5;241m=\u001b[39m \u001b[43m_spectrogram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpower\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2147\u001b[0m     \u001b[38;5;66;03m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m     mel_basis \u001b[38;5;241m=\u001b[39m filters\u001b[38;5;241m.\u001b[39mmel(sr\u001b[38;5;241m=\u001b[39msr, n_fft\u001b[38;5;241m=\u001b[39mn_fft, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/core/spectrum.py:2945\u001b[0m, in \u001b[0;36m_spectrogram\u001b[0;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2940\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ParameterError(\n\u001b[1;32m   2941\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2942\u001b[0m         )\n\u001b[1;32m   2943\u001b[0m     S \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2944\u001b[0m         np\u001b[38;5;241m.\u001b[39mabs(\n\u001b[0;32m-> 2945\u001b[0m             \u001b[43mstft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2946\u001b[0m \u001b[43m                \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2947\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_fft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2948\u001b[0m \u001b[43m                \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2949\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2950\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2951\u001b[0m \u001b[43m                \u001b[49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2952\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpad_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2953\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2954\u001b[0m         )\n\u001b[1;32m   2955\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m power\n\u001b[1;32m   2956\u001b[0m     )\n\u001b[1;32m   2958\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m S, n_fft\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/librosa/core/spectrum.py:387\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m bl_s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], n_columns):\n\u001b[1;32m    385\u001b[0m     bl_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(bl_s \u001b[38;5;241m+\u001b[39m n_columns, y_frames\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 387\u001b[0m     stft_matrix[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, bl_s \u001b[38;5;241m+\u001b[39m off_start : bl_t \u001b[38;5;241m+\u001b[39m off_start] \u001b[38;5;241m=\u001b[39m \u001b[43mfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfft_window\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_frames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbl_s\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbl_t\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stft_matrix\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/fft/_backend.py:25\u001b[0m, in \u001b[0;36m_ScipyBackend.__ua_function__\u001b[0;34m(method, args, kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/fft/_pocketfft/basic.py:62\u001b[0m, in \u001b[0;36mr2c\u001b[0;34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid number of data points (\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m) specified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                      \u001b[38;5;241m.\u001b[39mformat(tmp\u001b[38;5;241m.\u001b[39mshape[axis]))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Note: overwrite_x is not utilised\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from g2pk import G2p\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "from pathlib import Path\n",
    "\n",
    "# G2p 초기화\n",
    "g2p = G2p()\n",
    "\n",
    "# 기본 디렉토리 경로 설정\n",
    "base_dir = \".\"  # 현재 디렉토리 (KsponSpeech_0450이 있는 곳)\n",
    "target_base_dir = \"./PreprocessData_04\"  # 전처리된 데이터를 저장할 기본 디렉토리\n",
    "\n",
    "# PCM 파일을 읽고 오디오 데이터로 변환하는 함수\n",
    "def read_pcm_file(pcm_file, sample_rate=16000, channels=1, bit_depth=16):\n",
    "    \"\"\"\n",
    "    PCM 파일을 읽어서 numpy 배열로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        pcm_file: PCM 파일 경로\n",
    "        sample_rate: 샘플링 레이트 (기본값: 16000)\n",
    "        channels: 채널 수 (기본값: 1, 모노)\n",
    "        bit_depth: 비트 깊이 (기본값: 16)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array: 오디오 데이터\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(pcm_file, 'rb') as f:\n",
    "            # PCM 데이터 읽기\n",
    "            raw_data = f.read()\n",
    "        \n",
    "        # 16비트 PCM 데이터를 numpy 배열로 변환\n",
    "        if bit_depth == 16:\n",
    "            audio_data = np.frombuffer(raw_data, dtype=np.int16)\n",
    "        elif bit_depth == 32:\n",
    "            audio_data = np.frombuffer(raw_data, dtype=np.int32)\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 비트 깊이: {bit_depth}\")\n",
    "        \n",
    "        # 정규화 (-1.0 ~ 1.0 범위로)\n",
    "        if bit_depth == 16:\n",
    "            audio_data = audio_data.astype(np.float32) / 32768.0\n",
    "        elif bit_depth == 32:\n",
    "            audio_data = audio_data.astype(np.float32) / 2147483648.0\n",
    "        \n",
    "        return audio_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"PCM 파일 읽기 오류 ({pcm_file}): {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 오디오 특성 추출 함수\n",
    "def extract_audio_features(audio_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    오디오 데이터에서 다양한 특성을 추출합니다.\n",
    "    \n",
    "    Args:\n",
    "        audio_data: 오디오 데이터 (numpy array)\n",
    "        sample_rate: 샘플링 레이트\n",
    "    \n",
    "    Returns:\n",
    "        dict: 추출된 특성들\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. MFCC 특성 추출\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
    "        \n",
    "        # 2. 멜 스펙트로그램\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_mels=80)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "        \n",
    "        # 3. 스펙트럴 센트로이드 (음색의 밝기)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 4. 스펙트럴 롤오프\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 5. 제로 크로싱 레이트\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio_data)\n",
    "        \n",
    "        # 6. 크로마 특성\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 7. 피치(F0) 추출\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(audio_data, \n",
    "                                                   fmin=librosa.note_to_hz('C2'), \n",
    "                                                   fmax=librosa.note_to_hz('C7'))\n",
    "        \n",
    "        features = {\n",
    "            'mfcc': mfcc,\n",
    "            'mel_spectrogram': log_mel_spec,\n",
    "            'spectral_centroids': spectral_centroids,\n",
    "            'spectral_rolloff': spectral_rolloff,\n",
    "            'zero_crossing_rate': zcr,\n",
    "            'chroma': chroma,\n",
    "            'f0': f0,\n",
    "            'voiced_flag': voiced_flag,\n",
    "            'voiced_probs': voiced_probs,\n",
    "            'sample_rate': sample_rate,\n",
    "            'duration': len(audio_data) / sample_rate\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"특성 추출 오류: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 텍스트 파일 로드 함수 (다양한 인코딩 시도)\n",
    "def load_text(text_file):\n",
    "    \"\"\"다양한 인코딩을 시도하여 텍스트 파일을 로드합니다.\"\"\"\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(text_file, 'r', encoding=encoding) as f:\n",
    "                return f.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    # 모든 인코딩이 실패하면 바이너리 모드로 읽고 강제 디코딩\n",
    "    try:\n",
    "        with open(text_file, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            return raw_data.decode('latin1').encode('utf-8').decode('utf-8')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"모든 인코딩 시도 실패: {str(e)}\")\n",
    "\n",
    "# 디렉토리 내의 PCM 및 TXT 파일 경로 가져오기\n",
    "def get_file_paths(directory):\n",
    "    \"\"\"디렉토리 내의 모든 PCM 및 TXT 파일 경로를 반환합니다.\"\"\"\n",
    "    pcm_files = sorted(glob.glob(os.path.join(directory, \"*.pcm\")))\n",
    "    txt_files = sorted(glob.glob(os.path.join(directory, \"*.txt\")))\n",
    "    return pcm_files, txt_files\n",
    "\n",
    "# 단일 폴더 처리 함수\n",
    "def process_single_folder(source_dir, audio_target_dir, text_target_dir):\n",
    "    \"\"\"단일 폴더를 처리합니다.\"\"\"\n",
    "    \n",
    "    # 파일 경로 가져오기\n",
    "    pcm_files, txt_files = get_file_paths(source_dir)\n",
    "    \n",
    "    # 통계 변수\n",
    "    audio_success = 0\n",
    "    audio_skip = 0\n",
    "    audio_error = 0\n",
    "    \n",
    "    text_success = 0\n",
    "    text_skip = 0\n",
    "    text_error = 0\n",
    "    \n",
    "    # PCM 파일들 처리 (오디오 특성 추출)\n",
    "    for pcm_file in tqdm(pcm_files, desc=f\"PCM 파일 처리 ({os.path.basename(source_dir)})\"):\n",
    "        # 파일명 추출 (예: KsponSpeech_449656)\n",
    "        base_name = os.path.basename(pcm_file).split('.')[0]\n",
    "        target_file = os.path.join(audio_target_dir, f\"{base_name}.npz\")\n",
    "        \n",
    "        # 이미 처리된 파일인지 확인\n",
    "        if os.path.exists(target_file):\n",
    "            audio_skip += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # PCM 파일 읽기\n",
    "            audio_data = read_pcm_file(pcm_file)\n",
    "            \n",
    "            if audio_data is None:\n",
    "                audio_error += 1\n",
    "                continue\n",
    "            \n",
    "            # 오디오 특성 추출\n",
    "            features = extract_audio_features(audio_data)\n",
    "            \n",
    "            if features is None:\n",
    "                audio_error += 1\n",
    "                continue\n",
    "            \n",
    "            # NPZ 파일로 저장\n",
    "            np.savez_compressed(target_file, **features)\n",
    "            \n",
    "            audio_success += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            audio_error += 1\n",
    "            print(f\"오디오 처리 오류 ({base_name}): {str(e)}\")\n",
    "    \n",
    "    # TXT 파일들 처리 (G2P 변환)\n",
    "    for txt_file in tqdm(txt_files, desc=f\"TXT 파일 처리 ({os.path.basename(source_dir)})\"):\n",
    "        # 파일명 추출 (예: KsponSpeech_449656)\n",
    "        base_name = os.path.basename(txt_file).split('.')[0]\n",
    "        target_file = os.path.join(text_target_dir, f\"{base_name}.txt\")\n",
    "        \n",
    "        # 이미 처리된 파일인지 확인\n",
    "        if os.path.exists(target_file):\n",
    "            text_skip += 1\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # 텍스트 로드\n",
    "            original_text = load_text(txt_file)\n",
    "            \n",
    "            # G2P 변환 적용\n",
    "            g2p_text = g2p(original_text)\n",
    "            \n",
    "            # 결과 저장\n",
    "            with open(target_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(g2p_text)\n",
    "            \n",
    "            text_success += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            text_error += 1\n",
    "            print(f\"텍스트 처리 오류 ({base_name}): {str(e)}\")\n",
    "    \n",
    "    return {\n",
    "        'folder': os.path.basename(source_dir),\n",
    "        'pcm_files': len(pcm_files),\n",
    "        'txt_files': len(txt_files),\n",
    "        'audio_success': audio_success,\n",
    "        'audio_skip': audio_skip,\n",
    "        'audio_error': audio_error,\n",
    "        'text_success': text_success,\n",
    "        'text_skip': text_skip,\n",
    "        'text_error': text_error\n",
    "    }\n",
    "\n",
    "# 메인 전처리 함수\n",
    "def preprocess_kspon_data():\n",
    "    \"\"\"KsponSpeech_04 모든 폴더의 데이터를 전처리합니다.\"\"\"\n",
    "    \n",
    "    # 현재 디렉토리에서 KsponSpeech_로 시작하는 모든 폴더 찾기 (0450 제외)\n",
    "    all_folders = sorted([d for d in os.listdir('.') if d.startswith('KsponSpeech_') and os.path.isdir(d) and d != 'KsponSpeech_0450'])\n",
    "    \n",
    "    print(f\"발견된 KsponSpeech 폴더들: {len(all_folders)}개\")\n",
    "    for folder in all_folders:\n",
    "        print(f\"  - {folder}\")\n",
    "    \n",
    "    # 저장할 디렉토리들 생성\n",
    "    audio_target_dir = f\"{target_base_dir}/audio_features\"  # NPZ 파일들\n",
    "    text_target_dir = f\"{target_base_dir}/g2p_texts\"       # G2P 변환된 텍스트들\n",
    "    \n",
    "    print(f\"\\n전처리된 데이터 저장 위치: {target_base_dir}\")\n",
    "    print(f\"  - 오디오 특성 (NPZ): {audio_target_dir}\")\n",
    "    print(f\"  - G2P 텍스트: {text_target_dir}\")\n",
    "    \n",
    "    # 대상 디렉토리들 생성\n",
    "    os.makedirs(audio_target_dir, exist_ok=True)\n",
    "    os.makedirs(text_target_dir, exist_ok=True)\n",
    "    \n",
    "    # 전체 통계\n",
    "    total_results = []\n",
    "    total_audio_success = 0\n",
    "    total_audio_skip = 0\n",
    "    total_audio_error = 0\n",
    "    total_text_success = 0\n",
    "    total_text_skip = 0\n",
    "    total_text_error = 0\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KsponSpeech_04 전체 폴더 처리 시작\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 각 폴더 처리\n",
    "    for i, folder in enumerate(all_folders, 1):\n",
    "        source_dir = f\"./{folder}\"\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(all_folders)}] {folder} 처리 중...\")\n",
    "        \n",
    "        # 소스 디렉토리가 존재하는지 확인\n",
    "        if not os.path.exists(source_dir):\n",
    "            print(f\"소스 디렉토리가 존재하지 않습니다: {source_dir}, 건너뜁니다.\")\n",
    "            continue\n",
    "        \n",
    "        # 단일 폴더 처리\n",
    "        result = process_single_folder(source_dir, audio_target_dir, text_target_dir)\n",
    "        total_results.append(result)\n",
    "        \n",
    "        # 통계 누적\n",
    "        total_audio_success += result['audio_success']\n",
    "        total_audio_skip += result['audio_skip']\n",
    "        total_audio_error += result['audio_error']\n",
    "        total_text_success += result['text_success']\n",
    "        total_text_skip += result['text_skip']\n",
    "        total_text_error += result['text_error']\n",
    "        \n",
    "        print(f\"  {folder} 완료:\")\n",
    "        print(f\"    파일 수: PCM {result['pcm_files']}개, TXT {result['txt_files']}개\")\n",
    "        print(f\"    오디오: 성공 {result['audio_success']}, 건너뜀 {result['audio_skip']}, 오류 {result['audio_error']}\")\n",
    "        print(f\"    텍스트: 성공 {result['text_success']}, 건너뜀 {result['text_skip']}, 오류 {result['text_error']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"전체 처리 완료!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n=== 전체 통계 ===\")\n",
    "    print(f\"처리된 폴더 수: {len(total_results)}개\")\n",
    "    print(f\"오디오 파일:\")\n",
    "    print(f\"  성공: {total_audio_success}개\")\n",
    "    print(f\"  건너뜀 (이미 처리됨): {total_audio_skip}개\")\n",
    "    print(f\"  오류: {total_audio_error}개\")\n",
    "    print(f\"텍스트 파일:\")\n",
    "    print(f\"  성공: {total_text_success}개\")\n",
    "    print(f\"  건너뜀 (이미 처리됨): {total_text_skip}개\")\n",
    "    print(f\"  오류: {total_text_error}개\")\n",
    "    \n",
    "    # 최종 파일 수 확인\n",
    "    try:\n",
    "        processed_audio_files = len([f for f in os.listdir(audio_target_dir) if f.endswith('.npz')])\n",
    "        processed_text_files = len([f for f in os.listdir(text_target_dir) if f.endswith('.txt')])\n",
    "        \n",
    "        print(f\"\\n=== 최종 결과 ===\")\n",
    "        print(f\"저장된 오디오 특성 파일: {processed_audio_files}개\")\n",
    "        print(f\"저장된 G2P 텍스트 파일: {processed_text_files}개\")\n",
    "        \n",
    "        if processed_audio_files == processed_text_files:\n",
    "            print(\"✅ 오디오와 텍스트 파일 수가 일치합니다!\")\n",
    "        else:\n",
    "            print(\"⚠️ 오디오와 텍스트 파일 수가 일치하지 않습니다.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"최종 파일 수 확인 중 오류: {str(e)}\")\n",
    "    \n",
    "    return total_results\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"KsponSpeech_04 데이터 전처리를 시작합니다...\")\n",
    "    preprocess_kspon_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e36b47d-95b8-415b-be00-bcdf799dfad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 KsponSpeech_04 멀티프로세싱 전처리\n",
      "선택하세요:\n",
      "1. 전체 처리 - run_full_processing()\n",
      "2. 테스트 (20개) - run_partial_processing(20)\n",
      "3. 재시작 - run_resume_processing('KsponSpeech_0392')\n",
      "🔄 KsponSpeech_0392부터 재시작\n",
      "🖥️ 시스템 정보:\n",
      "  - 총 CPU 코어: 48개\n",
      "  - 사용할 프로세스: 8개\n",
      "📍 KsponSpeech_0392부터 재시작\n",
      "📁 처리할 폴더: 105개\n",
      "\n",
      "💾 저장 위치: ./PreprocessData_04\n",
      "  - 오디오 특성 (NPZ): ./PreprocessData_04/audio_features\n",
      "  - G2P 텍스트: ./PreprocessData_04/g2p_texts\n",
      "\n",
      "================================================================================\n",
      "🚀 KsponSpeech_04 멀티프로세싱 전처리 시작\n",
      "================================================================================\n",
      "\n",
      "[1/105] 📂 KsponSpeech_0392 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0392): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 32585.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0392): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 30476.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0392 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[2/105] 📂 KsponSpeech_0393 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0393): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 27425.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0393): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 31823.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0393 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[3/105] 📂 KsponSpeech_0394 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0394): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 29115.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0394): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 29631.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0394 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[4/105] 📂 KsponSpeech_0395 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0395): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 28409.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0395): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 24776.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0395 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[5/105] 📂 KsponSpeech_0396 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0396): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 26893.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0396): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 26656.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0396 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[6/105] 📂 KsponSpeech_0397 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0397): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 34724.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0397): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 25585.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0397 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[7/105] 📂 KsponSpeech_0398 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0398): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 32024.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0398): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 28738.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0398 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[8/105] 📂 KsponSpeech_0399 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0399): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 32507.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0399): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 31025.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0399 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[9/105] 📂 KsponSpeech_0400 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0400): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 30761.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0400): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 43146.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0400 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[10/105] 📂 KsponSpeech_0401 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0401): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 34939.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0401): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 34570.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0401 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[11/105] 📂 KsponSpeech_0402 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0402): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 31345.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT 처리 (KsponSpeech_0402): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 26726.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0402 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[12/105] 📂 KsponSpeech_0403 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0403): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 27000.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0403): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 33293.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0403 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[13/105] 📂 KsponSpeech_0404 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0404): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 27033.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0404): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 33152.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0404 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[14/105] 📂 KsponSpeech_0405 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0405): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 27469.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0405): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 34387.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0405 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[15/105] 📂 KsponSpeech_0406 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0406): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 27086.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0406): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 29556.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0406 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[16/105] 📂 KsponSpeech_0407 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0407): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 31187.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0407): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 29575.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0407 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[17/105] 📂 KsponSpeech_0408 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0408): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 25236.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0408): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 27443.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0408 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[18/105] 📂 KsponSpeech_0409 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0409): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 29853.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0409): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 33597.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0409 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.0시간\n",
      "\n",
      "[19/105] 📂 KsponSpeech_0410 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0410): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [03:48<00:00,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0410): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0410 완료 (소요시간: 314.9초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 828, 건너뜀 172, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.4시간\n",
      "\n",
      "[20/105] 📂 KsponSpeech_0411 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0411): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:24<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT 처리 (KsponSpeech_0411): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0411 완료 (소요시간: 349.3초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 0.8시간\n",
      "\n",
      "[21/105] 📂 KsponSpeech_0412 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0412): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:17<00:00,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0412): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0412 완료 (소요시간: 342.5초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 1.1시간\n",
      "\n",
      "[22/105] 📂 KsponSpeech_0413 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0413): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:24<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0413): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0413 완료 (소요시간: 349.5초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 1.4시간\n",
      "\n",
      "[23/105] 📂 KsponSpeech_0414 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0414): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:12<00:00,  3.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0414): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0414 완료 (소요시간: 337.0초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 1.7시간\n",
      "\n",
      "[24/105] 📂 KsponSpeech_0415 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0415): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:09<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0415): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0415 완료 (소요시간: 335.4초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 1.9시간\n",
      "\n",
      "[25/105] 📂 KsponSpeech_0416 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0416): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:18<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0416): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0416 완료 (소요시간: 344.4초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.1시간\n",
      "\n",
      "[26/105] 📂 KsponSpeech_0417 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0417): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:13<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0417): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0417 완료 (소요시간: 338.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.3시간\n",
      "\n",
      "[27/105] 📂 KsponSpeech_0418 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0418): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:23<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0418): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0418 완료 (소요시간: 349.7초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.5시간\n",
      "\n",
      "[28/105] 📂 KsponSpeech_0419 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0419): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:06<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0419): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0419 완료 (소요시간: 332.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.6시간\n",
      "\n",
      "[29/105] 📂 KsponSpeech_0420 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0420): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:13<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0420): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0420 완료 (소요시간: 338.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.7시간\n",
      "\n",
      "[30/105] 📂 KsponSpeech_0421 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0421): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0421): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0421 완료 (소요시간: 339.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.8시간\n",
      "\n",
      "[31/105] 📂 KsponSpeech_0422 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0422): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0422): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0422 완료 (소요시간: 340.1초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.9시간\n",
      "\n",
      "[32/105] 📂 KsponSpeech_0423 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0423): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:02<00:00,  4.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0423): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0423 완료 (소요시간: 327.7초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.0시간\n",
      "\n",
      "[33/105] 📂 KsponSpeech_0424 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0424): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:19<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0424): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0424 완료 (소요시간: 345.7초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.1시간\n",
      "\n",
      "[34/105] 📂 KsponSpeech_0425 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0425): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:13<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0425): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0425 완료 (소요시간: 339.4초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.1시간\n",
      "\n",
      "[35/105] 📂 KsponSpeech_0426 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0426): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:07<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0426): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0426 완료 (소요시간: 333.1초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.2시간\n",
      "\n",
      "[36/105] 📂 KsponSpeech_0427 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0427): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0427): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0427 완료 (소요시간: 339.6초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.2시간\n",
      "\n",
      "[37/105] 📂 KsponSpeech_0428 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0428): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:07<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0428): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0428 완료 (소요시간: 333.1초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.3시간\n",
      "\n",
      "[38/105] 📂 KsponSpeech_0429 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0429): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:18<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0429): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0429 완료 (소요시간: 343.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.3시간\n",
      "\n",
      "[39/105] 📂 KsponSpeech_0430 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0430): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:09<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0430): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0430 완료 (소요시간: 336.0초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.3시간\n",
      "\n",
      "[40/105] 📂 KsponSpeech_0431 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0431): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:20<00:00,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0431): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0431 완료 (소요시간: 345.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[41/105] 📂 KsponSpeech_0432 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0432): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:17<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0432): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0432 완료 (소요시간: 343.5초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[42/105] 📂 KsponSpeech_0433 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0433): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:09<00:00,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0433): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0433 완료 (소요시간: 334.6초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[43/105] 📂 KsponSpeech_0434 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0434): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:02<00:00,  4.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0434): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0434 완료 (소요시간: 327.5초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[44/105] 📂 KsponSpeech_0435 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0435): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:12<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TXT 처리 (KsponSpeech_0435): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0435 완료 (소요시간: 338.0초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[45/105] 📂 KsponSpeech_0436 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0436): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:05<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0436): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0436 완료 (소요시간: 331.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[46/105] 📂 KsponSpeech_0437 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0437): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:18<00:00,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0437): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0437 완료 (소요시간: 344.0초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[47/105] 📂 KsponSpeech_0438 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0438): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0438): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0438 완료 (소요시간: 339.3초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[48/105] 📂 KsponSpeech_0439 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0439): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0439): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0439 완료 (소요시간: 339.5초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.4시간\n",
      "\n",
      "[49/105] 📂 KsponSpeech_0440 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0440): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:11<00:00,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0440): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0440 완료 (소요시간: 336.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.3시간\n",
      "\n",
      "[50/105] 📂 KsponSpeech_0441 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0441): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:16<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0441): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0441 완료 (소요시간: 342.9초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.3시간\n",
      "\n",
      "[51/105] 📂 KsponSpeech_0442 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0442): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:13<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0442): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0442 완료 (소요시간: 338.6초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.3시간\n",
      "\n",
      "[52/105] 📂 KsponSpeech_0443 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0443): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:13<00:00,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0443): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0443 완료 (소요시간: 338.7초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.3시간\n",
      "\n",
      "[53/105] 📂 KsponSpeech_0444 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0444): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:15<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0444): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0444 완료 (소요시간: 340.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.2시간\n",
      "\n",
      "[54/105] 📂 KsponSpeech_0445 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0445): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:10<00:00,  4.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0445): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0445 완료 (소요시간: 335.3초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.2시간\n",
      "\n",
      "[55/105] 📂 KsponSpeech_0446 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0446): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:18<00:00,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0446): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0446 완료 (소요시간: 344.1초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.2시간\n",
      "\n",
      "[56/105] 📂 KsponSpeech_0447 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0447): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:16<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0447): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0447 완료 (소요시간: 341.7초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.1시간\n",
      "\n",
      "[57/105] 📂 KsponSpeech_0448 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0448): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:07<00:00,  4.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0448): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0448 완료 (소요시간: 332.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.1시간\n",
      "\n",
      "[58/105] 📂 KsponSpeech_0449 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0449): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:06<00:00,  4.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0449): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0449 완료 (소요시간: 332.3초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 3.0시간\n",
      "\n",
      "[59/105] 📂 KsponSpeech_0450 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0450): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 29825.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0450): 100%|█████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 32162.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0450 완료 (소요시간: 0.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 0, 건너뜀 1000, 오류 0\n",
      "    📝 텍스트: 성공 0, 건너뜀 1000, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.9시간\n",
      "\n",
      "[60/105] 📂 KsponSpeech_0451 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0451): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:29<00:00,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0451): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0451 완료 (소요시간: 354.6초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.9시간\n",
      "\n",
      "[61/105] 📂 KsponSpeech_0452 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0452): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:01<00:00,  4.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0452): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0452 완료 (소요시간: 326.0초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.8시간\n",
      "\n",
      "[62/105] 📂 KsponSpeech_0453 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0453): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:23<00:00,  3.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0453): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0453 완료 (소요시간: 348.7초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.8시간\n",
      "\n",
      "[63/105] 📂 KsponSpeech_0454 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0454): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0454): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0454 완료 (소요시간: 340.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.8시간\n",
      "\n",
      "[64/105] 📂 KsponSpeech_0455 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0455): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:11<00:00,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0455): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0455 완료 (소요시간: 337.4초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.7시간\n",
      "\n",
      "[65/105] 📂 KsponSpeech_0456 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0456): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:19<00:00,  3.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0456): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0456 완료 (소요시간: 345.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.7시간\n",
      "\n",
      "[66/105] 📂 KsponSpeech_0457 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0457): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:05<00:00,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0457): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0457 완료 (소요시간: 331.4초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.6시간\n",
      "\n",
      "[67/105] 📂 KsponSpeech_0458 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0458): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:15<00:00,  3.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0458): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0458 완료 (소요시간: 341.1초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.6시간\n",
      "\n",
      "[68/105] 📂 KsponSpeech_0459 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0459): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:16<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0459): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0459 완료 (소요시간: 341.8초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.5시간\n",
      "\n",
      "[69/105] 📂 KsponSpeech_0460 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0460): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:17<00:00,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0460): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0460 완료 (소요시간: 343.9초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.5시간\n",
      "\n",
      "[70/105] 📂 KsponSpeech_0461 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0461): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:21<00:00,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0461): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0461 완료 (소요시간: 347.3초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.4시간\n",
      "\n",
      "[71/105] 📂 KsponSpeech_0462 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PCM 처리 (KsponSpeech_0462): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:08<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0462): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0462 완료 (소요시간: 333.9초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.3시간\n",
      "\n",
      "[72/105] 📂 KsponSpeech_0463 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0463): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:08<00:00,  4.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0463): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0463 완료 (소요시간: 333.9초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.3시간\n",
      "\n",
      "[73/105] 📂 KsponSpeech_0464 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0464): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:16<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0464): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0464 완료 (소요시간: 341.5초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.2시간\n",
      "\n",
      "[74/105] 📂 KsponSpeech_0465 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0465): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:13<00:00,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0465): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0465 완료 (소요시간: 339.5초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.2시간\n",
      "\n",
      "[75/105] 📂 KsponSpeech_0466 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0466): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0466): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:24<00:00, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0466 완료 (소요시간: 339.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.1시간\n",
      "\n",
      "[76/105] 📂 KsponSpeech_0467 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0467): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:07<00:00,  4.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0467): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0467 완료 (소요시간: 334.4초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.0시간\n",
      "\n",
      "[77/105] 📂 KsponSpeech_0468 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0468): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:14<00:00,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0468): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:25<00:00, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0468 완료 (소요시간: 340.3초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 2.0시간\n",
      "\n",
      "[78/105] 📂 KsponSpeech_0469 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0469): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:11<00:00,  3.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0469): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0469 완료 (소요시간: 338.2초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 1.9시간\n",
      "\n",
      "[79/105] 📂 KsponSpeech_0470 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0470): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [04:16<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  📝 TXT 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "TXT 처리 (KsponSpeech_0470): 100%|████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:26<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ KsponSpeech_0470 완료 (소요시간: 342.9초):\n",
      "    📊 파일 수: PCM 1000개, TXT 1000개\n",
      "    🎵 오디오: 성공 1000, 건너뜀 0, 오류 0\n",
      "    📝 텍스트: 성공 1000, 건너뜀 0, 오류 0\n",
      "    ⏱️ 예상 남은 시간: 1.9시간\n",
      "\n",
      "[80/105] 📂 KsponSpeech_0471 처리 중...\n",
      "🔄 멀티프로세싱 시작: 8개 프로세스 사용\n",
      "  🎵 PCM 파일 1000개 병렬 처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PCM 처리 (KsponSpeech_0471):   3%|██▍                                                                                   | 29/1000 [00:11<04:46,  3.39it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "from g2pk import G2p\n",
    "from tqdm import tqdm\n",
    "import struct\n",
    "from pathlib import Path\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "# 기본 디렉토리 경로 설정\n",
    "base_dir = \".\"  # 현재 디렉토리 (KsponSpeech_0450이 있는 곳)\n",
    "target_base_dir = \"./PreprocessData_04\"  # 전처리된 데이터를 저장할 기본 디렉토리\n",
    "\n",
    "# PCM 파일을 읽고 오디오 데이터로 변환하는 함수\n",
    "def read_pcm_file(pcm_file, sample_rate=16000, channels=1, bit_depth=16):\n",
    "    \"\"\"\n",
    "    PCM 파일을 읽어서 numpy 배열로 변환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(pcm_file, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "        \n",
    "        if bit_depth == 16:\n",
    "            audio_data = np.frombuffer(raw_data, dtype=np.int16)\n",
    "            audio_data = audio_data.astype(np.float32) / 32768.0\n",
    "        elif bit_depth == 32:\n",
    "            audio_data = np.frombuffer(raw_data, dtype=np.int32)\n",
    "            audio_data = audio_data.astype(np.float32) / 2147483648.0\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 비트 깊이: {bit_depth}\")\n",
    "        \n",
    "        return audio_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"PCM 파일 읽기 오류 ({pcm_file}): {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 오디오 특성 추출 함수\n",
    "def extract_audio_features(audio_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    오디오 데이터에서 다양한 특성을 추출합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. MFCC 특성 추출\n",
    "        mfcc = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=13)\n",
    "        \n",
    "        # 2. 멜 스펙트로그램\n",
    "        mel_spec = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate, n_mels=80)\n",
    "        log_mel_spec = librosa.power_to_db(mel_spec)\n",
    "        \n",
    "        # 3. 스펙트럴 센트로이드 (음색의 밝기)\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 4. 스펙트럴 롤오프\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 5. 제로 크로싱 레이트\n",
    "        zcr = librosa.feature.zero_crossing_rate(audio_data)\n",
    "        \n",
    "        # 6. 크로마 특성\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
    "        \n",
    "        # 7. 피치(F0) 추출\n",
    "        f0, voiced_flag, voiced_probs = librosa.pyin(audio_data, \n",
    "                                                   fmin=librosa.note_to_hz('C2'), \n",
    "                                                   fmax=librosa.note_to_hz('C7'))\n",
    "        \n",
    "        features = {\n",
    "            'mfcc': mfcc,\n",
    "            'mel_spectrogram': log_mel_spec,\n",
    "            'spectral_centroids': spectral_centroids,\n",
    "            'spectral_rolloff': spectral_rolloff,\n",
    "            'zero_crossing_rate': zcr,\n",
    "            'chroma': chroma,\n",
    "            'f0': f0,\n",
    "            'voiced_flag': voiced_flag,\n",
    "            'voiced_probs': voiced_probs,\n",
    "            'sample_rate': sample_rate,\n",
    "            'duration': len(audio_data) / sample_rate\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"특성 추출 오류: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 텍스트 파일 로드 함수 (다양한 인코딩 시도)\n",
    "def load_text(text_file):\n",
    "    \"\"\"다양한 인코딩을 시도하여 텍스트 파일을 로드합니다.\"\"\"\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1']\n",
    "    \n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(text_file, 'r', encoding=encoding) as f:\n",
    "                return f.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    try:\n",
    "        with open(text_file, 'rb') as f:\n",
    "            raw_data = f.read()\n",
    "            return raw_data.decode('latin1').encode('utf-8').decode('utf-8')\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"모든 인코딩 시도 실패: {str(e)}\")\n",
    "\n",
    "# 단일 PCM 파일 처리 함수 (멀티프로세싱용)\n",
    "def process_single_audio_file(args):\n",
    "    \"\"\"단일 PCM 파일을 처리합니다 (멀티프로세싱용)\"\"\"\n",
    "    pcm_file, audio_target_dir = args\n",
    "    \n",
    "    base_name = os.path.basename(pcm_file).split('.')[0]\n",
    "    target_file = os.path.join(audio_target_dir, f\"{base_name}.npz\")\n",
    "    \n",
    "    # 이미 처리된 파일인지 확인\n",
    "    if os.path.exists(target_file):\n",
    "        return {'status': 'skip', 'file': base_name}\n",
    "    \n",
    "    try:\n",
    "        # PCM 파일 읽기\n",
    "        audio_data = read_pcm_file(pcm_file)\n",
    "        \n",
    "        if audio_data is None:\n",
    "            return {'status': 'error', 'file': base_name, 'error': 'PCM 읽기 실패'}\n",
    "        \n",
    "        # 오디오 특성 추출\n",
    "        features = extract_audio_features(audio_data)\n",
    "        \n",
    "        if features is None:\n",
    "            return {'status': 'error', 'file': base_name, 'error': '특성 추출 실패'}\n",
    "        \n",
    "        # NPZ 파일로 저장\n",
    "        np.savez_compressed(target_file, **features)\n",
    "        \n",
    "        return {'status': 'success', 'file': base_name}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'status': 'error', 'file': base_name, 'error': str(e)}\n",
    "\n",
    "# 단일 TXT 파일 처리 함수 (멀티프로세싱용)\n",
    "def process_single_text_file(args):\n",
    "    \"\"\"단일 TXT 파일을 처리합니다 (멀티프로세싱용)\"\"\"\n",
    "    txt_file, text_target_dir = args\n",
    "    \n",
    "    base_name = os.path.basename(txt_file).split('.')[0]\n",
    "    target_file = os.path.join(text_target_dir, f\"{base_name}.txt\")\n",
    "    \n",
    "    # 이미 처리된 파일인지 확인\n",
    "    if os.path.exists(target_file):\n",
    "        return {'status': 'skip', 'file': base_name}\n",
    "    \n",
    "    try:\n",
    "        # G2p 초기화 (각 프로세스마다)\n",
    "        g2p = G2p()\n",
    "        \n",
    "        # 텍스트 로드\n",
    "        original_text = load_text(txt_file)\n",
    "        \n",
    "        # G2P 변환 적용\n",
    "        g2p_text = g2p(original_text)\n",
    "        \n",
    "        # 결과 저장\n",
    "        with open(target_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(g2p_text)\n",
    "        \n",
    "        return {'status': 'success', 'file': base_name}\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {'status': 'error', 'file': base_name, 'error': str(e)}\n",
    "\n",
    "# 디렉토리 내의 PCM 및 TXT 파일 경로 가져오기\n",
    "def get_file_paths(directory):\n",
    "    \"\"\"디렉토리 내의 모든 PCM 및 TXT 파일 경로를 반환합니다.\"\"\"\n",
    "    pcm_files = sorted(glob.glob(os.path.join(directory, \"*.pcm\")))\n",
    "    txt_files = sorted(glob.glob(os.path.join(directory, \"*.txt\")))\n",
    "    return pcm_files, txt_files\n",
    "\n",
    "# 단일 폴더 처리 함수 (멀티프로세싱 적용)\n",
    "def process_single_folder_mp(source_dir, audio_target_dir, text_target_dir, num_processes=None):\n",
    "    \"\"\"단일 폴더를 멀티프로세싱으로 처리합니다.\"\"\"\n",
    "    \n",
    "    # CPU 코어 수 자동 설정\n",
    "    if num_processes is None:\n",
    "        num_processes = min(mp.cpu_count(), 8)  # 최대 8개 프로세스\n",
    "    \n",
    "    print(f\"🔄 멀티프로세싱 시작: {num_processes}개 프로세스 사용\")\n",
    "    \n",
    "    # 파일 경로 가져오기\n",
    "    pcm_files, txt_files = get_file_paths(source_dir)\n",
    "    \n",
    "    folder_name = os.path.basename(source_dir)\n",
    "    \n",
    "    # 통계 변수\n",
    "    audio_success = audio_skip = audio_error = 0\n",
    "    text_success = text_skip = text_error = 0\n",
    "    \n",
    "    # 1. PCM 파일들 병렬 처리\n",
    "    if pcm_files:\n",
    "        print(f\"  🎵 PCM 파일 {len(pcm_files)}개 병렬 처리 중...\")\n",
    "        \n",
    "        audio_args = [(pcm_file, audio_target_dir) for pcm_file in pcm_files]\n",
    "        \n",
    "        with mp.Pool(processes=num_processes) as pool:\n",
    "            # 진행률 표시를 위한 imap 사용\n",
    "            results = []\n",
    "            with tqdm(total=len(audio_args), desc=f\"PCM 처리 ({folder_name})\") as pbar:\n",
    "                for result in pool.imap(process_single_audio_file, audio_args):\n",
    "                    results.append(result)\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        # 결과 집계\n",
    "        for result in results:\n",
    "            if result['status'] == 'success':\n",
    "                audio_success += 1\n",
    "            elif result['status'] == 'skip':\n",
    "                audio_skip += 1\n",
    "            else:\n",
    "                audio_error += 1\n",
    "                print(f\"    ❌ 오디오 오류 ({result['file']}): {result.get('error', '알 수 없음')}\")\n",
    "    \n",
    "    # 2. TXT 파일들 병렬 처리\n",
    "    if txt_files:\n",
    "        print(f\"  📝 TXT 파일 {len(txt_files)}개 병렬 처리 중...\")\n",
    "        \n",
    "        text_args = [(txt_file, text_target_dir) for txt_file in txt_files]\n",
    "        \n",
    "        with mp.Pool(processes=num_processes) as pool:\n",
    "            results = []\n",
    "            with tqdm(total=len(text_args), desc=f\"TXT 처리 ({folder_name})\") as pbar:\n",
    "                for result in pool.imap(process_single_text_file, text_args):\n",
    "                    results.append(result)\n",
    "                    pbar.update(1)\n",
    "        \n",
    "        # 결과 집계\n",
    "        for result in results:\n",
    "            if result['status'] == 'success':\n",
    "                text_success += 1\n",
    "            elif result['status'] == 'skip':\n",
    "                text_skip += 1\n",
    "            else:\n",
    "                text_error += 1\n",
    "                print(f\"    ❌ 텍스트 오류 ({result['file']}): {result.get('error', '알 수 없음')}\")\n",
    "    \n",
    "    return {\n",
    "        'folder': folder_name,\n",
    "        'pcm_files': len(pcm_files),\n",
    "        'txt_files': len(txt_files),\n",
    "        'audio_success': audio_success,\n",
    "        'audio_skip': audio_skip,\n",
    "        'audio_error': audio_error,\n",
    "        'text_success': text_success,\n",
    "        'text_skip': text_skip,\n",
    "        'text_error': text_error\n",
    "    }\n",
    "\n",
    "# 메인 전처리 함수 (멀티프로세싱 적용)\n",
    "def preprocess_kspon_data_mp(num_processes=None, start_from=None, max_folders=None):\n",
    "    \"\"\"KsponSpeech_04 모든 폴더의 데이터를 멀티프로세싱으로 전처리합니다.\"\"\"\n",
    "    \n",
    "    # CPU 코어 수 확인\n",
    "    total_cores = mp.cpu_count()\n",
    "    if num_processes is None:\n",
    "        num_processes = min(total_cores, 8)\n",
    "    \n",
    "    print(f\"🖥️ 시스템 정보:\")\n",
    "    print(f\"  - 총 CPU 코어: {total_cores}개\")\n",
    "    print(f\"  - 사용할 프로세스: {num_processes}개\")\n",
    "    \n",
    "    # 현재 디렉토리에서 KsponSpeech_로 시작하는 모든 폴더 찾기\n",
    "    all_folders = sorted([d for d in os.listdir('.') \n",
    "                         if d.startswith('KsponSpeech_') and os.path.isdir(d)])\n",
    "    \n",
    "    # 시작 지점 설정\n",
    "    if start_from:\n",
    "        try:\n",
    "            start_idx = all_folders.index(start_from)\n",
    "            all_folders = all_folders[start_idx:]\n",
    "            print(f\"📍 {start_from}부터 재시작\")\n",
    "        except ValueError:\n",
    "            print(f\"⚠️ {start_from} 폴더를 찾을 수 없습니다. 처음부터 시작합니다.\")\n",
    "    \n",
    "    # 폴더 수 제한\n",
    "    if max_folders:\n",
    "        all_folders = all_folders[:max_folders]\n",
    "        print(f\"📊 최대 {max_folders}개 폴더만 처리\")\n",
    "    \n",
    "    print(f\"📁 처리할 폴더: {len(all_folders)}개\")\n",
    "    \n",
    "    # 저장할 디렉토리들 생성\n",
    "    audio_target_dir = f\"{target_base_dir}/audio_features\"\n",
    "    text_target_dir = f\"{target_base_dir}/g2p_texts\"\n",
    "    \n",
    "    print(f\"\\n💾 저장 위치: {target_base_dir}\")\n",
    "    print(f\"  - 오디오 특성 (NPZ): {audio_target_dir}\")\n",
    "    print(f\"  - G2P 텍스트: {text_target_dir}\")\n",
    "    \n",
    "    os.makedirs(audio_target_dir, exist_ok=True)\n",
    "    os.makedirs(text_target_dir, exist_ok=True)\n",
    "    \n",
    "    # 전체 통계\n",
    "    total_results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 KsponSpeech_04 멀티프로세싱 전처리 시작\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 각 폴더 처리\n",
    "    for i, folder in enumerate(all_folders, 1):\n",
    "        source_dir = f\"./{folder}\"\n",
    "        \n",
    "        print(f\"\\n[{i}/{len(all_folders)}] 📂 {folder} 처리 중...\")\n",
    "        \n",
    "        if not os.path.exists(source_dir):\n",
    "            print(f\"⚠️ 소스 디렉토리가 존재하지 않습니다: {source_dir}\")\n",
    "            continue\n",
    "        \n",
    "        folder_start_time = time.time()\n",
    "        \n",
    "        # 단일 폴더 멀티프로세싱 처리\n",
    "        result = process_single_folder_mp(source_dir, audio_target_dir, text_target_dir, num_processes)\n",
    "        total_results.append(result)\n",
    "        \n",
    "        folder_time = time.time() - folder_start_time\n",
    "        \n",
    "        print(f\"  ✅ {folder} 완료 (소요시간: {folder_time:.1f}초):\")\n",
    "        print(f\"    📊 파일 수: PCM {result['pcm_files']}개, TXT {result['txt_files']}개\")\n",
    "        print(f\"    🎵 오디오: 성공 {result['audio_success']}, 건너뜀 {result['audio_skip']}, 오류 {result['audio_error']}\")\n",
    "        print(f\"    📝 텍스트: 성공 {result['text_success']}, 건너뜀 {result['text_skip']}, 오류 {result['text_error']}\")\n",
    "        \n",
    "        # 예상 남은 시간 계산\n",
    "        if i < len(all_folders):\n",
    "            elapsed = time.time() - start_time\n",
    "            avg_time_per_folder = elapsed / i\n",
    "            remaining_folders = len(all_folders) - i\n",
    "            estimated_remaining = avg_time_per_folder * remaining_folders\n",
    "            print(f\"    ⏱️ 예상 남은 시간: {estimated_remaining/3600:.1f}시간\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🎉 전체 처리 완료!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 전체 통계 계산\n",
    "    total_audio_success = sum(r['audio_success'] for r in total_results)\n",
    "    total_audio_skip = sum(r['audio_skip'] for r in total_results)\n",
    "    total_audio_error = sum(r['audio_error'] for r in total_results)\n",
    "    total_text_success = sum(r['text_success'] for r in total_results)\n",
    "    total_text_skip = sum(r['text_skip'] for r in total_results)\n",
    "    total_text_error = sum(r['text_error'] for r in total_results)\n",
    "    \n",
    "    print(f\"\\n📈 전체 통계:\")\n",
    "    print(f\"  - 처리된 폴더: {len(total_results)}개\")\n",
    "    print(f\"  - 총 소요시간: {total_time/3600:.1f}시간\")\n",
    "    print(f\"  - 평균 폴더당 시간: {total_time/len(total_results)/60:.1f}분\")\n",
    "    print(f\"\\n🎵 오디오 파일:\")\n",
    "    print(f\"  - 성공: {total_audio_success:,}개\")\n",
    "    print(f\"  - 건너뜀: {total_audio_skip:,}개\")\n",
    "    print(f\"  - 오류: {total_audio_error:,}개\")\n",
    "    print(f\"\\n📝 텍스트 파일:\")\n",
    "    print(f\"  - 성공: {total_text_success:,}개\")\n",
    "    print(f\"  - 건너뜀: {total_text_skip:,}개\")\n",
    "    print(f\"  - 오류: {total_text_error:,}개\")\n",
    "    \n",
    "    # 최종 파일 수 확인\n",
    "    try:\n",
    "        processed_audio_files = len([f for f in os.listdir(audio_target_dir) if f.endswith('.npz')])\n",
    "        processed_text_files = len([f for f in os.listdir(text_target_dir) if f.endswith('.txt')])\n",
    "        \n",
    "        print(f\"\\n📦 최종 결과:\")\n",
    "        print(f\"  - 저장된 오디오 파일: {processed_audio_files:,}개\")\n",
    "        print(f\"  - 저장된 텍스트 파일: {processed_text_files:,}개\")\n",
    "        \n",
    "        if processed_audio_files == processed_text_files:\n",
    "            print(\"  ✅ 오디오와 텍스트 파일 수가 일치합니다!\")\n",
    "        else:\n",
    "            print(\"  ⚠️ 오디오와 텍스트 파일 수가 일치하지 않습니다.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 최종 파일 수 확인 중 오류: {str(e)}\")\n",
    "    \n",
    "    return total_results\n",
    "\n",
    "# 실행 함수들\n",
    "def run_full_processing():\n",
    "    \"\"\"전체 폴더 처리\"\"\"\n",
    "    print(\"🚀 전체 폴더 멀티프로세싱 시작\")\n",
    "    return preprocess_kspon_data_mp()\n",
    "\n",
    "def run_partial_processing(max_folders=20):\n",
    "    \"\"\"일부 폴더만 처리 (테스트용)\"\"\"\n",
    "    print(f\"🧪 {max_folders}개 폴더만 처리 (테스트)\")\n",
    "    return preprocess_kspon_data_mp(max_folders=max_folders)\n",
    "\n",
    "def run_resume_processing(start_from=\"KsponSpeech_0380\"):\n",
    "    \"\"\"특정 폴더부터 재시작\"\"\"\n",
    "    print(f\"🔄 {start_from}부터 재시작\")\n",
    "    return preprocess_kspon_data_mp(start_from=start_from)\n",
    "\n",
    "# 메인 실행부\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 KsponSpeech_04 멀티프로세싱 전처리\")\n",
    "    print(\"선택하세요:\")\n",
    "    print(\"1. 전체 처리 - run_full_processing()\")\n",
    "    print(\"2. 테스트 (20개) - run_partial_processing(20)\")\n",
    "    print(\"3. 재시작 - run_resume_processing('KsponSpeech_0392')\")\n",
    "    \n",
    "    # 기본 실행: 재시작 모드\n",
    "    run_resume_processing(\"KsponSpeech_0392\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626de895-27e8-46d8-9fd5-d8e6c192ef6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2542020279.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    find g2p_texts -type f | wc -l\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!find \"g2p_texts -type f | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ab0452-2563-4b9b-8b7f-264068627199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/Traindata_2/KsponSpeech_04/PreprocessData_04\n"
     ]
    }
   ],
   "source": [
    "cd PreprocessData_04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d425b48-40b6-4b16-9293-af1e617f5c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "📂 데이터 디렉토리 경로 (기본값: Traindata_2/KsponSpeech_04/PreprocessData_04):  KsponSpeech_04/PreprocessData_04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 설정된 경로:\n",
      "  - 기본 경로: KsponSpeech_04/PreprocessData_04\n",
      "  - 텍스트: KsponSpeech_04/PreprocessData_04/g2p_texts\n",
      "  - 오디오: KsponSpeech_04/PreprocessData_04/audio_features\n",
      "\n",
      "📊 파일 수 확인:\n",
      "  - 텍스트 파일: 124,000개\n",
      "  - 오디오 파일: 124,000개 (.npy: 0, .npz: 124000)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 124,000개 파일로 학습을 시작하시겠습니까? (y/N):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Whisper 파인튜닝 시작!\n",
      "📊 데이터셋 크기: 60,000개\n",
      "🎯 모델: openai/whisper-small\n",
      "⚡ GPU: 사용 가능\n",
      "📂 데이터셋 로딩 중...\n",
      "📁 텍스트 디렉토리: KsponSpeech_04/PreprocessData_04/g2p_texts\n",
      "🎵 오디오 디렉토리: KsponSpeech_04/PreprocessData_04/audio_features\n",
      "📝 텍스트 파일 수: 124,000개\n",
      "📊 진행 상황: 10,000개 매칭됨\n",
      "📊 진행 상황: 20,000개 매칭됨\n",
      "📊 진행 상황: 30,000개 매칭됨\n",
      "📊 진행 상황: 40,000개 매칭됨\n",
      "📊 진행 상황: 50,000개 매칭됨\n",
      "📊 진행 상황: 60,000개 매칭됨\n",
      "📊 진행 상황: 70,000개 매칭됨\n",
      "📊 진행 상황: 80,000개 매칭됨\n",
      "📊 진행 상황: 90,000개 매칭됨\n",
      "📊 진행 상황: 100,000개 매칭됨\n",
      "📊 진행 상황: 110,000개 매칭됨\n",
      "📊 진행 상황: 120,000개 매칭됨\n",
      "✅ 총 124,000개 파일 쌍 매칭됨\n",
      "📊 데이터 검증:\n",
      "  - 평균 텍스트 길이: 39.8자\n",
      "  - 최대 텍스트 길이: 384자\n",
      "  - 빈 텍스트: 0개\n",
      "  - 유효한 데이터: 124,000개\n",
      "📊 최종 데이터 분할:\n",
      "  - 학습: 48,000개\n",
      "  - 검증: 6,000개\n",
      "  - 테스트: 6,000개\n",
      "🤖 모델 초기화 중...\n",
      "✅ 모델 로드 완료: openai/whisper-small\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|█████████████████████████████████████████████████████████████████████████████████████| 48000/48000 [01:32<00:00, 517.16 examples/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 553\u001b[0m\n\u001b[1;32m    550\u001b[0m     exit(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# 테스트 (옵션)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m test_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎵 테스트할 오디오 특성 파일 경로 (선택사항, Enter로 건너뛰기): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[3], line 393\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;66;03m# 3. 데이터 전처리\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔄 데이터 전처리 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDATALOADER_NUM_WORKERS\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# 4. 데이터 콜레이터 설정\u001b[39;00m\n\u001b[1;32m    400\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorSpeechSeq2SeqWithPadding(\n\u001b[1;32m    401\u001b[0m     processor\u001b[38;5;241m=\u001b[39mprocessor,\n\u001b[1;32m    402\u001b[0m     decoder_start_token_id\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id,\n\u001b[1;32m    403\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/dataset_dict.py:886\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m--> 886\u001b[0m     {\n\u001b[1;32m    887\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    888\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[1;32m    889\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[1;32m    890\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[1;32m    891\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[1;32m    892\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[1;32m    893\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    894\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[1;32m    895\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[1;32m    896\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[1;32m    897\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[1;32m    898\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[1;32m    899\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[1;32m    900\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[1;32m    901\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[1;32m    902\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[1;32m    903\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[1;32m    904\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[1;32m    905\u001b[0m         )\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    907\u001b[0m     }\n\u001b[1;32m    908\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/dataset_dict.py:887\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m    886\u001b[0m     {\n\u001b[0;32m--> 887\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    907\u001b[0m     }\n\u001b[1;32m    908\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3147\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3141\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3142\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3143\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3144\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3145\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3146\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3147\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3148\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3149\u001b[0m     ):\n\u001b[1;32m   3150\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3151\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/utils/py_utils.py:711\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    709\u001b[0m             pool_changed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    710\u001b[0m             \u001b[38;5;66;03m# One of the subprocesses has died. We should not wait forever.\u001b[39;00m\n\u001b[0;32m--> 711\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne of the subprocesses has abruptly died during map operation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo debug the error, disable multiprocessing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    714\u001b[0m             )\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the subprocesses has abruptly died during map operation.To debug the error, disable multiprocessing."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Whisper Fine-tuning for Korean STT\n",
    "Dataset: 124,000 KsponSpeech samples\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict  # load_metric 제거\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate  # 새로 추가\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 설정 및 경로\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"학습 설정\"\"\"\n",
    "    \n",
    "    # 데이터 경로\n",
    "    DATA_DIR = \"/path/to/your/processed_data\"  # 실제 경로로 수정 필요\n",
    "    OUTPUT_DIR = \"./whisper-korean-finetuned\"\n",
    "    \n",
    "    # 모델 설정\n",
    "    MODEL_NAME = \"openai/whisper-small\"  # small, base, large 중 선택\n",
    "    LANGUAGE = \"korean\"\n",
    "    TASK = \"transcribe\"\n",
    "    \n",
    "    # 데이터셋 설정\n",
    "    TRAIN_SIZE = 48000    # 6만개의 80%\n",
    "    VAL_SIZE = 6000       # 6만개의 10%\n",
    "    TEST_SIZE = 6000      # 6만개의 10%\n",
    "    NUM_EPOCHS = 3  \n",
    "    \n",
    "    # 학습 하이퍼파라미터\n",
    "    BATCH_SIZE = 8       # GPU 메모리에 따라 조정\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    LEARNING_RATE = 1e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 500\n",
    "    \n",
    "    # 체크포인트 설정\n",
    "    SAVE_STEPS = 1000\n",
    "    EVAL_STEPS = 500\n",
    "    SAVE_TOTAL_LIMIT = 3\n",
    "    \n",
    "    # 하드웨어 설정\n",
    "    DATALOADER_NUM_WORKERS = 4\n",
    "    USE_FP16 = True  # Mixed precision\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# =============================================================================\n",
    "# 데이터 로딩 및 전처리\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset_from_directory(data_dir: str) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    실제 전처리된 데이터 구조에서 데이터셋 로드\n",
    "    - 텍스트: g2p_texts/*.txt\n",
    "    - 오디오 특성: audio_features/*.npy 또는 *.npz\n",
    "    \"\"\"\n",
    "    print(\"📂 데이터셋 로딩 중...\")\n",
    "    \n",
    "    # 실제 경로 설정\n",
    "    base_dir = Path(data_dir)\n",
    "    text_dir = base_dir / \"g2p_texts\"\n",
    "    audio_dir = base_dir / \"audio_features\"\n",
    "    \n",
    "    print(f\"📁 텍스트 디렉토리: {text_dir}\")\n",
    "    print(f\"🎵 오디오 디렉토리: {audio_dir}\")\n",
    "    \n",
    "    # 디렉토리 존재 확인\n",
    "    if not text_dir.exists():\n",
    "        raise FileNotFoundError(f\"텍스트 디렉토리가 없습니다: {text_dir}\")\n",
    "    if not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"오디오 디렉토리가 없습니다: {audio_dir}\")\n",
    "    \n",
    "    # 매칭되는 파일 쌍 찾기\n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    \n",
    "    # 텍스트 파일 기준으로 매칭\n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    print(f\"📝 텍스트 파일 수: {len(text_files):,}개\")\n",
    "    \n",
    "    matched_count = 0\n",
    "    for text_file in text_files:\n",
    "        # 대응하는 오디오 특성 파일 찾기\n",
    "        base_name = text_file.stem\n",
    "        \n",
    "        # .npy 또는 .npz 파일 찾기\n",
    "        audio_npy = audio_dir / f\"{base_name}.npy\"\n",
    "        audio_npz = audio_dir / f\"{base_name}.npz\"\n",
    "        \n",
    "        audio_file = None\n",
    "        if audio_npy.exists():\n",
    "            audio_file = audio_npy\n",
    "        elif audio_npz.exists():\n",
    "            audio_file = audio_npz\n",
    "        \n",
    "        if audio_file:\n",
    "            # 텍스트 읽기\n",
    "            try:\n",
    "                with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                    transcript = f.read().strip()\n",
    "                \n",
    "                if transcript:  # 빈 텍스트 제외\n",
    "                    audio_files.append(str(audio_file))\n",
    "                    transcripts.append(transcript)\n",
    "                    matched_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 텍스트 파일 읽기 실패: {text_file}, 오류: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if matched_count % 10000 == 0 and matched_count > 0:\n",
    "            print(f\"📊 진행 상황: {matched_count:,}개 매칭됨\")\n",
    "    \n",
    "    print(f\"✅ 총 {len(audio_files):,}개 파일 쌍 매칭됨\")\n",
    "    \n",
    "    if len(audio_files) == 0:\n",
    "        raise ValueError(\"매칭되는 파일이 없습니다. 경로와 파일 형식을 확인해주세요.\")\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame({\n",
    "        'audio_path': audio_files,\n",
    "        'transcript': transcripts\n",
    "    })\n",
    "    \n",
    "    # 데이터 검증\n",
    "    print(f\"📊 데이터 검증:\")\n",
    "    print(f\"  - 평균 텍스트 길이: {df['transcript'].str.len().mean():.1f}자\")\n",
    "    print(f\"  - 최대 텍스트 길이: {df['transcript'].str.len().max()}자\")\n",
    "    print(f\"  - 빈 텍스트: {df['transcript'].str.len().eq(0).sum()}개\")\n",
    "    \n",
    "    # 빈 텍스트 제거\n",
    "    df = df[df['transcript'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"  - 유효한 데이터: {len(df):,}개\")\n",
    "    \n",
    "    # 데이터 분할 (사용 가능한 데이터에 맞춰 조정)\n",
    "    total_size = len(df)\n",
    "    actual_train_size = min(config.TRAIN_SIZE, int(total_size * 0.8))\n",
    "    actual_val_size = min(config.VAL_SIZE, int(total_size * 0.1))\n",
    "    actual_test_size = min(config.TEST_SIZE, int(total_size * 0.1))\n",
    "    \n",
    "    # 데이터 셔플\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    train_df = df[:actual_train_size]\n",
    "    val_df = df[actual_train_size:actual_train_size + actual_val_size]\n",
    "    test_df = df[actual_train_size + actual_val_size:actual_train_size + actual_val_size + actual_test_size]\n",
    "    \n",
    "    print(f\"📊 최종 데이터 분할:\")\n",
    "    print(f\"  - 학습: {len(train_df):,}개\")\n",
    "    print(f\"  - 검증: {len(val_df):,}개\") \n",
    "    print(f\"  - 테스트: {len(test_df):,}개\")\n",
    "    \n",
    "    # Hugging Face Dataset으로 변환\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "\n",
    "def load_audio_features(audio_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    전처리된 오디오 특성 파일 로드 (.npy 또는 .npz)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if audio_path.endswith('.npy'):\n",
    "            # .npy 파일 로드\n",
    "            features = np.load(audio_path)\n",
    "        elif audio_path.endswith('.npz'):\n",
    "            # .npz 파일 로드\n",
    "            data = np.load(audio_path)\n",
    "            # 첫 번째 키의 데이터 사용 (또는 특정 키 지정)\n",
    "            key = list(data.keys())[0]\n",
    "            features = data[key]\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 파일 형식: {audio_path}\")\n",
    "        \n",
    "        # 특성이 올바른 형태인지 확인\n",
    "        if features.ndim == 1:\n",
    "            # 1D 배열이면 2D로 변환 (time_steps, features)\n",
    "            features = features.reshape(-1, 1)\n",
    "        elif features.ndim == 2:\n",
    "            # 2D 배열이면 그대로 사용\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"⚠️ 예상치 못한 특성 형태: {features.shape} in {audio_path}\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오디오 특성 로드 실패: {audio_path}, 오류: {e}\")\n",
    "        # 기본 특성 반환 (에러 방지)\n",
    "        return np.zeros((1, 80))  # Whisper 멜 스펙트로그램 차원\n",
    "\n",
    "def load_audio(audio_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    오디오 파일 로드 (WAV 파일용 - 실제로는 특성 파일을 로드)\n",
    "    \"\"\"\n",
    "    # 실제로는 전처리된 특성 파일을 로드\n",
    "    return load_audio_features(audio_path)\n",
    "\n",
    "# =============================================================================\n",
    "# 모델 및 프로세서 초기화  \n",
    "# =============================================================================\n",
    "\n",
    "def initialize_model_and_processor():\n",
    "    \"\"\"Whisper 모델과 프로세서 초기화\"\"\"\n",
    "    print(\"🤖 모델 초기화 중...\")\n",
    "    \n",
    "    # 프로세서 구성요소\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(config.MODEL_NAME)\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\n",
    "        config.MODEL_NAME, \n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    processor = WhisperProcessor.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    # 언어 토큰 설정\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # 한국어 설정\n",
    "    model.generation_config.language = config.LANGUAGE\n",
    "    model.generation_config.task = config.TASK\n",
    "    \n",
    "    print(f\"✅ 모델 로드 완료: {config.MODEL_NAME}\")\n",
    "    \n",
    "    return model, processor, feature_extractor, tokenizer\n",
    "\n",
    "# =============================================================================\n",
    "# 데이터 전처리 및 콜레이터\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    음성-텍스트 데이터 콜레이터\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 오디오 입력 패딩\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 레이블 패딩\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # -100으로 패딩된 토큰을 마스킹 (손실 계산에서 제외)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # 시작 토큰이 있다면 제거 (모델이 자동으로 추가)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch, processor, feature_extractor):\n",
    "    \"\"\"\n",
    "    전처리된 특성 데이터를 위한 데이터셋 준비 함수\n",
    "    \"\"\"\n",
    "    \n",
    "    # 전처리된 오디오 특성 로드\n",
    "    audio_features = load_audio_features(batch[\"audio_path\"])\n",
    "    \n",
    "    # 특성 형태 확인 및 조정\n",
    "    if audio_features.ndim == 2:\n",
    "        # (time_steps, features) → (features, time_steps) 변환이 필요할 수 있음\n",
    "        if audio_features.shape[1] > audio_features.shape[0]:\n",
    "            # 일반적으로 (features, time_steps) 형태가 맞음\n",
    "            pass\n",
    "        else:\n",
    "            # (time_steps, features) → (features, time_steps)\n",
    "            audio_features = audio_features.T\n",
    "    \n",
    "    # Whisper input_features 형태로 변환\n",
    "    # Whisper는 (80, 3000) 형태의 멜 스펙트로그램을 기대\n",
    "    target_time_steps = 3000  # 30초 * 100 (10ms 프레임)\n",
    "    \n",
    "    if audio_features.shape[1] > target_time_steps:\n",
    "        # 너무 긴 경우 자르기\n",
    "        audio_features = audio_features[:, :target_time_steps]\n",
    "    elif audio_features.shape[1] < target_time_steps:\n",
    "        # 너무 짧은 경우 패딩\n",
    "        pad_width = target_time_steps - audio_features.shape[1]\n",
    "        audio_features = np.pad(audio_features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # 특성 차원 조정 (80차원으로 맞추기)\n",
    "    if audio_features.shape[0] != 80:\n",
    "        if audio_features.shape[0] < 80:\n",
    "            # 부족한 차원 패딩\n",
    "            pad_features = 80 - audio_features.shape[0]\n",
    "            audio_features = np.pad(audio_features, ((0, pad_features), (0, 0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            # 초과 차원 자르기\n",
    "            audio_features = audio_features[:80, :]\n",
    "    \n",
    "    # input_features로 설정\n",
    "    batch[\"input_features\"] = audio_features.astype(np.float32)\n",
    "    \n",
    "    # 텍스트 토큰화\n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"transcript\"],\n",
    "        truncation=True,\n",
    "        max_length=448,  # Whisper 최대 길이\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# =============================================================================\n",
    "# 평가 메트릭\n",
    "# =============================================================================\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"WER 및 CER 계산\"\"\"\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    # -100을 패딩 토큰으로 교체\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # 디코딩\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # WER 계산 - evaluate 라이브러리 사용\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# =============================================================================\n",
    "# 학습 실행\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 학습 함수\"\"\"\n",
    "    \n",
    "    print(\"🚀 Whisper 파인튜닝 시작!\")\n",
    "    print(f\"📊 데이터셋 크기: {config.TRAIN_SIZE + config.VAL_SIZE + config.TEST_SIZE:,}개\")\n",
    "    print(f\"🎯 모델: {config.MODEL_NAME}\")\n",
    "    print(f\"⚡ GPU: {'사용 가능' if torch.cuda.is_available() else '사용 불가'}\")\n",
    "    \n",
    "    # 1. 데이터셋 로드\n",
    "    dataset = load_dataset_from_directory(config.DATA_DIR)\n",
    "    \n",
    "    # 2. 모델 초기화\n",
    "    model, processor, feature_extractor, tokenizer = initialize_model_and_processor()\n",
    "    \n",
    "    # 3. 데이터 전처리\n",
    "    print(\"🔄 데이터 전처리 중...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: prepare_dataset(batch, processor, feature_extractor),\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        num_proc=config.DATALOADER_NUM_WORKERS\n",
    "    )\n",
    "    \n",
    "    # 4. 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # 5. 학습 설정\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        warmup_steps=config.WARMUP_STEPS,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=config.EVAL_STEPS,\n",
    "        save_steps=config.SAVE_STEPS,\n",
    "        save_total_limit=config.SAVE_TOTAL_LIMIT,\n",
    "        logging_steps=100,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "        dataloader_num_workers=config.DATALOADER_NUM_WORKERS,\n",
    "        fp16=config.USE_FP16,\n",
    "        gradient_checkpointing=True,  # 메모리 절약\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "    \n",
    "    # 6. 트레이너 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # 7. 학습 시작\n",
    "    print(\"🎓 학습 시작!\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # 8. 모델 저장\n",
    "    print(\"💾 모델 저장 중...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(config.OUTPUT_DIR)\n",
    "    \n",
    "    # 9. 테스트 세트 평가\n",
    "    print(\"📊 테스트 세트 평가 중...\")\n",
    "    test_results = trainer.evaluate(dataset[\"test\"])\n",
    "    print(f\"🎯 최종 테스트 WER: {test_results['eval_wer']:.4f}\")\n",
    "    \n",
    "    # 10. 결과 저장\n",
    "    results = {\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"dataset_size\": len(dataset[\"train\"]) + len(dataset[\"validation\"]),\n",
    "        \"test_wer\": test_results['eval_wer'],\n",
    "        \"config\": config.__dict__\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(config.OUTPUT_DIR, \"training_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"🎉 파인튜닝 완료!\")\n",
    "    print(f\"📁 모델 저장 위치: {config.OUTPUT_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 간단한 추론 테스트\n",
    "# =============================================================================\n",
    "\n",
    "def test_inference(model_path: str, audio_path: str):\n",
    "    \"\"\"학습된 모델로 추론 테스트\"\"\"\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "    processor = WhisperProcessor.from_pretrained(model_path)\n",
    "    \n",
    "    # 오디오 로드\n",
    "    audio = load_audio(audio_path)\n",
    "    \n",
    "    # 특성 추출\n",
    "    input_features = processor(\n",
    "        audio, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # 추론\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    \n",
    "    # 디코딩\n",
    "    transcription = processor.batch_decode(\n",
    "        predicted_ids, \n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    print(f\"🎤 음성 인식 결과: {transcription}\")\n",
    "    return transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실제 데이터 경로 설정\n",
    "    default_path = \"Traindata_2/KsponSpeech_04/PreprocessData_04\"\n",
    "    \n",
    "    data_path = input(f\"📂 데이터 디렉토리 경로 (기본값: {default_path}): \").strip()\n",
    "    if not data_path:\n",
    "        data_path = default_path\n",
    "    \n",
    "    config.DATA_DIR = data_path\n",
    "    \n",
    "    # 경로 검증\n",
    "    text_dir = Path(data_path) / \"g2p_texts\"\n",
    "    audio_dir = Path(data_path) / \"audio_features\"\n",
    "    \n",
    "    print(f\"\\n📁 설정된 경로:\")\n",
    "    print(f\"  - 기본 경로: {data_path}\")\n",
    "    print(f\"  - 텍스트: {text_dir}\")\n",
    "    print(f\"  - 오디오: {audio_dir}\")\n",
    "    \n",
    "    if not text_dir.exists():\n",
    "        print(f\"❌ 텍스트 디렉토리가 존재하지 않습니다: {text_dir}\")\n",
    "        exit(1)\n",
    "    \n",
    "    if not audio_dir.exists():\n",
    "        print(f\"❌ 오디오 디렉토리가 존재하지 않습니다: {audio_dir}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 파일 수 미리 확인\n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    audio_npy_files = list(audio_dir.glob(\"*.npy\"))\n",
    "    audio_npz_files = list(audio_dir.glob(\"*.npz\"))\n",
    "    audio_files = audio_npy_files + audio_npz_files\n",
    "    \n",
    "    print(f\"\\n📊 파일 수 확인:\")\n",
    "    print(f\"  - 텍스트 파일: {len(text_files):,}개\")\n",
    "    print(f\"  - 오디오 파일: {len(audio_files):,}개 (.npy: {len(audio_npy_files)}, .npz: {len(audio_npz_files)})\")\n",
    "    \n",
    "    if len(text_files) == 0 or len(audio_files) == 0:\n",
    "        print(\"❌ 처리할 파일이 없습니다!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 확인 후 진행\n",
    "    proceed = input(f\"\\n🚀 {min(len(text_files), len(audio_files)):,}개 파일로 학습을 시작하시겠습니까? (y/N): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"학습을 중단합니다.\")\n",
    "        exit(0)\n",
    "    \n",
    "    # 학습 실행\n",
    "    main()\n",
    "    \n",
    "    # 테스트 (옵션)\n",
    "    test_audio = input(\"\\n🎵 테스트할 오디오 특성 파일 경로 (선택사항, Enter로 건너뛰기): \").strip()\n",
    "    if test_audio and os.path.exists(test_audio):\n",
    "        try:\n",
    "            test_inference(config.OUTPUT_DIR, test_audio)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 추론 테스트 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ccc726-2c88-4d3b-9c4f-1d29bfa5e120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.30.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.24.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2024.6.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\" in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.10.11)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (2.4.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (5.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (1.15.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1; extra == \"http\"->fsspec[http]>=2021.05.0->evaluate) (0.2.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d44b7b1b-d8e1-4310-a8f7-abf61bb2b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mKsponSpeech_04\u001b[0m/  \u001b[01;34mPreprocessData_04\u001b[0m/  Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ffadc9-ed1d-4036-853e-391c1c342e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "📂 데이터 디렉토리 경로 (기본값: Traindata_2/KsponSpeech_04/PreprocessData_04):  KsponSpeech_04/PreprocessData_04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 설정된 경로:\n",
      "  - 기본 경로: KsponSpeech_04/PreprocessData_04\n",
      "  - 텍스트: KsponSpeech_04/PreprocessData_04/g2p_texts\n",
      "  - 오디오: KsponSpeech_04/PreprocessData_04/audio_features\n",
      "\n",
      "📊 파일 수 확인:\n",
      "  - 텍스트 파일: 124,000개\n",
      "  - 오디오 파일: 124,000개 (.npy: 0, .npz: 124000)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 124,000개 파일로 학습을 시작하시겠습니까? (y/N):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Whisper 파인튜닝 시작!\n",
      "📊 데이터셋 크기: 60,000개\n",
      "🎯 모델: openai/whisper-small\n",
      "⚡ GPU: 사용 가능\n",
      "📂 데이터셋 로딩 중...\n",
      "📁 텍스트 디렉토리: KsponSpeech_04/PreprocessData_04/g2p_texts\n",
      "🎵 오디오 디렉토리: KsponSpeech_04/PreprocessData_04/audio_features\n",
      "📝 텍스트 파일 수: 124,000개\n",
      "📊 진행 상황: 10,000개 매칭됨\n",
      "📊 진행 상황: 20,000개 매칭됨\n",
      "📊 진행 상황: 30,000개 매칭됨\n",
      "📊 진행 상황: 40,000개 매칭됨\n",
      "📊 진행 상황: 50,000개 매칭됨\n",
      "📊 진행 상황: 60,000개 매칭됨\n",
      "📊 진행 상황: 70,000개 매칭됨\n",
      "📊 진행 상황: 80,000개 매칭됨\n",
      "📊 진행 상황: 90,000개 매칭됨\n",
      "📊 진행 상황: 100,000개 매칭됨\n",
      "📊 진행 상황: 110,000개 매칭됨\n",
      "📊 진행 상황: 120,000개 매칭됨\n",
      "✅ 총 124,000개 파일 쌍 매칭됨\n",
      "📊 데이터 검증:\n",
      "  - 평균 텍스트 길이: 39.8자\n",
      "  - 최대 텍스트 길이: 384자\n",
      "  - 빈 텍스트: 0개\n",
      "  - 유효한 데이터: 124,000개\n",
      "📊 최종 데이터 분할:\n",
      "  - 학습: 48,000개\n",
      "  - 검증: 6,000개\n",
      "  - 테스트: 6,000개\n",
      "🤖 모델 초기화 중...\n",
      "✅ 모델 로드 완료: openai/whisper-small\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 48000/48000 [04:38<00:00, 172.44 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:31<00:00, 190.15 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:31<00:00, 191.56 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 학습 시작!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 501/4500 1:13:16 < 9:47:12, 0.11 it/s, Epoch 0.33/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  4/750 00:03 < 14:13, 0.87 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.33 GiB. GPU 0 has a total capacity of 23.69 GiB of which 463.00 MiB is free. Process 264299 has 17.13 GiB memory in use. Process 1990663 has 6.09 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 553\u001b[0m\n\u001b[1;32m    550\u001b[0m     exit(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    552\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m--> 553\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# 테스트 (옵션)\u001b[39;00m\n\u001b[1;32m    556\u001b[0m test_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎵 테스트할 오디오 특성 파일 경로 (선택사항, Enter로 건너뛰기): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[4], line 445\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# 7. 학습 시작\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎓 학습 시작!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 445\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# 8. 모델 저장\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m💾 모델 저장 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3004\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3002\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2958\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2958\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2959\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2961\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_seq2seq.py:195\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3975\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3972\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3974\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3975\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3985\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:4196\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4194\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   4195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4196\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4198\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:322\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:136\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    139\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:100\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtensor1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m result[: tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], : tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor1\n\u001b[1;32m    102\u001b[0m result[tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] :, : tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor2\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.33 GiB. GPU 0 has a total capacity of 23.69 GiB of which 463.00 MiB is free. Process 264299 has 17.13 GiB memory in use. Process 1990663 has 6.09 GiB memory in use. Of the allocated memory 4.12 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Whisper Fine-tuning for Korean STT\n",
    "Dataset: 124,000 KsponSpeech samples\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict  # load_metric 제거\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate  # 새로 추가\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 설정 및 경로\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"학습 설정\"\"\"\n",
    "    \n",
    "    # 데이터 경로\n",
    "    DATA_DIR = \"/path/to/your/processed_data\"  # 실제 경로로 수정 필요\n",
    "    OUTPUT_DIR = \"./whisper-korean-finetuned\"\n",
    "    \n",
    "    # 모델 설정\n",
    "    MODEL_NAME = \"openai/whisper-small\"  # small, base, large 중 선택\n",
    "    LANGUAGE = \"korean\"\n",
    "    TASK = \"transcribe\"\n",
    "    \n",
    "    # 데이터셋 설정\n",
    "    TRAIN_SIZE = 48000    # 6만개의 80%\n",
    "    VAL_SIZE = 6000       # 6만개의 10% \n",
    "    TEST_SIZE = 6000      # 6만개의 10%\n",
    "    MAX_INPUT_LENGTH = 40.0  # 40초\n",
    "    \n",
    "    # 학습 하이퍼파라미터\n",
    "    BATCH_SIZE = 8        # GPU 메모리에 따라 조정 (16→8로 줄임)\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4  # 2→4로 늘림 (실효 배치 크기 유지)\n",
    "    LEARNING_RATE = 1e-5\n",
    "    NUM_EPOCHS = 3        # 5→3으로 줄임\n",
    "    WARMUP_STEPS = 500\n",
    "    \n",
    "    # 체크포인트 설정\n",
    "    SAVE_STEPS = 1000\n",
    "    EVAL_STEPS = 500\n",
    "    SAVE_TOTAL_LIMIT = 3\n",
    "    \n",
    "    # 하드웨어 설정\n",
    "    DATALOADER_NUM_WORKERS = 0  # 멀티프로세싱 비활성화\n",
    "    USE_FP16 = True  # Mixed precision\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# =============================================================================\n",
    "# 데이터 로딩 및 전처리\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset_from_directory(data_dir: str) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    실제 전처리된 데이터 구조에서 데이터셋 로드\n",
    "    - 텍스트: g2p_texts/*.txt\n",
    "    - 오디오 특성: audio_features/*.npy 또는 *.npz\n",
    "    \"\"\"\n",
    "    print(\"📂 데이터셋 로딩 중...\")\n",
    "    \n",
    "    # 실제 경로 설정\n",
    "    base_dir = Path(data_dir)\n",
    "    text_dir = base_dir / \"g2p_texts\"\n",
    "    audio_dir = base_dir / \"audio_features\"\n",
    "    \n",
    "    print(f\"📁 텍스트 디렉토리: {text_dir}\")\n",
    "    print(f\"🎵 오디오 디렉토리: {audio_dir}\")\n",
    "    \n",
    "    # 디렉토리 존재 확인\n",
    "    if not text_dir.exists():\n",
    "        raise FileNotFoundError(f\"텍스트 디렉토리가 없습니다: {text_dir}\")\n",
    "    if not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"오디오 디렉토리가 없습니다: {audio_dir}\")\n",
    "    \n",
    "    # 매칭되는 파일 쌍 찾기\n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    \n",
    "    # 텍스트 파일 기준으로 매칭\n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    print(f\"📝 텍스트 파일 수: {len(text_files):,}개\")\n",
    "    \n",
    "    matched_count = 0\n",
    "    for text_file in text_files:\n",
    "        # 대응하는 오디오 특성 파일 찾기\n",
    "        base_name = text_file.stem\n",
    "        \n",
    "        # .npy 또는 .npz 파일 찾기\n",
    "        audio_npy = audio_dir / f\"{base_name}.npy\"\n",
    "        audio_npz = audio_dir / f\"{base_name}.npz\"\n",
    "        \n",
    "        audio_file = None\n",
    "        if audio_npy.exists():\n",
    "            audio_file = audio_npy\n",
    "        elif audio_npz.exists():\n",
    "            audio_file = audio_npz\n",
    "        \n",
    "        if audio_file:\n",
    "            # 텍스트 읽기\n",
    "            try:\n",
    "                with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                    transcript = f.read().strip()\n",
    "                \n",
    "                if transcript:  # 빈 텍스트 제외\n",
    "                    audio_files.append(str(audio_file))\n",
    "                    transcripts.append(transcript)\n",
    "                    matched_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 텍스트 파일 읽기 실패: {text_file}, 오류: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if matched_count % 10000 == 0 and matched_count > 0:\n",
    "            print(f\"📊 진행 상황: {matched_count:,}개 매칭됨\")\n",
    "    \n",
    "    print(f\"✅ 총 {len(audio_files):,}개 파일 쌍 매칭됨\")\n",
    "    \n",
    "    if len(audio_files) == 0:\n",
    "        raise ValueError(\"매칭되는 파일이 없습니다. 경로와 파일 형식을 확인해주세요.\")\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame({\n",
    "        'audio_path': audio_files,\n",
    "        'transcript': transcripts\n",
    "    })\n",
    "    \n",
    "    # 데이터 검증\n",
    "    print(f\"📊 데이터 검증:\")\n",
    "    print(f\"  - 평균 텍스트 길이: {df['transcript'].str.len().mean():.1f}자\")\n",
    "    print(f\"  - 최대 텍스트 길이: {df['transcript'].str.len().max()}자\")\n",
    "    print(f\"  - 빈 텍스트: {df['transcript'].str.len().eq(0).sum()}개\")\n",
    "    \n",
    "    # 빈 텍스트 제거\n",
    "    df = df[df['transcript'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"  - 유효한 데이터: {len(df):,}개\")\n",
    "    \n",
    "    # 데이터 분할 (사용 가능한 데이터에 맞춰 조정)\n",
    "    total_size = len(df)\n",
    "    actual_train_size = min(config.TRAIN_SIZE, int(total_size * 0.8))\n",
    "    actual_val_size = min(config.VAL_SIZE, int(total_size * 0.1))\n",
    "    actual_test_size = min(config.TEST_SIZE, int(total_size * 0.1))\n",
    "    \n",
    "    # 데이터 셔플\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    train_df = df[:actual_train_size]\n",
    "    val_df = df[actual_train_size:actual_train_size + actual_val_size]\n",
    "    test_df = df[actual_train_size + actual_val_size:actual_train_size + actual_val_size + actual_test_size]\n",
    "    \n",
    "    print(f\"📊 최종 데이터 분할:\")\n",
    "    print(f\"  - 학습: {len(train_df):,}개\")\n",
    "    print(f\"  - 검증: {len(val_df):,}개\") \n",
    "    print(f\"  - 테스트: {len(test_df):,}개\")\n",
    "    \n",
    "    # Hugging Face Dataset으로 변환\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "\n",
    "def load_audio_features(audio_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    전처리된 오디오 특성 파일 로드 (.npy 또는 .npz)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if audio_path.endswith('.npy'):\n",
    "            # .npy 파일 로드\n",
    "            features = np.load(audio_path)\n",
    "        elif audio_path.endswith('.npz'):\n",
    "            # .npz 파일 로드\n",
    "            data = np.load(audio_path)\n",
    "            # 첫 번째 키의 데이터 사용 (또는 특정 키 지정)\n",
    "            key = list(data.keys())[0]\n",
    "            features = data[key]\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 파일 형식: {audio_path}\")\n",
    "        \n",
    "        # 특성이 올바른 형태인지 확인\n",
    "        if features.ndim == 1:\n",
    "            # 1D 배열이면 2D로 변환 (time_steps, features)\n",
    "            features = features.reshape(-1, 1)\n",
    "        elif features.ndim == 2:\n",
    "            # 2D 배열이면 그대로 사용\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"⚠️ 예상치 못한 특성 형태: {features.shape} in {audio_path}\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오디오 특성 로드 실패: {audio_path}, 오류: {e}\")\n",
    "        # 기본 특성 반환 (에러 방지)\n",
    "        return np.zeros((1, 80))  # Whisper 멜 스펙트로그램 차원\n",
    "\n",
    "def load_audio(audio_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    오디오 파일 로드 (WAV 파일용 - 실제로는 특성 파일을 로드)\n",
    "    \"\"\"\n",
    "    # 실제로는 전처리된 특성 파일을 로드\n",
    "    return load_audio_features(audio_path)\n",
    "\n",
    "# =============================================================================\n",
    "# 모델 및 프로세서 초기화  \n",
    "# =============================================================================\n",
    "\n",
    "def initialize_model_and_processor():\n",
    "    \"\"\"Whisper 모델과 프로세서 초기화\"\"\"\n",
    "    print(\"🤖 모델 초기화 중...\")\n",
    "    \n",
    "    # 프로세서 구성요소\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(config.MODEL_NAME)\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\n",
    "        config.MODEL_NAME, \n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    processor = WhisperProcessor.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    # 언어 토큰 설정\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # 한국어 설정\n",
    "    model.generation_config.language = config.LANGUAGE\n",
    "    model.generation_config.task = config.TASK\n",
    "    \n",
    "    print(f\"✅ 모델 로드 완료: {config.MODEL_NAME}\")\n",
    "    \n",
    "    return model, processor, feature_extractor, tokenizer\n",
    "\n",
    "# =============================================================================\n",
    "# 데이터 전처리 및 콜레이터\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    음성-텍스트 데이터 콜레이터\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 오디오 입력 패딩\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 레이블 패딩\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # -100으로 패딩된 토큰을 마스킹 (손실 계산에서 제외)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # 시작 토큰이 있다면 제거 (모델이 자동으로 추가)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch, processor, feature_extractor):\n",
    "    \"\"\"\n",
    "    전처리된 특성 데이터를 위한 데이터셋 준비 함수\n",
    "    \"\"\"\n",
    "    \n",
    "    # 전처리된 오디오 특성 로드\n",
    "    audio_features = load_audio_features(batch[\"audio_path\"])\n",
    "    \n",
    "    # 특성 형태 확인 및 조정\n",
    "    if audio_features.ndim == 2:\n",
    "        # (time_steps, features) → (features, time_steps) 변환이 필요할 수 있음\n",
    "        if audio_features.shape[1] > audio_features.shape[0]:\n",
    "            # 일반적으로 (features, time_steps) 형태가 맞음\n",
    "            pass\n",
    "        else:\n",
    "            # (time_steps, features) → (features, time_steps)\n",
    "            audio_features = audio_features.T\n",
    "    \n",
    "    # Whisper input_features 형태로 변환\n",
    "    # Whisper는 (80, 3000) 형태의 멜 스펙트로그램을 기대\n",
    "    target_time_steps = 3000  # 30초 * 100 (10ms 프레임)\n",
    "    \n",
    "    if audio_features.shape[1] > target_time_steps:\n",
    "        # 너무 긴 경우 자르기\n",
    "        audio_features = audio_features[:, :target_time_steps]\n",
    "    elif audio_features.shape[1] < target_time_steps:\n",
    "        # 너무 짧은 경우 패딩\n",
    "        pad_width = target_time_steps - audio_features.shape[1]\n",
    "        audio_features = np.pad(audio_features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # 특성 차원 조정 (80차원으로 맞추기)\n",
    "    if audio_features.shape[0] != 80:\n",
    "        if audio_features.shape[0] < 80:\n",
    "            # 부족한 차원 패딩\n",
    "            pad_features = 80 - audio_features.shape[0]\n",
    "            audio_features = np.pad(audio_features, ((0, pad_features), (0, 0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            # 초과 차원 자르기\n",
    "            audio_features = audio_features[:80, :]\n",
    "    \n",
    "    # input_features로 설정\n",
    "    batch[\"input_features\"] = audio_features.astype(np.float32)\n",
    "    \n",
    "    # 텍스트 토큰화\n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"transcript\"],\n",
    "        truncation=True,\n",
    "        max_length=448,  # Whisper 최대 길이\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# =============================================================================\n",
    "# 평가 메트릭\n",
    "# =============================================================================\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"WER 및 CER 계산\"\"\"\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    # -100을 패딩 토큰으로 교체\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # 디코딩\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # WER 계산 - evaluate 라이브러리 사용\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# =============================================================================\n",
    "# 학습 실행\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 학습 함수\"\"\"\n",
    "    \n",
    "    print(\"🚀 Whisper 파인튜닝 시작!\")\n",
    "    print(f\"📊 데이터셋 크기: {config.TRAIN_SIZE + config.VAL_SIZE + config.TEST_SIZE:,}개\")\n",
    "    print(f\"🎯 모델: {config.MODEL_NAME}\")\n",
    "    print(f\"⚡ GPU: {'사용 가능' if torch.cuda.is_available() else '사용 불가'}\")\n",
    "    \n",
    "    # 1. 데이터셋 로드\n",
    "    dataset = load_dataset_from_directory(config.DATA_DIR)\n",
    "    \n",
    "    # 2. 모델 초기화\n",
    "    model, processor, feature_extractor, tokenizer = initialize_model_and_processor()\n",
    "    \n",
    "    # 3. 데이터 전처리\n",
    "    print(\"🔄 데이터 전처리 중...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: prepare_dataset(batch, processor, feature_extractor),\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        num_proc=1  # 싱글 프로세스로 변경\n",
    "    )\n",
    "    \n",
    "    # 4. 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # 5. 학습 설정\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        warmup_steps=config.WARMUP_STEPS,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=config.EVAL_STEPS,\n",
    "        save_steps=config.SAVE_STEPS,\n",
    "        save_total_limit=config.SAVE_TOTAL_LIMIT,\n",
    "        logging_steps=100,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "        dataloader_num_workers=0,  # 멀티프로세싱 비활성화\n",
    "        fp16=config.USE_FP16,\n",
    "        gradient_checkpointing=True,  # 메모리 절약\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "    \n",
    "    # 6. 트레이너 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # 7. 학습 시작\n",
    "    print(\"🎓 학습 시작!\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # 8. 모델 저장\n",
    "    print(\"💾 모델 저장 중...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(config.OUTPUT_DIR)\n",
    "    \n",
    "    # 9. 테스트 세트 평가\n",
    "    print(\"📊 테스트 세트 평가 중...\")\n",
    "    test_results = trainer.evaluate(dataset[\"test\"])\n",
    "    print(f\"🎯 최종 테스트 WER: {test_results['eval_wer']:.4f}\")\n",
    "    \n",
    "    # 10. 결과 저장\n",
    "    results = {\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"dataset_size\": len(dataset[\"train\"]) + len(dataset[\"validation\"]),\n",
    "        \"test_wer\": test_results['eval_wer'],\n",
    "        \"config\": config.__dict__\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(config.OUTPUT_DIR, \"training_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"🎉 파인튜닝 완료!\")\n",
    "    print(f\"📁 모델 저장 위치: {config.OUTPUT_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 간단한 추론 테스트\n",
    "# =============================================================================\n",
    "\n",
    "def test_inference(model_path: str, audio_path: str):\n",
    "    \"\"\"학습된 모델로 추론 테스트\"\"\"\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "    processor = WhisperProcessor.from_pretrained(model_path)\n",
    "    \n",
    "    # 오디오 로드\n",
    "    audio = load_audio(audio_path)\n",
    "    \n",
    "    # 특성 추출\n",
    "    input_features = processor(\n",
    "        audio, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # 추론\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    \n",
    "    # 디코딩\n",
    "    transcription = processor.batch_decode(\n",
    "        predicted_ids, \n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    print(f\"🎤 음성 인식 결과: {transcription}\")\n",
    "    return transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실제 데이터 경로 설정\n",
    "    default_path = \"Traindata_2/KsponSpeech_04/PreprocessData_04\"\n",
    "    \n",
    "    data_path = input(f\"📂 데이터 디렉토리 경로 (기본값: {default_path}): \").strip()\n",
    "    if not data_path:\n",
    "        data_path = default_path\n",
    "    \n",
    "    config.DATA_DIR = data_path\n",
    "    \n",
    "    # 경로 검증\n",
    "    text_dir = Path(data_path) / \"g2p_texts\"\n",
    "    audio_dir = Path(data_path) / \"audio_features\"\n",
    "    \n",
    "    print(f\"\\n📁 설정된 경로:\")\n",
    "    print(f\"  - 기본 경로: {data_path}\")\n",
    "    print(f\"  - 텍스트: {text_dir}\")\n",
    "    print(f\"  - 오디오: {audio_dir}\")\n",
    "    \n",
    "    if not text_dir.exists():\n",
    "        print(f\"❌ 텍스트 디렉토리가 존재하지 않습니다: {text_dir}\")\n",
    "        exit(1)\n",
    "    \n",
    "    if not audio_dir.exists():\n",
    "        print(f\"❌ 오디오 디렉토리가 존재하지 않습니다: {audio_dir}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 파일 수 미리 확인\n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    audio_npy_files = list(audio_dir.glob(\"*.npy\"))\n",
    "    audio_npz_files = list(audio_dir.glob(\"*.npz\"))\n",
    "    audio_files = audio_npy_files + audio_npz_files\n",
    "    \n",
    "    print(f\"\\n📊 파일 수 확인:\")\n",
    "    print(f\"  - 텍스트 파일: {len(text_files):,}개\")\n",
    "    print(f\"  - 오디오 파일: {len(audio_files):,}개 (.npy: {len(audio_npy_files)}, .npz: {len(audio_npz_files)})\")\n",
    "    \n",
    "    if len(text_files) == 0 or len(audio_files) == 0:\n",
    "        print(\"❌ 처리할 파일이 없습니다!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 확인 후 진행\n",
    "    proceed = input(f\"\\n🚀 {min(len(text_files), len(audio_files)):,}개 파일로 학습을 시작하시겠습니까? (y/N): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"학습을 중단합니다.\")\n",
    "        exit(0)\n",
    "    \n",
    "    # 학습 실행\n",
    "    main()\n",
    "    \n",
    "    # 테스트 (옵션)\n",
    "    test_audio = input(\"\\n🎵 테스트할 오디오 특성 파일 경로 (선택사항, Enter로 건너뛰기): \").strip()\n",
    "    if test_audio and os.path.exists(test_audio):\n",
    "        try:\n",
    "            test_inference(config.OUTPUT_DIR, test_audio)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 추론 테스트 실패: {e}\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee396a1-0a64-42c2-9e9d-8ebbbae68b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jun 11 07:29:39 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3090        Off | 00000000:CA:00.0 Off |                  N/A |\n",
      "| 61%   53C    P8              28W / 350W |  23796MiB / 24576MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa5abf28-f640-4e76-89c3-f8c235c11a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-12 05:06:16.538392: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-12 05:06:16.588093: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-12 05:06:17.337876: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "📂 데이터 디렉토리 경로 (기본값: Traindata_2/KsponSpeech_04/PreprocessData_04):  KsponSpeech_04/PreprocessData_04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 설정된 경로:\n",
      "  - 기본 경로: KsponSpeech_04/PreprocessData_04\n",
      "  - 텍스트: KsponSpeech_04/PreprocessData_04/g2p_texts\n",
      "  - 오디오: KsponSpeech_04/PreprocessData_04/audio_features\n",
      "\n",
      "📊 파일 수 확인:\n",
      "  - 텍스트 파일: 124,000개\n",
      "  - 오디오 파일: 124,000개 (.npy: 0, .npz: 124000)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 124,000개 파일로 학습을 시작하시겠습니까? (y/N):  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Whisper 파인튜닝 시작!\n",
      "📊 데이터셋 크기: 40,000개\n",
      "🎯 모델: openai/whisper-small\n",
      "⚡ GPU: 사용 가능\n",
      "📂 데이터셋 로딩 중...\n",
      "📁 텍스트 디렉토리: KsponSpeech_04/PreprocessData_04/g2p_texts\n",
      "🎵 오디오 디렉토리: KsponSpeech_04/PreprocessData_04/audio_features\n",
      "📝 텍스트 파일 수: 124,000개\n",
      "📊 진행 상황: 10,000개 매칭됨\n",
      "📊 진행 상황: 20,000개 매칭됨\n",
      "📊 진행 상황: 30,000개 매칭됨\n",
      "📊 진행 상황: 40,000개 매칭됨\n",
      "📊 진행 상황: 50,000개 매칭됨\n",
      "📊 진행 상황: 60,000개 매칭됨\n",
      "📊 진행 상황: 70,000개 매칭됨\n",
      "📊 진행 상황: 80,000개 매칭됨\n",
      "📊 진행 상황: 90,000개 매칭됨\n",
      "📊 진행 상황: 100,000개 매칭됨\n",
      "📊 진행 상황: 110,000개 매칭됨\n",
      "📊 진행 상황: 120,000개 매칭됨\n",
      "✅ 총 124,000개 파일 쌍 매칭됨\n",
      "📊 데이터 검증:\n",
      "  - 평균 텍스트 길이: 39.8자\n",
      "  - 최대 텍스트 길이: 384자\n",
      "  - 빈 텍스트: 0개\n",
      "  - 유효한 데이터: 124,000개\n",
      "📊 최종 데이터 분할:\n",
      "  - 학습: 32,000개\n",
      "  - 검증: 4,000개\n",
      "  - 테스트: 4,000개\n",
      "🤖 모델 초기화 중...\n",
      "✅ 모델 로드 완료: openai/whisper-small\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 32000/32000 [02:41<00:00, 198.59 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:19<00:00, 208.63 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:19<00:00, 209.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 학습 시작!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2001' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2001/3000 4:59:20 < 2:29:35, 0.11 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  10/1000 00:04 < 08:35, 1.92 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.10 GiB is free. Process 264299 has 17.13 GiB memory in use. Process 2004479 has 5.44 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 871.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 559\u001b[0m\n\u001b[1;32m    556\u001b[0m     exit(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# 학습 실행\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# 테스트 (옵션)\u001b[39;00m\n\u001b[1;32m    562\u001b[0m test_audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎵 테스트할 오디오 특성 파일 경로 (선택사항, Enter로 건너뛰기): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[0;32mIn[1], line 451\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# 7. 학습 시작\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎓 학습 시작!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 451\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# 8. 모델 저장\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m💾 모델 저장 중...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2550\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3004\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3002\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3003\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3004\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3006\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   3007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2958\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2958\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2959\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2961\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_seq2seq.py:195\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3975\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3972\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3974\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3975\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3985\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:4196\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4194\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   4195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4196\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4198\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:322\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:136\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    139\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:100\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     97\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Now let's fill the result tensor\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtensor1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m result[: tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], : tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor1\n\u001b[1;32m    102\u001b[0m result[tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] :, : tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m tensor2\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.45 GiB. GPU 0 has a total capacity of 23.69 GiB of which 1.10 GiB is free. Process 264299 has 17.13 GiB memory in use. Process 2004479 has 5.44 GiB memory in use. Of the allocated memory 4.27 GiB is allocated by PyTorch, and 871.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Whisper Fine-tuning for Korean STT\n",
    "Dataset: 124,000 KsponSpeech samples\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict  # load_metric 제거\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate  # 새로 추가\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =============================================================================\n",
    "# 설정 및 경로\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"학습 설정\"\"\"\n",
    "    \n",
    "    # 데이터 경로\n",
    "    DATA_DIR = \"/path/to/your/processed_data\"  # 실제 경로로 수정 필요\n",
    "    OUTPUT_DIR = \"./whisper-korean-finetuned\"\n",
    "    \n",
    "    # 모델 설정\n",
    "    MODEL_NAME = \"openai/whisper-small\"  # small, base, large 중 선택\n",
    "    LANGUAGE = \"korean\"\n",
    "    TASK = \"transcribe\"\n",
    "    \n",
    "    # 데이터셋 설정\n",
    "    TRAIN_SIZE = 32000    # 4만개의 80%\n",
    "    VAL_SIZE = 4000       # 4만개의 10% \n",
    "    TEST_SIZE = 4000      # 4만개의 10%\n",
    "    MAX_INPUT_LENGTH = 30.0  # 30초\n",
    "    \n",
    "    # 학습 하이퍼파라미터\n",
    "    BATCH_SIZE = 4        # 8→4로 더 줄임 (메모리 절약)\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8  # 4→8로 늘림 (실효 배치 크기 유지)\n",
    "    LEARNING_RATE = 1e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 500\n",
    "    \n",
    "    # 평가 설정 (메모리 절약)\n",
    "    EVAL_BATCH_SIZE = 2   # 평가 배치 크기 줄임\n",
    "    EVAL_STEPS = 3000     # 평가 주기 늘림 (메모리 절약)\n",
    "    \n",
    "    # 체크포인트 설정\n",
    "    SAVE_STEPS = 2000\n",
    "    EVAL_STEPS = 2000\n",
    "    SAVE_TOTAL_LIMIT = 2\n",
    "    \n",
    "    # 하드웨어 설정\n",
    "    DATALOADER_NUM_WORKERS = 0  # 멀티프로세싱 비활성화\n",
    "    USE_FP16 = True  # Mixed precision\n",
    "    \n",
    "    MAX_EVAL_SAMPLES = 100\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# =============================================================================\n",
    "# 데이터 로딩 및 전처리\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset_from_directory(data_dir: str) -> DatasetDict:\n",
    "    \"\"\"\n",
    "    실제 전처리된 데이터 구조에서 데이터셋 로드\n",
    "    - 텍스트: g2p_texts/*.txt\n",
    "    - 오디오 특성: audio_features/*.npy 또는 *.npz\n",
    "    \"\"\"\n",
    "    print(\"📂 데이터셋 로딩 중...\")\n",
    "    \n",
    "    # 실제 경로 설정\n",
    "    base_dir = Path(data_dir)\n",
    "    text_dir = base_dir / \"g2p_texts\"\n",
    "    audio_dir = base_dir / \"audio_features\"\n",
    "    \n",
    "    print(f\"📁 텍스트 디렉토리: {text_dir}\")\n",
    "    print(f\"🎵 오디오 디렉토리: {audio_dir}\")\n",
    "    \n",
    "    # 디렉토리 존재 확인\n",
    "    if not text_dir.exists():\n",
    "        raise FileNotFoundError(f\"텍스트 디렉토리가 없습니다: {text_dir}\")\n",
    "    if not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"오디오 디렉토리가 없습니다: {audio_dir}\")\n",
    "    \n",
    "    # 매칭되는 파일 쌍 찾기\n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    \n",
    "    # 텍스트 파일 기준으로 매칭\n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    print(f\"📝 텍스트 파일 수: {len(text_files):,}개\")\n",
    "    \n",
    "    matched_count = 0\n",
    "    for text_file in text_files:\n",
    "        # 대응하는 오디오 특성 파일 찾기\n",
    "        base_name = text_file.stem\n",
    "        \n",
    "        # .npy 또는 .npz 파일 찾기\n",
    "        audio_npy = audio_dir / f\"{base_name}.npy\"\n",
    "        audio_npz = audio_dir / f\"{base_name}.npz\"\n",
    "        \n",
    "        audio_file = None\n",
    "        if audio_npy.exists():\n",
    "            audio_file = audio_npy\n",
    "        elif audio_npz.exists():\n",
    "            audio_file = audio_npz\n",
    "        \n",
    "        if audio_file:\n",
    "            # 텍스트 읽기\n",
    "            try:\n",
    "                with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                    transcript = f.read().strip()\n",
    "                \n",
    "                if transcript:  # 빈 텍스트 제외\n",
    "                    audio_files.append(str(audio_file))\n",
    "                    transcripts.append(transcript)\n",
    "                    matched_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ 텍스트 파일 읽기 실패: {text_file}, 오류: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # 진행 상황 출력\n",
    "        if matched_count % 10000 == 0 and matched_count > 0:\n",
    "            print(f\"📊 진행 상황: {matched_count:,}개 매칭됨\")\n",
    "    \n",
    "    print(f\"✅ 총 {len(audio_files):,}개 파일 쌍 매칭됨\")\n",
    "    \n",
    "    if len(audio_files) == 0:\n",
    "        raise ValueError(\"매칭되는 파일이 없습니다. 경로와 파일 형식을 확인해주세요.\")\n",
    "    \n",
    "    # 데이터프레임 생성\n",
    "    df = pd.DataFrame({\n",
    "        'audio_path': audio_files,\n",
    "        'transcript': transcripts\n",
    "    })\n",
    "    \n",
    "    # 데이터 검증\n",
    "    print(f\"📊 데이터 검증:\")\n",
    "    print(f\"  - 평균 텍스트 길이: {df['transcript'].str.len().mean():.1f}자\")\n",
    "    print(f\"  - 최대 텍스트 길이: {df['transcript'].str.len().max()}자\")\n",
    "    print(f\"  - 빈 텍스트: {df['transcript'].str.len().eq(0).sum()}개\")\n",
    "    \n",
    "    # 빈 텍스트 제거\n",
    "    df = df[df['transcript'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"  - 유효한 데이터: {len(df):,}개\")\n",
    "    \n",
    "    # 데이터 분할 (사용 가능한 데이터에 맞춰 조정)\n",
    "    total_size = len(df)\n",
    "    actual_train_size = min(config.TRAIN_SIZE, int(total_size * 0.8))\n",
    "    actual_val_size = min(config.VAL_SIZE, int(total_size * 0.1))\n",
    "    actual_test_size = min(config.TEST_SIZE, int(total_size * 0.1))\n",
    "    \n",
    "    # 데이터 셔플\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    train_df = df[:actual_train_size]\n",
    "    val_df = df[actual_train_size:actual_train_size + actual_val_size]\n",
    "    test_df = df[actual_train_size + actual_val_size:actual_train_size + actual_val_size + actual_test_size]\n",
    "    \n",
    "    print(f\"📊 최종 데이터 분할:\")\n",
    "    print(f\"  - 학습: {len(train_df):,}개\")\n",
    "    print(f\"  - 검증: {len(val_df):,}개\") \n",
    "    print(f\"  - 테스트: {len(test_df):,}개\")\n",
    "    \n",
    "    # Hugging Face Dataset으로 변환\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    val_dataset = Dataset.from_pandas(val_df)\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset,\n",
    "        \"test\": test_dataset\n",
    "    })\n",
    "\n",
    "def load_audio_features(audio_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    전처리된 오디오 특성 파일 로드 (.npy 또는 .npz)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if audio_path.endswith('.npy'):\n",
    "            # .npy 파일 로드\n",
    "            features = np.load(audio_path)\n",
    "        elif audio_path.endswith('.npz'):\n",
    "            # .npz 파일 로드\n",
    "            data = np.load(audio_path)\n",
    "            # 첫 번째 키의 데이터 사용 (또는 특정 키 지정)\n",
    "            key = list(data.keys())[0]\n",
    "            features = data[key]\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 파일 형식: {audio_path}\")\n",
    "        \n",
    "        # 특성이 올바른 형태인지 확인\n",
    "        if features.ndim == 1:\n",
    "            # 1D 배열이면 2D로 변환 (time_steps, features)\n",
    "            features = features.reshape(-1, 1)\n",
    "        elif features.ndim == 2:\n",
    "            # 2D 배열이면 그대로 사용\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"⚠️ 예상치 못한 특성 형태: {features.shape} in {audio_path}\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오디오 특성 로드 실패: {audio_path}, 오류: {e}\")\n",
    "        # 기본 특성 반환 (에러 방지)\n",
    "        return np.zeros((1, 80))  # Whisper 멜 스펙트로그램 차원\n",
    "\n",
    "def load_audio(audio_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    오디오 파일 로드 (WAV 파일용 - 실제로는 특성 파일을 로드)\n",
    "    \"\"\"\n",
    "    # 실제로는 전처리된 특성 파일을 로드\n",
    "    return load_audio_features(audio_path)\n",
    "\n",
    "# =============================================================================\n",
    "# 모델 및 프로세서 초기화  \n",
    "# =============================================================================\n",
    "\n",
    "def initialize_model_and_processor():\n",
    "    \"\"\"Whisper 모델과 프로세서 초기화\"\"\"\n",
    "    print(\"🤖 모델 초기화 중...\")\n",
    "    \n",
    "    # 프로세서 구성요소\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(config.MODEL_NAME)\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\n",
    "        config.MODEL_NAME, \n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    processor = WhisperProcessor.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    # 언어 토큰 설정\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    \n",
    "    # 한국어 설정\n",
    "    model.generation_config.language = config.LANGUAGE\n",
    "    model.generation_config.task = config.TASK\n",
    "    \n",
    "    print(f\"✅ 모델 로드 완료: {config.MODEL_NAME}\")\n",
    "    \n",
    "    return model, processor, feature_extractor, tokenizer\n",
    "\n",
    "# =============================================================================\n",
    "# 데이터 전처리 및 콜레이터\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"\n",
    "    음성-텍스트 데이터 콜레이터\n",
    "    \"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # 오디오 입력 패딩\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # 레이블 패딩\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # -100으로 패딩된 토큰을 마스킹 (손실 계산에서 제외)\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        # 시작 토큰이 있다면 제거 (모델이 자동으로 추가)\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch, processor, feature_extractor):\n",
    "    \"\"\"\n",
    "    전처리된 특성 데이터를 위한 데이터셋 준비 함수\n",
    "    \"\"\"\n",
    "    \n",
    "    # 전처리된 오디오 특성 로드\n",
    "    audio_features = load_audio_features(batch[\"audio_path\"])\n",
    "    \n",
    "    # 특성 형태 확인 및 조정\n",
    "    if audio_features.ndim == 2:\n",
    "        # (time_steps, features) → (features, time_steps) 변환이 필요할 수 있음\n",
    "        if audio_features.shape[1] > audio_features.shape[0]:\n",
    "            # 일반적으로 (features, time_steps) 형태가 맞음\n",
    "            pass\n",
    "        else:\n",
    "            # (time_steps, features) → (features, time_steps)\n",
    "            audio_features = audio_features.T\n",
    "    \n",
    "    # Whisper input_features 형태로 변환\n",
    "    # Whisper는 (80, 3000) 형태의 멜 스펙트로그램을 기대\n",
    "    target_time_steps = 3000  # 30초 * 100 (10ms 프레임)\n",
    "    \n",
    "    if audio_features.shape[1] > target_time_steps:\n",
    "        # 너무 긴 경우 자르기\n",
    "        audio_features = audio_features[:, :target_time_steps]\n",
    "    elif audio_features.shape[1] < target_time_steps:\n",
    "        # 너무 짧은 경우 패딩\n",
    "        pad_width = target_time_steps - audio_features.shape[1]\n",
    "        audio_features = np.pad(audio_features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    \n",
    "    # 특성 차원 조정 (80차원으로 맞추기)\n",
    "    if audio_features.shape[0] != 80:\n",
    "        if audio_features.shape[0] < 80:\n",
    "            # 부족한 차원 패딩\n",
    "            pad_features = 80 - audio_features.shape[0]\n",
    "            audio_features = np.pad(audio_features, ((0, pad_features), (0, 0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            # 초과 차원 자르기\n",
    "            audio_features = audio_features[:80, :]\n",
    "    \n",
    "    # input_features로 설정\n",
    "    batch[\"input_features\"] = audio_features.astype(np.float32)\n",
    "    \n",
    "    # 텍스트 토큰화\n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"transcript\"],\n",
    "        truncation=True,\n",
    "        max_length=448,  # Whisper 최대 길이\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# =============================================================================\n",
    "# 평가 메트릭\n",
    "# =============================================================================\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"WER 및 CER 계산\"\"\"\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    # -100을 패딩 토큰으로 교체\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    # 디코딩\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # WER 계산 - evaluate 라이브러리 사용\n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# =============================================================================\n",
    "# 학습 실행\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 학습 함수\"\"\"\n",
    "    \n",
    "    print(\"🚀 Whisper 파인튜닝 시작!\")\n",
    "    print(f\"📊 데이터셋 크기: {config.TRAIN_SIZE + config.VAL_SIZE + config.TEST_SIZE:,}개\")\n",
    "    print(f\"🎯 모델: {config.MODEL_NAME}\")\n",
    "    print(f\"⚡ GPU: {'사용 가능' if torch.cuda.is_available() else '사용 불가'}\")\n",
    "    \n",
    "    # 1. 데이터셋 로드\n",
    "    dataset = load_dataset_from_directory(config.DATA_DIR)\n",
    "    \n",
    "    # 2. 모델 초기화\n",
    "    model, processor, feature_extractor, tokenizer = initialize_model_and_processor()\n",
    "    \n",
    "    # 3. 데이터 전처리\n",
    "    print(\"🔄 데이터 전처리 중...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: prepare_dataset(batch, processor, feature_extractor),\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        num_proc=1  # 싱글 프로세스로 변경\n",
    "    )\n",
    "    \n",
    "    # 4. 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # 5. 학습 설정\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        warmup_steps=config.WARMUP_STEPS,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=config.EVAL_STEPS,\n",
    "        save_steps=config.SAVE_STEPS,\n",
    "        save_total_limit=config.SAVE_TOTAL_LIMIT,\n",
    "        logging_steps=100,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "        dataloader_num_workers=0,  # 멀티프로세싱 비활성화\n",
    "        fp16=config.USE_FP16,\n",
    "        gradient_checkpointing=True,  # 메모리 절약\n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "    \n",
    "    # 6. 트레이너 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    # 7. 학습 시작\n",
    "    print(\"🎓 학습 시작!\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # 8. 모델 저장\n",
    "    print(\"💾 모델 저장 중...\")\n",
    "    trainer.save_model()\n",
    "    processor.save_pretrained(config.OUTPUT_DIR)\n",
    "    \n",
    "    # 9. 테스트 세트 평가\n",
    "    print(\"📊 테스트 세트 평가 중...\")\n",
    "    test_results = trainer.evaluate(dataset[\"test\"])\n",
    "    print(f\"🎯 최종 테스트 WER: {test_results['eval_wer']:.4f}\")\n",
    "    \n",
    "    # 10. 결과 저장\n",
    "    results = {\n",
    "        \"model_name\": config.MODEL_NAME,\n",
    "        \"dataset_size\": len(dataset[\"train\"]) + len(dataset[\"validation\"]),\n",
    "        \"test_wer\": test_results['eval_wer'],\n",
    "        \"config\": config.__dict__\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(config.OUTPUT_DIR, \"training_results.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(\"🎉 파인튜닝 완료!\")\n",
    "    print(f\"📁 모델 저장 위치: {config.OUTPUT_DIR}\")\n",
    "\n",
    "# =============================================================================\n",
    "# 간단한 추론 테스트\n",
    "# =============================================================================\n",
    "\n",
    "def test_inference(model_path: str, audio_path: str):\n",
    "    \"\"\"학습된 모델로 추론 테스트\"\"\"\n",
    "    \n",
    "    # 모델 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "    processor = WhisperProcessor.from_pretrained(model_path)\n",
    "    \n",
    "    # 오디오 로드\n",
    "    audio = load_audio(audio_path)\n",
    "    \n",
    "    # 특성 추출\n",
    "    input_features = processor(\n",
    "        audio, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # 추론\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    \n",
    "    # 디코딩\n",
    "    transcription = processor.batch_decode(\n",
    "        predicted_ids, \n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "    \n",
    "    print(f\"🎤 음성 인식 결과: {transcription}\")\n",
    "    return transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실제 데이터 경로 설정\n",
    "    default_path = \"Traindata_2/KsponSpeech_04/PreprocessData_04\"\n",
    "    \n",
    "    data_path = input(f\"📂 데이터 디렉토리 경로 (기본값: {default_path}): \").strip()\n",
    "    if not data_path:\n",
    "        data_path = default_path\n",
    "    \n",
    "    config.DATA_DIR = data_path\n",
    "    \n",
    "    # 경로 검증\n",
    "    text_dir = Path(data_path) / \"g2p_texts\"\n",
    "    audio_dir = Path(data_path) / \"audio_features\"\n",
    "    \n",
    "    print(f\"\\n📁 설정된 경로:\")\n",
    "    print(f\"  - 기본 경로: {data_path}\")\n",
    "    print(f\"  - 텍스트: {text_dir}\")\n",
    "    print(f\"  - 오디오: {audio_dir}\")\n",
    "    \n",
    "    if not text_dir.exists():\n",
    "        print(f\"❌ 텍스트 디렉토리가 존재하지 않습니다: {text_dir}\")\n",
    "        exit(1)\n",
    "    \n",
    "    if not audio_dir.exists():\n",
    "        print(f\"❌ 오디오 디렉토리가 존재하지 않습니다: {audio_dir}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 파일 수 미리 확인\n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    audio_npy_files = list(audio_dir.glob(\"*.npy\"))\n",
    "    audio_npz_files = list(audio_dir.glob(\"*.npz\"))\n",
    "    audio_files = audio_npy_files + audio_npz_files\n",
    "    \n",
    "    print(f\"\\n📊 파일 수 확인:\")\n",
    "    print(f\"  - 텍스트 파일: {len(text_files):,}개\")\n",
    "    print(f\"  - 오디오 파일: {len(audio_files):,}개 (.npy: {len(audio_npy_files)}, .npz: {len(audio_npz_files)})\")\n",
    "    \n",
    "    if len(text_files) == 0 or len(audio_files) == 0:\n",
    "        print(\"❌ 처리할 파일이 없습니다!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # 확인 후 진행\n",
    "    proceed = input(f\"\\n🚀 {min(len(text_files), len(audio_files)):,}개 파일로 학습을 시작하시겠습니까? (y/N): \").strip().lower()\n",
    "    if proceed != 'y':\n",
    "        print(\"학습을 중단합니다.\")\n",
    "        exit(0)\n",
    "    \n",
    "    # 학습 실행\n",
    "    main()\n",
    "    \n",
    "    # 테스트 (옵션)\n",
    "    test_audio = input(\"\\n🎵 테스트할 오디오 특성 파일 경로 (선택사항, Enter로 건너뛰기): \").strip()\n",
    "    if test_audio and os.path.exists(test_audio):\n",
    "        try:\n",
    "            test_inference(config.OUTPUT_DIR, test_audio)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 추론 테스트 실패: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2869ebd3-c9a2-4bc5-aa09-10f600138cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 체크포인트에서 안전하게 재시작!\n",
      "⚠️  기존 하이퍼파라미터 유지 (배치 크기, 학습률 등)\n",
      "✅ 메모리 절약 설정만 적용 (평가 관련)\n",
      "❌ 체크포인트가 없습니다!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "안전한 체크포인트 재시작 - 기존 설정 유지\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "\n",
    "# =============================================================================\n",
    "# 기존 설정 그대로 유지 (중요!)\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"기존 학습 설정 그대로 유지\"\"\"\n",
    "    \n",
    "    # 데이터 경로\n",
    "    DATA_DIR = \"KsponSpeech_04/PreprocessData_04\"\n",
    "    OUTPUT_DIR = \"./whisper-korean-finetuned\"\n",
    "    \n",
    "    # 모델 설정 (그대로 유지)\n",
    "    MODEL_NAME = \"openai/whisper-small\"\n",
    "    LANGUAGE = \"korean\"\n",
    "    TASK = \"transcribe\"\n",
    "    \n",
    "    # 데이터셋 설정 (그대로 유지)\n",
    "    TRAIN_SIZE = 32000\n",
    "    VAL_SIZE = 4000\n",
    "    TEST_SIZE = 4000\n",
    "    MAX_INPUT_LENGTH = 30.0\n",
    "    \n",
    "    # 하이퍼파라미터 (그대로 유지 - 중요!)\n",
    "    BATCH_SIZE = 4\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "    LEARNING_RATE = 1e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 500\n",
    "    \n",
    "    # 평가 설정 (메모리 절약만 적용)\n",
    "    EVAL_BATCH_SIZE = 1          # 2 → 1 (메모리 절약)\n",
    "    EVAL_STEPS = 4000            # 2000 → 4000 (평가 빈도 감소)\n",
    "    \n",
    "    # 체크포인트 설정 (안전하게 변경 가능)\n",
    "    SAVE_STEPS = 2000\n",
    "    SAVE_TOTAL_LIMIT = 1         # 2 → 1 (디스크 공간 절약)\n",
    "    \n",
    "    # 하드웨어 설정\n",
    "    DATALOADER_NUM_WORKERS = 0\n",
    "    USE_FP16 = True\n",
    "    \n",
    "    # 메모리 절약 설정 (안전)\n",
    "    MAX_EVAL_SAMPLES = 50        # 100 → 50\n",
    "    MAX_LENGTH = 448             # 원래 설정 유지\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"GPU 메모리 정리\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 기존 함수들 그대로 사용\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset_from_directory(data_dir: str) -> DatasetDict:\n",
    "    \"\"\"기존 데이터 로딩 함수 그대로\"\"\"\n",
    "    print(\"📂 데이터셋 로딩 중...\")\n",
    "    \n",
    "    base_dir = Path(data_dir)\n",
    "    text_dir = base_dir / \"g2p_texts\"\n",
    "    audio_dir = base_dir / \"audio_features\"\n",
    "    \n",
    "    if not text_dir.exists() or not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"디렉토리가 없습니다: {text_dir} 또는 {audio_dir}\")\n",
    "    \n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    \n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    print(f\"📝 텍스트 파일 수: {len(text_files):,}개\")\n",
    "    \n",
    "    matched_count = 0\n",
    "    for text_file in text_files:\n",
    "        base_name = text_file.stem\n",
    "        \n",
    "        audio_npy = audio_dir / f\"{base_name}.npy\"\n",
    "        audio_npz = audio_dir / f\"{base_name}.npz\"\n",
    "        \n",
    "        audio_file = None\n",
    "        if audio_npy.exists():\n",
    "            audio_file = audio_npy\n",
    "        elif audio_npz.exists():\n",
    "            audio_file = audio_npz\n",
    "        \n",
    "        if audio_file:\n",
    "            try:\n",
    "                with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                    transcript = f.read().strip()\n",
    "                \n",
    "                if transcript:\n",
    "                    audio_files.append(str(audio_file))\n",
    "                    transcripts.append(transcript)\n",
    "                    matched_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if matched_count % 10000 == 0 and matched_count > 0:\n",
    "            print(f\"📊 진행 상황: {matched_count:,}개 매칭됨\")\n",
    "    \n",
    "    print(f\"✅ 총 {len(audio_files):,}개 파일 쌍 매칭됨\")\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'audio_path': audio_files,\n",
    "        'transcript': transcripts\n",
    "    })\n",
    "    \n",
    "    # 빈 텍스트 제거\n",
    "    df = df[df['transcript'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"  - 유효한 데이터: {len(df):,}개\")\n",
    "    \n",
    "    # 데이터 분할 (기존과 동일)\n",
    "    total_size = len(df)\n",
    "    actual_train_size = min(config.TRAIN_SIZE, int(total_size * 0.8))\n",
    "    actual_val_size = min(config.VAL_SIZE, int(total_size * 0.1))\n",
    "    actual_test_size = min(config.TEST_SIZE, int(total_size * 0.1))\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    train_df = df[:actual_train_size]\n",
    "    val_df = df[actual_train_size:actual_train_size + actual_val_size]\n",
    "    test_df = df[actual_train_size + actual_val_size:actual_train_size + actual_val_size + actual_test_size]\n",
    "    \n",
    "    print(f\"📊 최종 데이터 분할:\")\n",
    "    print(f\"  - 학습: {len(train_df):,}개\")\n",
    "    print(f\"  - 검증: {len(val_df):,}개\") \n",
    "    print(f\"  - 테스트: {len(test_df):,}개\")\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"validation\": Dataset.from_pandas(val_df),\n",
    "        \"test\": Dataset.from_pandas(test_df)\n",
    "    })\n",
    "\n",
    "def load_audio_features(audio_path: str) -> np.ndarray:\n",
    "    \"\"\"기존 오디오 로딩 함수 그대로\"\"\"\n",
    "    try:\n",
    "        if audio_path.endswith('.npy'):\n",
    "            features = np.load(audio_path)\n",
    "        elif audio_path.endswith('.npz'):\n",
    "            data = np.load(audio_path)\n",
    "            key = list(data.keys())[0]\n",
    "            features = data[key]\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 파일 형식: {audio_path}\")\n",
    "        \n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1, 1)\n",
    "        elif features.ndim == 2:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"⚠️ 예상치 못한 특성 형태: {features.shape} in {audio_path}\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오디오 특성 로드 실패: {audio_path}, 오류: {e}\")\n",
    "        return np.zeros((1, 80))\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"기존 데이터 콜레이터 그대로\"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch, processor, feature_extractor):\n",
    "    \"\"\"기존 데이터 준비 함수 그대로\"\"\"\n",
    "    audio_features = load_audio_features(batch[\"audio_path\"])\n",
    "    \n",
    "    if audio_features.ndim == 2:\n",
    "        if audio_features.shape[1] > audio_features.shape[0]:\n",
    "            pass\n",
    "        else:\n",
    "            audio_features = audio_features.T\n",
    "    \n",
    "    target_time_steps = 3000  # 기존 설정 유지\n",
    "    \n",
    "    if audio_features.shape[1] > target_time_steps:\n",
    "        audio_features = audio_features[:, :target_time_steps]\n",
    "    elif audio_features.shape[1] < target_time_steps:\n",
    "        pad_width = target_time_steps - audio_features.shape[1]\n",
    "        audio_features = np.pad(audio_features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    \n",
    "    if audio_features.shape[0] != 80:\n",
    "        if audio_features.shape[0] < 80:\n",
    "            pad_features = 80 - audio_features.shape[0]\n",
    "            audio_features = np.pad(audio_features, ((0, pad_features), (0, 0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            audio_features = audio_features[:80, :]\n",
    "    \n",
    "    batch[\"input_features\"] = audio_features.astype(np.float32)\n",
    "    \n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"transcript\"],\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_LENGTH,  # 기존 448 유지\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"메모리 절약만 적용한 메트릭 계산\"\"\"\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    # 메모리 절약을 위해 샘플 수만 제한\n",
    "    if len(pred_ids) > config.MAX_EVAL_SAMPLES:\n",
    "        pred_ids = pred_ids[:config.MAX_EVAL_SAMPLES]\n",
    "        label_ids = label_ids[:config.MAX_EVAL_SAMPLES]\n",
    "    \n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# =============================================================================\n",
    "# 안전한 재시작 함수\n",
    "# =============================================================================\n",
    "\n",
    "def safe_resume_training():\n",
    "    \"\"\"기존 설정을 유지하면서 안전하게 재시작\"\"\"\n",
    "    \n",
    "    print(\"🔄 체크포인트에서 안전하게 재시작!\")\n",
    "    print(\"⚠️  기존 하이퍼파라미터 유지 (배치 크기, 학습률 등)\")\n",
    "    print(\"✅ 메모리 절약 설정만 적용 (평가 관련)\")\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    # 1. 체크포인트 확인\n",
    "    checkpoint_dir = Path(config.OUTPUT_DIR)\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint-*\"))\n",
    "    \n",
    "    if not checkpoints:\n",
    "        print(\"❌ 체크포인트가 없습니다!\")\n",
    "        return\n",
    "    \n",
    "    latest_checkpoint = max(checkpoints, key=lambda x: int(x.name.split('-')[1]))\n",
    "    print(f\"📂 사용할 체크포인트: {latest_checkpoint}\")\n",
    "    \n",
    "    # 2. 데이터셋 로드 (기존과 동일)\n",
    "    dataset = load_dataset_from_directory(config.DATA_DIR)\n",
    "    \n",
    "    # 3. 모델 로드 (체크포인트에서)\n",
    "    print(\"🤖 체크포인트에서 모델 로딩...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(latest_checkpoint)\n",
    "    processor = WhisperProcessor.from_pretrained(latest_checkpoint)\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(latest_checkpoint)\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(latest_checkpoint)\n",
    "    \n",
    "    print(\"✅ 체크포인트 로드 완료\")\n",
    "    \n",
    "    # 4. 데이터 전처리 (기존과 동일)\n",
    "    print(\"🔄 데이터 전처리 중...\")\n",
    "    dataset = dataset.map(\n",
    "        lambda batch: prepare_dataset(batch, processor, feature_extractor),\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "    \n",
    "    # 5. 데이터 콜레이터 (기존과 동일)\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # 6. 학습 설정 (기존 유지 + 메모리 절약)\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        \n",
    "        # 기존 하이퍼파라미터 유지 (중요!)\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,              # 4 유지\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,  # 8 유지\n",
    "        learning_rate=config.LEARNING_RATE,                        # 1e-5 유지\n",
    "        num_train_epochs=config.NUM_EPOCHS,                        # 3 유지\n",
    "        warmup_steps=config.WARMUP_STEPS,                          # 500 유지\n",
    "        \n",
    "        # 메모리 절약 변경\n",
    "        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,          # 1로 감소\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=config.EVAL_STEPS,                              # 4000으로 증가\n",
    "        \n",
    "        # 체크포인트/로깅 설정 (안전하게 변경)\n",
    "        save_steps=config.SAVE_STEPS,\n",
    "        save_total_limit=config.SAVE_TOTAL_LIMIT,                   # 1로 감소\n",
    "        logging_steps=200,                                         # 로깅 빈도 증가\n",
    "        \n",
    "        # 기타 설정\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "        \n",
    "        # 하드웨어 설정\n",
    "        dataloader_num_workers=config.DATALOADER_NUM_WORKERS,\n",
    "        fp16=config.USE_FP16,\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        # 평가 메모리 절약\n",
    "        eval_accumulation_steps=4,\n",
    "        \n",
    "        report_to=[\"tensorboard\"],\n",
    "    )\n",
    "    \n",
    "    # 7. 트레이너 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    \n",
    "    # 8. 안전한 재시작\n",
    "    print(\"🎓 체크포인트에서 학습 재시작!\")\n",
    "    print(f\"📊 설정 확인:\")\n",
    "    print(f\"  - 배치 크기: {config.BATCH_SIZE} (기존 유지)\")\n",
    "    print(f\"  - 그래디언트 누적: {config.GRADIENT_ACCUMULATION_STEPS} (기존 유지)\")\n",
    "    print(f\"  - 실효 배치: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS} (기존 유지)\")\n",
    "    print(f\"  - 학습률: {config.LEARNING_RATE} (기존 유지)\")\n",
    "    print(f\"  - 평가 배치: {config.EVAL_BATCH_SIZE} (메모리 절약)\")\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        # 체크포인트에서 자동 재시작\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "        \n",
    "        print(\"💾 모델 저장 중...\")\n",
    "        trainer.save_model()\n",
    "        processor.save_pretrained(config.OUTPUT_DIR)\n",
    "        \n",
    "        print(\"📊 테스트 세트 평가 중...\")\n",
    "        test_results = trainer.evaluate(dataset[\"test\"])\n",
    "        print(f\"🎯 최종 테스트 WER: {test_results['eval_wer']:.4f}\")\n",
    "        \n",
    "        print(\"🎉 학습 완료!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 학습 중 오류 발생: {e}\")\n",
    "        print(\"💡 여전히 메모리 부족이면 평가 빈도를 더 늘려보세요 (EVAL_STEPS = 8000)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    safe_resume_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a07fcc31-938e-4ec2-811f-4b720cf77fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 메모리 최적화 학습 시작!\n",
      "💪 이번엔 반드시 완주합니다!\n",
      "\n",
      "🎯 최적화 내용:\n",
      "  - 배치 크기: 2 (메모리 절약)\n",
      "  - 그래디언트 누적: 16 (실효 배치 32 유지)\n",
      "  - 오디오 길이: 25초 (30초→25초)\n",
      "  - 토큰 길이: 384 (448→384)\n",
      "  - 평가: 거의 안 함 (메모리 절약)\n",
      "  - 저장: 1000스텝마다 (더 자주)\n",
      "\n",
      "📂 기존 체크포인트 발견: whisper-korean-optimized/checkpoint-3000\n",
      "📂 데이터셋 로딩 중...\n",
      "📝 텍스트 파일 수: 124,000개\n",
      "📊 진행 상황: 10,000개 매칭됨\n",
      "📊 진행 상황: 20,000개 매칭됨\n",
      "📊 진행 상황: 30,000개 매칭됨\n",
      "📊 진행 상황: 40,000개 매칭됨\n",
      "📊 진행 상황: 50,000개 매칭됨\n",
      "📊 진행 상황: 60,000개 매칭됨\n",
      "📊 진행 상황: 70,000개 매칭됨\n",
      "📊 진행 상황: 80,000개 매칭됨\n",
      "📊 진행 상황: 90,000개 매칭됨\n",
      "📊 진행 상황: 100,000개 매칭됨\n",
      "📊 진행 상황: 110,000개 매칭됨\n",
      "📊 진행 상황: 120,000개 매칭됨\n",
      "✅ 총 124,000개 파일 쌍 매칭됨\n",
      "  - 유효한 데이터: 124,000개\n",
      "📊 최종 데이터 분할:\n",
      "  - 학습: 32,000개\n",
      "  - 검증: 4,000개\n",
      "  - 테스트: 4,000개\n",
      "🤖 체크포인트에서 모델 로딩...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'whisper-korean-optimized/checkpoint-3000'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'whisper-korean-optimized/checkpoint-3000' is the correct path to a directory containing all relevant files for a WhisperTokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 459\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 459\u001b[0m     \u001b[43msafe_resume_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 310\u001b[0m, in \u001b[0;36msafe_resume_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤖 체크포인트에서 모델 로딩...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    309\u001b[0m model \u001b[38;5;241m=\u001b[39m WhisperForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(latest_checkpoint)\n\u001b[0;32m--> 310\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mWhisperProcessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatest_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m feature_extractor \u001b[38;5;241m=\u001b[39m WhisperFeatureExtractor\u001b[38;5;241m.\u001b[39mfrom_pretrained(latest_checkpoint)\n\u001b[1;32m    312\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m WhisperTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(latest_checkpoint)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/processing_utils.py:944\u001b[0m, in \u001b[0;36mProcessorMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token\n\u001b[0;32m--> 944\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_arguments_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    945\u001b[0m processor_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mget_processor_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_args_and_dict(args, processor_dict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/processing_utils.py:990\u001b[0m, in \u001b[0;36mProcessorMixin._get_arguments_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    988\u001b[0m         attribute_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(transformers_module, class_name)\n\u001b[0;32m--> 990\u001b[0m     args\u001b[38;5;241m.\u001b[39mappend(\u001b[43mattribute_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m args\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2197\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[0;32m-> 2197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   2198\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2199\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2200\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2202\u001b[0m     )\n\u001b[1;32m   2204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'whisper-korean-optimized/checkpoint-3000'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'whisper-korean-optimized/checkpoint-3000' is the correct path to a directory containing all relevant files for a WhisperTokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "안전한 체크포인트 재시작 - 기존 설정 유지\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "\n",
    "# =============================================================================\n",
    "# 기존 설정 그대로 유지 (중요!)\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"메모리 최적화된 학습 설정 - 완주 보장\"\"\"\n",
    "    \n",
    "    # 데이터 경로\n",
    "    DATA_DIR = \"KsponSpeech_04/PreprocessData_04\"\n",
    "    OUTPUT_DIR = \"./whisper-korean-optimized\"  # 새 디렉토리\n",
    "    \n",
    "    # 모델 설정\n",
    "    MODEL_NAME = \"openai/whisper-small\"\n",
    "    LANGUAGE = \"korean\"\n",
    "    TASK = \"transcribe\"\n",
    "    \n",
    "    # 데이터셋 설정 (원하는 크기 유지)\n",
    "    TRAIN_SIZE = 32000      # 원래대로 32K 유지\n",
    "    VAL_SIZE = 4000         # 원래대로 4K 유지  \n",
    "    TEST_SIZE = 4000        # 원래대로 4K 유지\n",
    "    MAX_INPUT_LENGTH = 30.0 # 30초 유지\n",
    "    \n",
    "    # 하이퍼파라미터 (RTX 3090에 맞는 설정)\n",
    "    BATCH_SIZE = 4              # 원래 설정 유지 (3090이면 충분)\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8   # 원래 설정 유지\n",
    "    LEARNING_RATE = 1e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 500\n",
    "    \n",
    "    # 평가 설정 (대폭 축소)\n",
    "    EVAL_BATCH_SIZE = 1\n",
    "    EVAL_STEPS = 999999999      # 사실상 평가 안 함 (메모리 절약)\n",
    "    \n",
    "    # 체크포인트 설정 (자주 저장)\n",
    "    SAVE_STEPS = 1000           # 2000 → 1000 (더 자주 저장)\n",
    "    SAVE_TOTAL_LIMIT = 1\n",
    "    \n",
    "    # 하드웨어 설정\n",
    "    DATALOADER_NUM_WORKERS = 0\n",
    "    USE_FP16 = True\n",
    "    \n",
    "    # 메모리 절약 설정\n",
    "    MAX_EVAL_SAMPLES = 10       # 50 → 10 (최소)\n",
    "    MAX_LENGTH = 384            # 448 → 384 (토큰 길이 감소)\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"GPU 메모리 정리\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 기존 함수들 그대로 사용\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset_from_directory(data_dir: str) -> DatasetDict:\n",
    "    \"\"\"기존 데이터 로딩 함수 그대로\"\"\"\n",
    "    print(\"📂 데이터셋 로딩 중...\")\n",
    "    \n",
    "    base_dir = Path(data_dir)\n",
    "    text_dir = base_dir / \"g2p_texts\"\n",
    "    audio_dir = base_dir / \"audio_features\"\n",
    "    \n",
    "    if not text_dir.exists() or not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"디렉토리가 없습니다: {text_dir} 또는 {audio_dir}\")\n",
    "    \n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    \n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    print(f\"📝 텍스트 파일 수: {len(text_files):,}개\")\n",
    "    \n",
    "    matched_count = 0\n",
    "    for text_file in text_files:\n",
    "        base_name = text_file.stem\n",
    "        \n",
    "        audio_npy = audio_dir / f\"{base_name}.npy\"\n",
    "        audio_npz = audio_dir / f\"{base_name}.npz\"\n",
    "        \n",
    "        audio_file = None\n",
    "        if audio_npy.exists():\n",
    "            audio_file = audio_npy\n",
    "        elif audio_npz.exists():\n",
    "            audio_file = audio_npz\n",
    "        \n",
    "        if audio_file:\n",
    "            try:\n",
    "                with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                    transcript = f.read().strip()\n",
    "                \n",
    "                if transcript:\n",
    "                    audio_files.append(str(audio_file))\n",
    "                    transcripts.append(transcript)\n",
    "                    matched_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if matched_count % 10000 == 0 and matched_count > 0:\n",
    "            print(f\"📊 진행 상황: {matched_count:,}개 매칭됨\")\n",
    "    \n",
    "    print(f\"✅ 총 {len(audio_files):,}개 파일 쌍 매칭됨\")\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'audio_path': audio_files,\n",
    "        'transcript': transcripts\n",
    "    })\n",
    "    \n",
    "    # 빈 텍스트 제거\n",
    "    df = df[df['transcript'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"  - 유효한 데이터: {len(df):,}개\")\n",
    "    \n",
    "    # 데이터 분할 (기존과 동일)\n",
    "    total_size = len(df)\n",
    "    actual_train_size = min(config.TRAIN_SIZE, int(total_size * 0.8))\n",
    "    actual_val_size = min(config.VAL_SIZE, int(total_size * 0.1))\n",
    "    actual_test_size = min(config.TEST_SIZE, int(total_size * 0.1))\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    train_df = df[:actual_train_size]\n",
    "    val_df = df[actual_train_size:actual_train_size + actual_val_size]\n",
    "    test_df = df[actual_train_size + actual_val_size:actual_train_size + actual_val_size + actual_test_size]\n",
    "    \n",
    "    print(f\"📊 최종 데이터 분할:\")\n",
    "    print(f\"  - 학습: {len(train_df):,}개\")\n",
    "    print(f\"  - 검증: {len(val_df):,}개\") \n",
    "    print(f\"  - 테스트: {len(test_df):,}개\")\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"validation\": Dataset.from_pandas(val_df),\n",
    "        \"test\": Dataset.from_pandas(test_df)\n",
    "    })\n",
    "\n",
    "def load_audio_features(audio_path: str) -> np.ndarray:\n",
    "    \"\"\"기존 오디오 로딩 함수 그대로\"\"\"\n",
    "    try:\n",
    "        if audio_path.endswith('.npy'):\n",
    "            features = np.load(audio_path)\n",
    "        elif audio_path.endswith('.npz'):\n",
    "            data = np.load(audio_path)\n",
    "            key = list(data.keys())[0]\n",
    "            features = data[key]\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 파일 형식: {audio_path}\")\n",
    "        \n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1, 1)\n",
    "        elif features.ndim == 2:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"⚠️ 예상치 못한 특성 형태: {features.shape} in {audio_path}\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오디오 특성 로드 실패: {audio_path}, 오류: {e}\")\n",
    "        return np.zeros((1, 80))\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"기존 데이터 콜레이터 그대로\"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch, processor, feature_extractor):\n",
    "    \"\"\"기존 데이터 준비 함수 그대로\"\"\"\n",
    "    audio_features = load_audio_features(batch[\"audio_path\"])\n",
    "    \n",
    "    if audio_features.ndim == 2:\n",
    "        if audio_features.shape[1] > audio_features.shape[0]:\n",
    "            pass\n",
    "        else:\n",
    "            audio_features = audio_features.T\n",
    "    \n",
    "    target_time_steps = 3000  # Whisper 필수 요구사항 (변경 불가)\n",
    "    \n",
    "    if audio_features.shape[1] > target_time_steps:\n",
    "        audio_features = audio_features[:, :target_time_steps]\n",
    "    elif audio_features.shape[1] < target_time_steps:\n",
    "        pad_width = target_time_steps - audio_features.shape[1]\n",
    "        audio_features = np.pad(audio_features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    \n",
    "    if audio_features.shape[0] != 80:\n",
    "        if audio_features.shape[0] < 80:\n",
    "            pad_features = 80 - audio_features.shape[0]\n",
    "            audio_features = np.pad(audio_features, ((0, pad_features), (0, 0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            audio_features = audio_features[:80, :]\n",
    "    \n",
    "    batch[\"input_features\"] = audio_features.astype(np.float32)\n",
    "    \n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"transcript\"],\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_LENGTH,  # 기존 448 유지\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"메모리 절약만 적용한 메트릭 계산\"\"\"\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    # 메모리 절약을 위해 샘플 수만 제한\n",
    "    if len(pred_ids) > config.MAX_EVAL_SAMPLES:\n",
    "        pred_ids = pred_ids[:config.MAX_EVAL_SAMPLES]\n",
    "        label_ids = label_ids[:config.MAX_EVAL_SAMPLES]\n",
    "    \n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# =============================================================================\n",
    "# 안전한 재시작 함수\n",
    "# =============================================================================\n",
    "\n",
    "def safe_resume_training():\n",
    "    \"\"\"처음부터 메모리 최적화 학습 시작\"\"\"\n",
    "    \n",
    "    print(\"🚀 메모리 최적화 학습 시작!\")\n",
    "    print(\"💪 이번엔 반드시 완주합니다!\")\n",
    "    print()\n",
    "    print(\"🎯 최적화 내용:\")\n",
    "    print(f\"  - 배치 크기: 2 (메모리 절약)\")\n",
    "    print(f\"  - 그래디언트 누적: 16 (실효 배치 32 유지)\")\n",
    "    print(f\"  - 오디오 길이: 25초 (30초→25초)\")\n",
    "    print(f\"  - 토큰 길이: 384 (448→384)\")\n",
    "    print(f\"  - 평가: 거의 안 함 (메모리 절약)\")\n",
    "    print(f\"  - 저장: 1000스텝마다 (더 자주)\")\n",
    "    print()\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    # 1. 체크포인트 확인 (처음부터 시작하지만 혹시 있으면 사용)\n",
    "    checkpoint_dir = Path(config.OUTPUT_DIR)\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint-*\")) if checkpoint_dir.exists() else []\n",
    "    \n",
    "    latest_checkpoint = None\n",
    "    if checkpoints:\n",
    "        latest_checkpoint = max(checkpoints, key=lambda x: int(x.name.split('-')[1]))\n",
    "        print(f\"📂 기존 체크포인트 발견: {latest_checkpoint}\")\n",
    "    else:\n",
    "        print(\"🆕 처음부터 새로 시작\")\n",
    "    \n",
    "    # 2. 데이터셋 로드\n",
    "    dataset = load_dataset_from_directory(config.DATA_DIR)\n",
    "    \n",
    "    # 3. 모델 초기화 (체크포인트가 있으면 로드, 없으면 새로)\n",
    "    if latest_checkpoint:\n",
    "        print(\"🤖 체크포인트에서 모델 로딩...\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(latest_checkpoint)\n",
    "        processor = WhisperProcessor.from_pretrained(latest_checkpoint)\n",
    "        feature_extractor = WhisperFeatureExtractor.from_pretrained(latest_checkpoint)\n",
    "        tokenizer = WhisperTokenizer.from_pretrained(latest_checkpoint)\n",
    "    else:\n",
    "        print(\"🤖 새 모델 초기화...\")\n",
    "        feature_extractor = WhisperFeatureExtractor.from_pretrained(config.MODEL_NAME)\n",
    "        tokenizer = WhisperTokenizer.from_pretrained(\n",
    "            config.MODEL_NAME, \n",
    "            language=config.LANGUAGE,\n",
    "            task=config.TASK\n",
    "        )\n",
    "        processor = WhisperProcessor.from_pretrained(\n",
    "            config.MODEL_NAME,\n",
    "            language=config.LANGUAGE,\n",
    "            task=config.TASK\n",
    "        )\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(config.MODEL_NAME)\n",
    "        \n",
    "        # 언어 설정\n",
    "        model.config.forced_decoder_ids = None\n",
    "        model.config.suppress_tokens = []\n",
    "        model.generation_config.language = config.LANGUAGE\n",
    "        model.generation_config.task = config.TASK\n",
    "    \n",
    "    print(\"✅ 모델 준비 완료\")\n",
    "    \n",
    "    # 4. 데이터 전처리\n",
    "    print(\"🔄 데이터 전처리 중...\")\n",
    "    clear_memory()\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        lambda batch: prepare_dataset(batch, processor, feature_extractor),\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        num_proc=1,\n",
    "        batch_size=50  # 작은 배치로 처리\n",
    "    )\n",
    "    \n",
    "    # 5. 데이터 콜레이터\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # 6. 메모리 최적화 학습 설정\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        \n",
    "        # 메모리 최적화된 배치 설정\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,              # 2\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,  # 16\n",
    "        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,          # 1\n",
    "        \n",
    "        # 기본 하이퍼파라미터\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        warmup_steps=config.WARMUP_STEPS,\n",
    "        \n",
    "        # 평가 최소화 (메모리 절약)\n",
    "        evaluation_strategy=\"no\",           # 평가 안 함\n",
    "        # eval_steps=config.EVAL_STEPS,     # 주석 처리\n",
    "        \n",
    "        # 체크포인트 자주 저장\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config.SAVE_STEPS,       # 1000스텝마다\n",
    "        save_total_limit=config.SAVE_TOTAL_LIMIT,\n",
    "        logging_steps=100,\n",
    "        \n",
    "        # 기타 설정\n",
    "        remove_unused_columns=True,         # 사용하지 않는 컬럼 제거\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=False,       # 평가 안 하므로 False\n",
    "        push_to_hub=False,\n",
    "        \n",
    "        # 하드웨어 최적화\n",
    "        dataloader_num_workers=config.DATALOADER_NUM_WORKERS,\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=config.USE_FP16,\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        report_to=[],  # 텐서보드도 끔 (메모리 절약)\n",
    "    )\n",
    "    \n",
    "    # 7. 트레이너 설정 (평가 관련 제거)\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        # eval_dataset=dataset[\"validation\"],  # 평가 데이터셋 제거\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),  # 메트릭 제거\n",
    "        tokenizer=processor.feature_extractor,\n",
    "        # callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]  # 조기 종료 제거\n",
    "    )\n",
    "    \n",
    "    # 8. 학습 시작\n",
    "    print(\"🎓 최적화된 학습 시작!\")\n",
    "    print(f\"📊 설정 확인:\")\n",
    "    print(f\"  - 학습 데이터: {len(dataset['train']):,}개\")\n",
    "    print(f\"  - 배치 크기: {config.BATCH_SIZE}\")\n",
    "    print(f\"  - 그래디언트 누적: {config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  - 실효 배치: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  - 총 스텝: 약 {len(dataset['train']) // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS) * config.NUM_EPOCHS:,}\")\n",
    "    print(f\"  - 체크포인트: {config.SAVE_STEPS}스텝마다\")\n",
    "    print(f\"  - 평가: 없음 (메모리 절약)\")\n",
    "    print()\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        # 체크포인트가 있으면 이어서, 없으면 처음부터\n",
    "        if latest_checkpoint:\n",
    "            print(f\"🔄 {latest_checkpoint}에서 재시작...\")\n",
    "            trainer.train(resume_from_checkpoint=str(latest_checkpoint))\n",
    "        else:\n",
    "            print(\"🆕 처음부터 시작...\")\n",
    "            trainer.train()\n",
    "        \n",
    "        print(\"💾 최종 모델 저장 중...\")\n",
    "        trainer.save_model()\n",
    "        processor.save_pretrained(config.OUTPUT_DIR)\n",
    "        \n",
    "        # 마지막에 한 번만 테스트 평가\n",
    "        print(\"📊 최종 테스트 평가...\")\n",
    "        test_trainer = Seq2SeqTrainer(\n",
    "            args=Seq2SeqTrainingArguments(\n",
    "                output_dir=config.OUTPUT_DIR,\n",
    "                per_device_eval_batch_size=1,\n",
    "                fp16=True,\n",
    "            ),\n",
    "            model=model,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "            tokenizer=processor.feature_extractor,\n",
    "        )\n",
    "        \n",
    "        test_results = test_trainer.evaluate(dataset[\"test\"])\n",
    "        print(f\"🎯 최종 테스트 WER: {test_results['eval_wer']:.4f}\")\n",
    "        \n",
    "        print(\"🎉 학습 완전 완료!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 학습 중 오류 발생: {e}\")\n",
    "        print(\"💡 추가 최적화 제안:\")\n",
    "        print(\"  - BATCH_SIZE = 1\")\n",
    "        print(\"  - GRADIENT_ACCUMULATION_STEPS = 32\")\n",
    "        print(\"  - TRAIN_SIZE = 20000\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    safe_resume_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "898848b0-4a97-431d-9766-5ce76e27ad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Whisper 한국어 파인튜닝 시작!\n",
      "🆕 처음부터 새로 시작합니다!\n",
      "\n",
      "🎯 학습 설정:\n",
      "  - 모델: openai/whisper-small\n",
      "  - 배치 크기: 4\n",
      "  - 그래디언트 누적: 8\n",
      "  - 실효 배치: 32\n",
      "  - 학습률: 1e-05\n",
      "  - 에포크: 3\n",
      "  - 체크포인트: 1000스텝마다\n",
      "\n",
      "📂 데이터셋 로딩...\n",
      "📂 데이터셋 로딩 중...\n",
      "📝 텍스트 파일 수: 124,000개\n",
      "📊 진행 상황: 10,000개 매칭됨\n",
      "📊 진행 상황: 20,000개 매칭됨\n",
      "📊 진행 상황: 30,000개 매칭됨\n",
      "📊 진행 상황: 40,000개 매칭됨\n",
      "📊 진행 상황: 50,000개 매칭됨\n",
      "📊 진행 상황: 60,000개 매칭됨\n",
      "📊 진행 상황: 70,000개 매칭됨\n",
      "📊 진행 상황: 80,000개 매칭됨\n",
      "📊 진행 상황: 90,000개 매칭됨\n",
      "📊 진행 상황: 100,000개 매칭됨\n",
      "📊 진행 상황: 110,000개 매칭됨\n",
      "📊 진행 상황: 120,000개 매칭됨\n",
      "✅ 총 124,000개 파일 쌍 매칭됨\n",
      "  - 유효한 데이터: 124,000개\n",
      "📊 최종 데이터 분할:\n",
      "  - 학습: 32,000개\n",
      "  - 검증: 4,000개\n",
      "  - 테스트: 4,000개\n",
      "🤖 새 모델 초기화...\n",
      "✅ 모델 준비 완료\n",
      "🔄 데이터 전처리 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 32000/32000 [02:02<00:00, 261.19 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:15<00:00, 260.94 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:15<00:00, 263.28 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎓 학습 시작!\n",
      "📊 최종 설정 확인:\n",
      "  - 학습 데이터: 32,000개\n",
      "  - 총 스텝: 약 3,000\n",
      "  - 예상 시간: 약 50시간\n",
      "\n",
      "🆕 처음부터 새로 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 7:18:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.396500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.017200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.489800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.288100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.137600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.011200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.963600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.928200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.895900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.817600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.788900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.788600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.786000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.743800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.759500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.748300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.728800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.633000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.638600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.636500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.613100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.612900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.631800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.617700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 최종 모델 저장 중...\n",
      "📊 최종 테스트 평가...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='41' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  41/4000 00:05 < 08:50, 7.47 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ 학습 중 오류 발생: CUDA out of memory. Tried to allocate 186.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 107.00 MiB is free. Process 264299 has 17.13 GiB memory in use. Process 2019775 has 6.43 GiB memory in use. Of the allocated memory 5.38 GiB is allocated by PyTorch, and 746.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "💡 메모리 부족 시 추가 최적화 제안:\n",
      "  - BATCH_SIZE = 2\n",
      "  - GRADIENT_ACCUMULATION_STEPS = 16\n",
      "  - TRAIN_SIZE = 20000\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 186.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 107.00 MiB is free. Process 264299 has 17.13 GiB memory in use. Process 2019775 has 6.43 GiB memory in use. Of the allocated memory 5.38 GiB is allocated by PyTorch, and 746.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 434\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mtrain_from_scratch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 431\u001b[0m, in \u001b[0;36mtrain_from_scratch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - GRADIENT_ACCUMULATION_STEPS = 16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  - TRAIN_SIZE = 20000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 431\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[6], line 419\u001b[0m, in \u001b[0;36mtrain_from_scratch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📊 최종 테스트 평가...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m test_trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m    408\u001b[0m     args\u001b[38;5;241m=\u001b[39mSeq2SeqTrainingArguments(\n\u001b[1;32m    409\u001b[0m         output_dir\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mOUTPUT_DIR,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mprocessor\u001b[38;5;241m.\u001b[39mfeature_extractor,\n\u001b[1;32m    417\u001b[0m )\n\u001b[0;32m--> 419\u001b[0m test_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎯 최종 테스트 WER: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_wer\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎉 학습 완전 완료!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_seq2seq.py:195\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:3975\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3972\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3974\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3975\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3978\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3979\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3985\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3986\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:4196\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4194\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[1;32m   4195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_eval_metrics \u001b[38;5;129;01mor\u001b[39;00m description \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 4196\u001b[0m         \u001b[43mall_preds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4198\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:322\u001b[0m, in \u001b[0;36mEvalLoopContainer.add\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m tensors \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat \u001b[38;5;28;01melse\u001b[39;00m [tensors]\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_nested_concat:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors \u001b[38;5;241m=\u001b[39m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mappend(tensors)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:134\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(new_tensors)\n\u001b[1;32m    132\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:136\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    139\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer_pt_utils.py:94\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     91\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     97\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 186.00 MiB. GPU 0 has a total capacity of 23.69 GiB of which 107.00 MiB is free. Process 264299 has 17.13 GiB memory in use. Process 2019775 has 6.43 GiB memory in use. Of the allocated memory 5.38 GiB is allocated by PyTorch, and 746.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "처음부터 새로 시작하는 Whisper 한국어 파인튜닝\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    WhisperFeatureExtractor,\n",
    "    WhisperTokenizer,\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import librosa\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "import evaluate\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "\n",
    "# =============================================================================\n",
    "# 설정 클래스\n",
    "# =============================================================================\n",
    "\n",
    "class Config:\n",
    "    \"\"\"메모리 최적화된 학습 설정 - 완주 보장\"\"\"\n",
    "    \n",
    "    # 데이터 경로\n",
    "    DATA_DIR = \"KsponSpeech_04/PreprocessData_04\"\n",
    "    OUTPUT_DIR = \"./whisper-korean-fresh\"  # 새 디렉토리명\n",
    "    \n",
    "    # 모델 설정\n",
    "    MODEL_NAME = \"openai/whisper-small\"\n",
    "    LANGUAGE = \"korean\"\n",
    "    TASK = \"transcribe\"\n",
    "    \n",
    "    # 데이터셋 설정\n",
    "    TRAIN_SIZE = 32000\n",
    "    VAL_SIZE = 4000  \n",
    "    TEST_SIZE = 4000\n",
    "    MAX_INPUT_LENGTH = 30.0\n",
    "    \n",
    "    # 하이퍼파라미터 (RTX 3090에 맞는 설정)\n",
    "    BATCH_SIZE = 4\n",
    "    GRADIENT_ACCUMULATION_STEPS = 8\n",
    "    LEARNING_RATE = 1e-5\n",
    "    NUM_EPOCHS = 3\n",
    "    WARMUP_STEPS = 500\n",
    "    \n",
    "    # 평가 설정 (대폭 축소)\n",
    "    EVAL_BATCH_SIZE = 1\n",
    "    \n",
    "    # 체크포인트 설정 (자주 저장)\n",
    "    SAVE_STEPS = 1000\n",
    "    SAVE_TOTAL_LIMIT = 1\n",
    "    \n",
    "    # 하드웨어 설정\n",
    "    DATALOADER_NUM_WORKERS = 0\n",
    "    USE_FP16 = True\n",
    "    \n",
    "    # 메모리 절약 설정\n",
    "    MAX_EVAL_SAMPLES = 10\n",
    "    MAX_LENGTH = 384\n",
    "\n",
    "config = Config()\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"GPU 메모리 정리\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# 데이터 로딩 및 전처리 함수들\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset_from_directory(data_dir: str) -> DatasetDict:\n",
    "    \"\"\"기존 데이터 로딩 함수 그대로\"\"\"\n",
    "    print(\"📂 데이터셋 로딩 중...\")\n",
    "    \n",
    "    base_dir = Path(data_dir)\n",
    "    text_dir = base_dir / \"g2p_texts\"\n",
    "    audio_dir = base_dir / \"audio_features\"\n",
    "    \n",
    "    if not text_dir.exists() or not audio_dir.exists():\n",
    "        raise FileNotFoundError(f\"디렉토리가 없습니다: {text_dir} 또는 {audio_dir}\")\n",
    "    \n",
    "    audio_files = []\n",
    "    transcripts = []\n",
    "    \n",
    "    text_files = list(text_dir.glob(\"*.txt\"))\n",
    "    print(f\"📝 텍스트 파일 수: {len(text_files):,}개\")\n",
    "    \n",
    "    matched_count = 0\n",
    "    for text_file in text_files:\n",
    "        base_name = text_file.stem\n",
    "        \n",
    "        audio_npy = audio_dir / f\"{base_name}.npy\"\n",
    "        audio_npz = audio_dir / f\"{base_name}.npz\"\n",
    "        \n",
    "        audio_file = None\n",
    "        if audio_npy.exists():\n",
    "            audio_file = audio_npy\n",
    "        elif audio_npz.exists():\n",
    "            audio_file = audio_npz\n",
    "        \n",
    "        if audio_file:\n",
    "            try:\n",
    "                with open(text_file, 'r', encoding='utf-8') as f:\n",
    "                    transcript = f.read().strip()\n",
    "                \n",
    "                if transcript:\n",
    "                    audio_files.append(str(audio_file))\n",
    "                    transcripts.append(transcript)\n",
    "                    matched_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        if matched_count % 10000 == 0 and matched_count > 0:\n",
    "            print(f\"📊 진행 상황: {matched_count:,}개 매칭됨\")\n",
    "    \n",
    "    print(f\"✅ 총 {len(audio_files):,}개 파일 쌍 매칭됨\")\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'audio_path': audio_files,\n",
    "        'transcript': transcripts\n",
    "    })\n",
    "    \n",
    "    # 빈 텍스트 제거\n",
    "    df = df[df['transcript'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"  - 유효한 데이터: {len(df):,}개\")\n",
    "    \n",
    "    # 데이터 분할\n",
    "    total_size = len(df)\n",
    "    actual_train_size = min(config.TRAIN_SIZE, int(total_size * 0.8))\n",
    "    actual_val_size = min(config.VAL_SIZE, int(total_size * 0.1))\n",
    "    actual_test_size = min(config.TEST_SIZE, int(total_size * 0.1))\n",
    "    \n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    train_df = df[:actual_train_size]\n",
    "    val_df = df[actual_train_size:actual_train_size + actual_val_size]\n",
    "    test_df = df[actual_train_size + actual_val_size:actual_train_size + actual_val_size + actual_test_size]\n",
    "    \n",
    "    print(f\"📊 최종 데이터 분할:\")\n",
    "    print(f\"  - 학습: {len(train_df):,}개\")\n",
    "    print(f\"  - 검증: {len(val_df):,}개\") \n",
    "    print(f\"  - 테스트: {len(test_df):,}개\")\n",
    "    \n",
    "    return DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"validation\": Dataset.from_pandas(val_df),\n",
    "        \"test\": Dataset.from_pandas(test_df)\n",
    "    })\n",
    "\n",
    "def load_audio_features(audio_path: str) -> np.ndarray:\n",
    "    \"\"\"기존 오디오 로딩 함수 그대로\"\"\"\n",
    "    try:\n",
    "        if audio_path.endswith('.npy'):\n",
    "            features = np.load(audio_path)\n",
    "        elif audio_path.endswith('.npz'):\n",
    "            data = np.load(audio_path)\n",
    "            key = list(data.keys())[0]\n",
    "            features = data[key]\n",
    "        else:\n",
    "            raise ValueError(f\"지원하지 않는 파일 형식: {audio_path}\")\n",
    "        \n",
    "        if features.ndim == 1:\n",
    "            features = features.reshape(-1, 1)\n",
    "        elif features.ndim == 2:\n",
    "            pass\n",
    "        else:\n",
    "            print(f\"⚠️ 예상치 못한 특성 형태: {features.shape} in {audio_path}\")\n",
    "        \n",
    "        return features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"오디오 특성 로드 실패: {audio_path}, 오류: {e}\")\n",
    "        return np.zeros((1, 80))\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    \"\"\"기존 데이터 콜레이터 그대로\"\"\"\n",
    "    processor: Any\n",
    "    decoder_start_token_id: int\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch.attention_mask.ne(1), -100\n",
    "        )\n",
    "\n",
    "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def prepare_dataset(batch, processor, feature_extractor):\n",
    "    \"\"\"기존 데이터 준비 함수 그대로\"\"\"\n",
    "    audio_features = load_audio_features(batch[\"audio_path\"])\n",
    "    \n",
    "    if audio_features.ndim == 2:\n",
    "        if audio_features.shape[1] > audio_features.shape[0]:\n",
    "            pass\n",
    "        else:\n",
    "            audio_features = audio_features.T\n",
    "    \n",
    "    target_time_steps = 3000  # Whisper 필수 요구사항\n",
    "    \n",
    "    if audio_features.shape[1] > target_time_steps:\n",
    "        audio_features = audio_features[:, :target_time_steps]\n",
    "    elif audio_features.shape[1] < target_time_steps:\n",
    "        pad_width = target_time_steps - audio_features.shape[1]\n",
    "        audio_features = np.pad(audio_features, ((0, 0), (0, pad_width)), mode='constant', constant_values=0)\n",
    "    \n",
    "    if audio_features.shape[0] != 80:\n",
    "        if audio_features.shape[0] < 80:\n",
    "            pad_features = 80 - audio_features.shape[0]\n",
    "            audio_features = np.pad(audio_features, ((0, pad_features), (0, 0)), mode='constant', constant_values=0)\n",
    "        else:\n",
    "            audio_features = audio_features[:80, :]\n",
    "    \n",
    "    batch[\"input_features\"] = audio_features.astype(np.float32)\n",
    "    \n",
    "    batch[\"labels\"] = processor.tokenizer(\n",
    "        batch[\"transcript\"],\n",
    "        truncation=True,\n",
    "        max_length=config.MAX_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids[0]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"메모리 절약한 메트릭 계산\"\"\"\n",
    "    pred_ids, label_ids = eval_pred\n",
    "    \n",
    "    # 메모리 절약을 위해 샘플 수 제한\n",
    "    if len(pred_ids) > config.MAX_EVAL_SAMPLES:\n",
    "        pred_ids = pred_ids[:config.MAX_EVAL_SAMPLES]\n",
    "        label_ids = label_ids[:config.MAX_EVAL_SAMPLES]\n",
    "    \n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer_metric = evaluate.load(\"wer\")\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "    \n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# =============================================================================\n",
    "# 메인 학습 함수 - 체크포인트 없이 새로 시작\n",
    "# =============================================================================\n",
    "\n",
    "def train_from_scratch():\n",
    "    \"\"\"체크포인트 없이 처음부터 새로 시작하는 학습\"\"\"\n",
    "    \n",
    "    print(\"🚀 Whisper 한국어 파인튜닝 시작!\")\n",
    "    print(\"🆕 처음부터 새로 시작합니다!\")\n",
    "    print()\n",
    "    print(\"🎯 학습 설정:\")\n",
    "    print(f\"  - 모델: {config.MODEL_NAME}\")\n",
    "    print(f\"  - 배치 크기: {config.BATCH_SIZE}\")\n",
    "    print(f\"  - 그래디언트 누적: {config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  - 실효 배치: {config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  - 학습률: {config.LEARNING_RATE}\")\n",
    "    print(f\"  - 에포크: {config.NUM_EPOCHS}\")\n",
    "    print(f\"  - 체크포인트: {config.SAVE_STEPS}스텝마다\")\n",
    "    print()\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    # 1. 데이터셋 로드\n",
    "    print(\"📂 데이터셋 로딩...\")\n",
    "    dataset = load_dataset_from_directory(config.DATA_DIR)\n",
    "    \n",
    "    # 2. 모델 초기화 (항상 새로 시작)\n",
    "    print(\"🤖 새 모델 초기화...\")\n",
    "    \n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(config.MODEL_NAME)\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(\n",
    "        config.MODEL_NAME, \n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    processor = WhisperProcessor.from_pretrained(\n",
    "        config.MODEL_NAME,\n",
    "        language=config.LANGUAGE,\n",
    "        task=config.TASK\n",
    "    )\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(config.MODEL_NAME)\n",
    "    \n",
    "    # 언어 설정\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    model.generation_config.language = config.LANGUAGE\n",
    "    model.generation_config.task = config.TASK\n",
    "    \n",
    "    print(\"✅ 모델 준비 완료\")\n",
    "    \n",
    "    # 3. 데이터 전처리\n",
    "    print(\"🔄 데이터 전처리 중...\")\n",
    "    clear_memory()\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        lambda batch: prepare_dataset(batch, processor, feature_extractor),\n",
    "        remove_columns=dataset[\"train\"].column_names,\n",
    "        num_proc=1,\n",
    "        batch_size=50\n",
    "    )\n",
    "    \n",
    "    # 4. 데이터 콜레이터\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
    "        processor=processor,\n",
    "        decoder_start_token_id=model.config.decoder_start_token_id,\n",
    "    )\n",
    "    \n",
    "    # 5. 학습 설정\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.OUTPUT_DIR,\n",
    "        \n",
    "        # 배치 설정\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        per_device_eval_batch_size=config.EVAL_BATCH_SIZE,\n",
    "        \n",
    "        # 하이퍼파라미터\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        warmup_steps=config.WARMUP_STEPS,\n",
    "        \n",
    "        # 평가 최소화 (메모리 절약)\n",
    "        evaluation_strategy=\"no\",\n",
    "        \n",
    "        # 체크포인트 저장\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=config.SAVE_STEPS,\n",
    "        save_total_limit=config.SAVE_TOTAL_LIMIT,\n",
    "        logging_steps=100,\n",
    "        \n",
    "        # 기타 설정\n",
    "        remove_unused_columns=True,\n",
    "        label_names=[\"labels\"],\n",
    "        load_best_model_at_end=False,\n",
    "        push_to_hub=False,\n",
    "        \n",
    "        # 하드웨어 최적화\n",
    "        dataloader_num_workers=config.DATALOADER_NUM_WORKERS,\n",
    "        dataloader_pin_memory=False,\n",
    "        fp16=config.USE_FP16,\n",
    "        gradient_checkpointing=True,\n",
    "        \n",
    "        report_to=[],  # 텐서보드 끔\n",
    "    )\n",
    "    \n",
    "    # 6. 트레이너 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        args=training_args,\n",
    "        model=model,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    # 7. 학습 시작\n",
    "    print(\"🎓 학습 시작!\")\n",
    "    print(f\"📊 최종 설정 확인:\")\n",
    "    print(f\"  - 학습 데이터: {len(dataset['train']):,}개\")\n",
    "    print(f\"  - 총 스텝: 약 {len(dataset['train']) // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS) * config.NUM_EPOCHS:,}\")\n",
    "    print(f\"  - 예상 시간: 약 {(len(dataset['train']) // (config.BATCH_SIZE * config.GRADIENT_ACCUMULATION_STEPS) * config.NUM_EPOCHS) // 60}시간\")\n",
    "    print()\n",
    "    \n",
    "    clear_memory()\n",
    "    \n",
    "    try:\n",
    "        # 처음부터 새로 시작\n",
    "        print(\"🆕 처음부터 새로 학습 시작...\")\n",
    "        trainer.train()\n",
    "        \n",
    "        print(\"💾 최종 모델 저장 중...\")\n",
    "        trainer.save_model()\n",
    "        processor.save_pretrained(config.OUTPUT_DIR)\n",
    "        tokenizer.save_pretrained(config.OUTPUT_DIR)\n",
    "        feature_extractor.save_pretrained(config.OUTPUT_DIR)\n",
    "        \n",
    "        # 최종 테스트 평가\n",
    "        print(\"📊 최종 테스트 평가...\")\n",
    "        test_trainer = Seq2SeqTrainer(\n",
    "            args=Seq2SeqTrainingArguments(\n",
    "                output_dir=config.OUTPUT_DIR,\n",
    "                per_device_eval_batch_size=1,\n",
    "                fp16=True,\n",
    "            ),\n",
    "            model=model,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=lambda eval_pred: compute_metrics(eval_pred, tokenizer),\n",
    "            tokenizer=processor.feature_extractor,\n",
    "        )\n",
    "        \n",
    "        test_results = test_trainer.evaluate(dataset[\"test\"])\n",
    "        print(f\"🎯 최종 테스트 WER: {test_results['eval_wer']:.4f}\")\n",
    "        \n",
    "        print(\"🎉 학습 완전 완료!\")\n",
    "        print(f\"📁 모델 저장 위치: {config.OUTPUT_DIR}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 학습 중 오류 발생: {e}\")\n",
    "        print(\"💡 메모리 부족 시 추가 최적화 제안:\")\n",
    "        print(\"  - BATCH_SIZE = 2\")\n",
    "        print(\"  - GRADIENT_ACCUMULATION_STEPS = 16\")\n",
    "        print(\"  - TRAIN_SIZE = 20000\")\n",
    "        raise e\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_from_scratch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5d4445-6ef5-4c21-8037-4d660aa3ef3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
