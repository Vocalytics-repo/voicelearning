{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76b7eb87-df3f-4512-ae23-4d8461721845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71f068c3-2570-4314-bb19-8099c29b3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb2c688-45a0-41ab-9f24-a912f157d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"TrainData\"\n",
    "train_x,train_y,test_x,test_y,valid_x,valid_y = [],[],[],[],[],[]\n",
    "data_rate = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b55933-0374-4953-a06f-75dbc315f690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실행여부 확인\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "# cuda(gpu) 사용 가능한 경우 gpu 사용 하고 else 인 경우 cpu 사용\n",
    "print(\"실행여부 확인\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09f44b38-3ab3-4b6f-ace9-01095e4effcb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7f935cc75c10>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  p7zip\n",
      "Suggested packages:\n",
      "  p7zip-rar\n",
      "The following NEW packages will be installed:\n",
      "  p7zip p7zip-full\n",
      "0 upgraded, 2 newly installed, 0 to remove and 68 not upgraded.\n",
      "Need to get 1545 kB of archives.\n",
      "After this operation, 5896 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 p7zip amd64 16.02+dfsg-7build1 [358 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/universe amd64 p7zip-full amd64 16.02+dfsg-7build1 [1187 kB]\n",
      "Fetched 1545 kB in 2s (774 kB/s)    \n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package p7zip.\n",
      "(Reading database ... 16670 files and directories currently installed.)\n",
      "Preparing to unpack .../p7zip_16.02+dfsg-7build1_amd64.deb ...\n",
      "Unpacking p7zip (16.02+dfsg-7build1) ...\n",
      "Selecting previously unselected package p7zip-full.\n",
      "Preparing to unpack .../p7zip-full_16.02+dfsg-7build1_amd64.deb ...\n",
      "Unpacking p7zip-full (16.02+dfsg-7build1) ...\n",
      "Setting up p7zip (16.02+dfsg-7build1) ...\n",
      "Setting up p7zip-full (16.02+dfsg-7build1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y p7zip-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f357a4e-2100-4c10-8075-5e403c94d07b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "p7zip-full is already the newest version (16.02+dfsg-7build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 68 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y p7zip-full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b19248dc-2c79-431c-b598-9fd10856b52a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
      "p7zip Version 16.02 (locale=C.UTF-8,Utf16=on,HugeFiles=on,64 bits,48 CPUs Intel(R) Xeon(R) Silver 4310 CPU @ 2.10GHz (606A6),ASM,AES-NI)\n",
      "\n",
      "Scanning the drive for archives:\n",
      "1 file, 15301122689 bytes (15 GiB)\n",
      "\n",
      "Extracting archive: KsponSpeech_01.zip\n",
      "\n",
      "ERRORS:\n",
      "Headers Error\n",
      "Unconfirmed start of archive\n",
      "\n",
      "\n",
      "WARNINGS:\n",
      "There are data after the end of archive\n",
      "\n",
      "--\n",
      "Path = KsponSpeech_01.zip\n",
      "Type = zip\n",
      "ERRORS:\n",
      "Headers Error\n",
      "Unconfirmed start of archive\n",
      "WARNINGS:\n",
      "There are data after the end of archive\n",
      "Physical Size = 2147585904\n",
      "Tail Size = 13153536785\n",
      "\n",
      "ERROR: Data Error : KsponSpeech_01/KsponSpeech_0018/KsponSpeech_017485.pcm\n",
      "\n",
      "Sub items Errors: 1\n",
      "\n",
      "Archives with Errors: 1\n",
      "\n",
      "Warnings: 1\n",
      "\n",
      "Open Errors: 1\n",
      "\n",
      "Sub items Errors: 1\n"
     ]
    }
   ],
   "source": [
    "!7z x KsponSpeech_01.zip -o\"unzipped_Speech\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2413486-b9e9-4482-86fc-93408423aca4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.30.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.10.11)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0; python_version < \"3.11\" in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f66f2632-f136-44e1-8ad6-fc45fe2d96a6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "\n",
    "def read_npz(file_path, sr=16000):\n",
    "    \"\"\"\n",
    "    NPZ 파일을 읽어 numpy 배열로 반환합니다.\n",
    "    \"\"\"\n",
    "    # npz 파일 로드\n",
    "    with np.load(file_path) as data:\n",
    "        # npz 파일에 저장된 첫 번째 배열 가져오기\n",
    "        # 일반적으로 npz 파일에는 'arr_0'이라는 키로 데이터가 저장됩니다\n",
    "        if 'arr_0' in data:\n",
    "            audio_data = data['arr_0']\n",
    "        # 또는 특정 키가 있는 경우 그 키를 사용할 수 있습니다\n",
    "        # 예: audio_data = data['audio']\n",
    "        else:\n",
    "            # 첫 번째 키 사용\n",
    "            key = list(data.keys())[0]\n",
    "            audio_data = data[key]\n",
    "\n",
    "    # 이미 정규화되어 있는지 확인하고, 필요한 경우 정규화\n",
    "    if audio_data.dtype in [np.int16, np.int8]:\n",
    "        max_value = float(2 ** (15 if audio_data.dtype == np.int16 else 7))\n",
    "        audio_data = audio_data.astype(np.float32) / max_value\n",
    "\n",
    "    return audio_data, sr\n",
    "def load_text(file_path):\n",
    "    # 일반적인 한국어 인코딩 시도\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "def get_file_paths_npz(base_directory):\n",
    "    \"\"\"\n",
    "    디렉토리에서 PCM 파일과 텍스트 파일의 경로를 가져옵니다.\n",
    "    \"\"\"\n",
    "    base = []\n",
    "\n",
    "    for i in range(1, 19):\n",
    "        if i > 10 :\n",
    "            base.append(base_directory + f\"/KsponSpeech_00{i}\")\n",
    "        else:\n",
    "            base.append(base_directory + f\"/KsponSpeech_000{i}\")\n",
    "\n",
    "    npz_files = []\n",
    "\n",
    "    for base_path in base:\n",
    "        for root, _, files in os.walk(base_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.npz'):\n",
    "                    npz_path = os.path.join(root, file)\n",
    "                    if os.path.exists(npz_path):\n",
    "                        npz_files.append(npz_path)\n",
    "                        print(\"성공\")\n",
    "                    else:\n",
    "                        print(\"실패\")\n",
    "        return npz_files\n",
    "\n",
    "def get_file_paths_txt(npz_files):\n",
    "    txt_files = []\n",
    "    # 모든 파일을 검색\n",
    "    for path in npz_files:\n",
    "        temp = path.split(\".\")\n",
    "        temp_p = temp[1] + \"_g2p\"\n",
    "        temp_path = temp[0] + temp_p\n",
    "        txt_files.append(temp_path)\n",
    "\n",
    "    return txt_files\n",
    "\n",
    "def create_dataset(npz_files, txt_files, max_samples=None):\n",
    "    \"\"\"\n",
    "    PCM 파일과 텍스트 파일로부터 데이터셋을 생성합니다.\n",
    "    \"\"\"\n",
    "    data = {\"audio\": [], \"text\": []}\n",
    "    # 최대 샘플 수 제한\n",
    "    if max_samples is not None:\n",
    "        npz_files = npz_files[:max_samples]\n",
    "        txt_files = txt_files[:max_samples]\n",
    "\n",
    "    for npz_file, txt_file in zip(npz_files, txt_files):\n",
    "        data[\"audio\"].append(npz_file)\n",
    "        data[\"text\"].append(load_text(txt_file))\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "def train_whisper_model():\n",
    "    base_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    output_dir = \"whisper_finetuned\"\n",
    "    pcm_files = get_file_paths_npz(base_directory)\n",
    "    txt_files = get_file_paths_txt(pcm_files)\n",
    "    print(f\"총 {len(pcm_files)}개의 파일이 로드되었습니다.\")\n",
    "\n",
    "    dataset = create_dataset(pcm_files, txt_files)\n",
    "\n",
    "    def map_to_array(batch):\n",
    "        arrays = []\n",
    "        rates = []\n",
    "\n",
    "        for audio_path in batch[\"audio\"]:\n",
    "            audio_array, sampling_rate = read_npz(audio_path)\n",
    "            arrays.append(audio_array)\n",
    "            rates.append(sampling_rate)\n",
    "\n",
    "        batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "        return batch\n",
    "\n",
    "    # 데이터셋에 Audio 형식 적용 (배치 처리)\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "\n",
    "    # 훈련/검증/테스트 세트 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    # Whisper 모델 및 프로세서 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        # 오디오 처리\n",
    "        audio = batch[\"audio\"]\n",
    "\n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "\n",
    "        # NumPy 배열을 PyTorch 텐서로 변환 후 데이터 타입 조정\n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "\n",
    "        # 이제 텐서가 되었으므로 데이터 타입 변환 가능\n",
    "        if hasattr(model, \"dtype\"):\n",
    "            input_features = input_features.to(model.dtype)\n",
    "\n",
    "        # 텍스트 처리\n",
    "        labels = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    # 데이터셋 전처리 적용 (비배치 함수)\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    # 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "\n",
    "        # pred_ids가 튜플인 경우 (일반적으로 첫 번째 요소가 예측값)\n",
    "        if isinstance(pred_ids, tuple):\n",
    "            pred_ids = pred_ids[0]\n",
    "\n",
    "        # 이제 pred_ids가 텐서/배열인지 확인하고 3차원인 경우 처리\n",
    "        if hasattr(pred_ids, 'shape') and len(pred_ids.shape) > 2:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "        # 패딩 토큰 무시\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "        # 예측과 레이블을 텍스트로 디코딩\n",
    "        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # WER 계산 \n",
    "        # 참고: jiwer 라이브러리가 임포트되었는지 확인\n",
    "        import jiwer  # 이 라인 추가\n",
    "        wer = jiwer.wer(label_str, pred_str)\n",
    "\n",
    "        return {\"wer\": wer}\n",
    "\n",
    "    # 훈련 인자 설정\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=500,\n",
    "        max_steps=4000,\n",
    "        gradient_checkpointing=True,\n",
    "        # fp16=True,\n",
    "        fp16_full_eval=False,\n",
    "        evaluation_strategy=\"steps\",  # 올바른 파라미터명 사용\n",
    "        eval_steps=500,\n",
    "        save_steps=500,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"wer\",\n",
    "        greater_is_better=False,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # 훈련기 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        eval_dataset=processed_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,  # processing_class 대신 tokenizer 사용\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # 모델 훈련\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 저장\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    # 테스트 평가\n",
    "    test_results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "    print(f\"테스트 결과: {test_results}\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# 모델 추론 함수\n",
    "def transcribe_audio(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    # PCM 파일 로드\n",
    "    audio_array, sr = read_npz(audio_file)\n",
    "\n",
    "    # 특성 추출\n",
    "    input_features = processor.feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "\n",
    "    # 모델을 통한 예측\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    # 예측된 토큰을 텍스트로 디코딩\n",
    "    transcription = processor.tokenizer.batch_decode(\n",
    "        predicted_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a374fd-a55a-420c-b0a5-43af3560e4f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 0개의 파일이 로드되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_628/2787886146.py:252: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature. The following columns have been ignored: []. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 모델 훈련\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_whisper_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 테스트할 디렉터리 설정\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     test_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessData/KsponSpeech_01\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[9], line 263\u001b[0m, in \u001b[0;36mtrain_whisper_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    252\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m    253\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    254\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# 모델 훈련\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[1;32m    266\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(output_dir)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:2152\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2150\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently training with a batch size of: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2151\u001b[0m \u001b[38;5;66;03m# Data loader and number of training steps\u001b[39;00m\n\u001b[0;32m-> 2152\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_train_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   2154\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(train_dataloader)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:958\u001b[0m, in \u001b[0;36mTrainer.get_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    956\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m--> 958\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py:884\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    882\u001b[0m columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcolumn_names]\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 884\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    885\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo columns in the dataset match the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms forward method signature. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    886\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    887\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    888\u001b[0m     )\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(datasets\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    891\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mset_format(\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], columns\u001b[38;5;241m=\u001b[39mcolumns, format_kwargs\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    893\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature. The following columns have been ignored: []. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 모델 훈련\n",
    "    model, processor = train_whisper_model()\n",
    "    \n",
    "    # 테스트할 디렉터리 설정\n",
    "    test_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    \n",
    "    # 테스트 파일 가져오기\n",
    "    pcm_files, txt_files = get_file_paths(test_directory)\n",
    "    print(f\"테스트할 파일: {len(pcm_files)}개\")\n",
    "    \n",
    "    # 모든 파일 테스트 (또는 원하는 만큼 제한)\n",
    "    for i in range(len(pcm_files)):\n",
    "        test_audio = pcm_files[i]\n",
    "        reference_text = load_text(txt_files[i])\n",
    "        \n",
    "        transcription = transcribe_audio(model, processor, test_audio)\n",
    "        \n",
    "        print(f\"파일: {os.path.basename(test_audio)}\")\n",
    "        print(f\"원본 텍스트: {reference_text}\")\n",
    "        print(f\"변환 결과: {transcription}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b7c2d7a-c92a-469e-beec-3961df711dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /data\n",
      "\n",
      "PreprocessData 디렉토리 구조:\n",
      "📁 .ipynb_checkpoints\n",
      "📁 KsponSpeech_01\n",
      "  📁 .ipynb_checkpoints\n",
      "  📁 KsponSpeech_0001\n",
      "    📄 파일 997개:\n",
      "      KsponSpeech_000001_combined_features.npy\n",
      "      KsponSpeech_000002_combined_features.npy\n",
      "      KsponSpeech_000003_combined_features.npy\n",
      "      KsponSpeech_000004_combined_features.npy\n",
      "      KsponSpeech_000005_combined_features.npy\n",
      "      ... 외 992개\n",
      "  📁 KsponSpeech_0001_g2p\n",
      "    📁 .ipynb_checkpoints\n",
      "      📄 파일 10개:\n",
      "        KsponSpeech_000001-checkpoint.txt\n",
      "        KsponSpeech_000002-checkpoint.txt\n",
      "        KsponSpeech_000003-checkpoint.txt\n",
      "        KsponSpeech_000004-checkpoint.txt\n",
      "        KsponSpeech_000009-checkpoint.txt\n",
      "        ... 외 5개\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_000001.txt\n",
      "      KsponSpeech_000002.txt\n",
      "      KsponSpeech_000003.txt\n",
      "      KsponSpeech_000004.txt\n",
      "      KsponSpeech_000005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0002\n",
      "    📄 파일 160개:\n",
      "      KsponSpeech_001001_combined_features.npy\n",
      "      KsponSpeech_001003_combined_features.npy\n",
      "      KsponSpeech_001009_combined_features.npy\n",
      "      KsponSpeech_001014_combined_features.npy\n",
      "      KsponSpeech_001039_combined_features.npy\n",
      "      ... 외 155개\n",
      "  📁 KsponSpeech_0002_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_001001.txt\n",
      "      KsponSpeech_001002.txt\n",
      "      KsponSpeech_001003.txt\n",
      "      KsponSpeech_001004.txt\n",
      "      KsponSpeech_001005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0003\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_002001_combined_features.npy\n",
      "      KsponSpeech_002002_combined_features.npy\n",
      "      KsponSpeech_002003_combined_features.npy\n",
      "      KsponSpeech_002004_combined_features.npy\n",
      "      KsponSpeech_002005_combined_features.npy\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0003_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_002001.txt\n",
      "      KsponSpeech_002002.txt\n",
      "      KsponSpeech_002003.txt\n",
      "      KsponSpeech_002004.txt\n",
      "      KsponSpeech_002005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0004\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_003001_combined_features.npy\n",
      "      KsponSpeech_003002_combined_features.npy\n",
      "      KsponSpeech_003003_combined_features.npy\n",
      "      KsponSpeech_003004_combined_features.npy\n",
      "      KsponSpeech_003005_combined_features.npy\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0004_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_003001.txt\n",
      "      KsponSpeech_003002.txt\n",
      "      KsponSpeech_003003.txt\n",
      "      KsponSpeech_003004.txt\n",
      "      KsponSpeech_003005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0005\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_004001_combined_features.npy\n",
      "      KsponSpeech_004002_combined_features.npy\n",
      "      KsponSpeech_004003_combined_features.npy\n",
      "      KsponSpeech_004004_combined_features.npy\n",
      "      KsponSpeech_004005_combined_features.npy\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0005_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_004001.txt\n",
      "      KsponSpeech_004002.txt\n",
      "      KsponSpeech_004003.txt\n",
      "      KsponSpeech_004004.txt\n",
      "      KsponSpeech_004005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0006\n",
      "    📄 파일 37개:\n",
      "      KsponSpeech_005038_combined_features.npy\n",
      "      KsponSpeech_005075_combined_features.npy\n",
      "      KsponSpeech_005093_combined_features.npy\n",
      "      KsponSpeech_005113_combined_features.npy\n",
      "      KsponSpeech_005120_combined_features.npy\n",
      "      ... 외 32개\n",
      "  📁 KsponSpeech_0006_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_005001.txt\n",
      "      KsponSpeech_005002.txt\n",
      "      KsponSpeech_005003.txt\n",
      "      KsponSpeech_005004.txt\n",
      "      KsponSpeech_005005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0007\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_006001_combined_features.npy\n",
      "      KsponSpeech_006002_combined_features.npy\n",
      "      KsponSpeech_006003_combined_features.npy\n",
      "      KsponSpeech_006004_combined_features.npy\n",
      "      KsponSpeech_006005_combined_features.npy\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0007_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_006001.txt\n",
      "      KsponSpeech_006002.txt\n",
      "      KsponSpeech_006003.txt\n",
      "      KsponSpeech_006004.txt\n",
      "      KsponSpeech_006005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0008\n",
      "    📄 파일 937개:\n",
      "      KsponSpeech_007002_combined_features.npy\n",
      "      KsponSpeech_007003_combined_features.npy\n",
      "      KsponSpeech_007004_combined_features.npy\n",
      "      KsponSpeech_007007_combined_features.npy\n",
      "      KsponSpeech_007008_combined_features.npy\n",
      "      ... 외 932개\n",
      "  📁 KsponSpeech_0008_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_007001.txt\n",
      "      KsponSpeech_007002.txt\n",
      "      KsponSpeech_007003.txt\n",
      "      KsponSpeech_007004.txt\n",
      "      KsponSpeech_007005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0009\n",
      "    📄 파일 834개:\n",
      "      KsponSpeech_008002_combined_features.npy\n",
      "      KsponSpeech_008003_combined_features.npy\n",
      "      KsponSpeech_008004_combined_features.npy\n",
      "      KsponSpeech_008005_combined_features.npy\n",
      "      KsponSpeech_008006_combined_features.npy\n",
      "      ... 외 829개\n",
      "  📁 KsponSpeech_0009_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_008001.txt\n",
      "      KsponSpeech_008002.txt\n",
      "      KsponSpeech_008003.txt\n",
      "      KsponSpeech_008004.txt\n",
      "      KsponSpeech_008005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0010\n",
      "    📄 파일 376개:\n",
      "      KsponSpeech_009002_combined_features.npy\n",
      "      KsponSpeech_009005_combined_features.npy\n",
      "      KsponSpeech_009007_combined_features.npy\n",
      "      KsponSpeech_009008_combined_features.npy\n",
      "      KsponSpeech_009010_combined_features.npy\n",
      "      ... 외 371개\n",
      "  📁 KsponSpeech_0010_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_009001.txt\n",
      "      KsponSpeech_009002.txt\n",
      "      KsponSpeech_009003.txt\n",
      "      KsponSpeech_009004.txt\n",
      "      KsponSpeech_009005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0011\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_010001_combined_features.npy\n",
      "      KsponSpeech_010002_combined_features.npy\n",
      "      KsponSpeech_010003_combined_features.npy\n",
      "      KsponSpeech_010004_combined_features.npy\n",
      "      KsponSpeech_010005_combined_features.npy\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0011_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_010001.txt\n",
      "      KsponSpeech_010002.txt\n",
      "      KsponSpeech_010003.txt\n",
      "      KsponSpeech_010004.txt\n",
      "      KsponSpeech_010005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0012\n",
      "    📄 파일 961개:\n",
      "      KsponSpeech_011001_combined_features.npy\n",
      "      KsponSpeech_011002_combined_features.npy\n",
      "      KsponSpeech_011003_combined_features.npy\n",
      "      KsponSpeech_011004_combined_features.npy\n",
      "      KsponSpeech_011005_combined_features.npy\n",
      "      ... 외 956개\n",
      "  📁 KsponSpeech_0012_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_011001.txt\n",
      "      KsponSpeech_011002.txt\n",
      "      KsponSpeech_011003.txt\n",
      "      KsponSpeech_011004.txt\n",
      "      KsponSpeech_011005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0013\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_012001_combined_features.npy\n",
      "      KsponSpeech_012002_combined_features.npy\n",
      "      KsponSpeech_012003_combined_features.npy\n",
      "      KsponSpeech_012004_combined_features.npy\n",
      "      KsponSpeech_012005_combined_features.npy\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0013_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_012001.txt\n",
      "      KsponSpeech_012002.txt\n",
      "      KsponSpeech_012003.txt\n",
      "      KsponSpeech_012004.txt\n",
      "      KsponSpeech_012005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0014\n",
      "    📄 파일 861개:\n",
      "      KsponSpeech_013001_combined_features.npy\n",
      "      KsponSpeech_013002_combined_features.npy\n",
      "      KsponSpeech_013003_combined_features.npy\n",
      "      KsponSpeech_013004_combined_features.npy\n",
      "      KsponSpeech_013005_combined_features.npy\n",
      "      ... 외 856개\n",
      "  📁 KsponSpeech_0014_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_013001.txt\n",
      "      KsponSpeech_013002.txt\n",
      "      KsponSpeech_013003.txt\n",
      "      KsponSpeech_013004.txt\n",
      "      KsponSpeech_013005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0015\n",
      "    📄 파일 65개:\n",
      "      KsponSpeech_014019_combined_features.npy\n",
      "      KsponSpeech_014054_combined_features.npy\n",
      "      KsponSpeech_014056_combined_features.npy\n",
      "      KsponSpeech_014057_combined_features.npy\n",
      "      KsponSpeech_014074_combined_features.npy\n",
      "      ... 외 60개\n",
      "  📁 KsponSpeech_0015_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_014001.txt\n",
      "      KsponSpeech_014002.txt\n",
      "      KsponSpeech_014003.txt\n",
      "      KsponSpeech_014004.txt\n",
      "      KsponSpeech_014005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0016\n",
      "    📄 파일 1개:\n",
      "      KsponSpeech_015872_combined_features.npy\n",
      "  📁 KsponSpeech_0016_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_015001.txt\n",
      "      KsponSpeech_015002.txt\n",
      "      KsponSpeech_015003.txt\n",
      "      KsponSpeech_015004.txt\n",
      "      KsponSpeech_015005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0017\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_016001_combined_features.npy\n",
      "      KsponSpeech_016002_combined_features.npy\n",
      "      KsponSpeech_016003_combined_features.npy\n",
      "      KsponSpeech_016004_combined_features.npy\n",
      "      KsponSpeech_016005_combined_features.npy\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0017_g2p\n",
      "    📄 파일 1000개:\n",
      "      KsponSpeech_016001.txt\n",
      "      KsponSpeech_016002.txt\n",
      "      KsponSpeech_016003.txt\n",
      "      KsponSpeech_016004.txt\n",
      "      KsponSpeech_016005.txt\n",
      "      ... 외 995개\n",
      "  📁 KsponSpeech_0018\n",
      "    📄 파일 485개:\n",
      "      KsponSpeech_017001_combined_features.npy\n",
      "      KsponSpeech_017002_combined_features.npy\n",
      "      KsponSpeech_017003_combined_features.npy\n",
      "      KsponSpeech_017004_combined_features.npy\n",
      "      KsponSpeech_017005_combined_features.npy\n",
      "      ... 외 480개\n",
      "  📁 KsponSpeech_0018_g2p\n",
      "    📄 파일 484개:\n",
      "      KsponSpeech_017001.txt\n",
      "      KsponSpeech_017002.txt\n",
      "      KsponSpeech_017003.txt\n",
      "      KsponSpeech_017004.txt\n",
      "      KsponSpeech_017005.txt\n",
      "      ... 외 479개\n",
      "\n",
      "❌ PreprocessData_Speech/KsponSpeech_01가 존재하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "# 현재 작업 디렉토리 확인\n",
    "import os\n",
    "print(\"현재 작업 디렉토리:\", os.getcwd())\n",
    "\n",
    "# 특정 디렉토리의 구조 확인\n",
    "def print_directory_structure(directory, level=0, max_depth=3, max_files=5):\n",
    "    if level > max_depth:\n",
    "        print(\"  \" * level + \"...\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        items = os.listdir(directory)\n",
    "        dirs = []\n",
    "        files = []\n",
    "        \n",
    "        for item in items:\n",
    "            item_path = os.path.join(directory, item)\n",
    "            if os.path.isdir(item_path):\n",
    "                dirs.append(item)\n",
    "            else:\n",
    "                files.append(item)\n",
    "        \n",
    "        for d in sorted(dirs):\n",
    "            print(\"  \" * level + \"📁 \" + d)\n",
    "            print_directory_structure(os.path.join(directory, d), level + 1, max_depth, max_files)\n",
    "        \n",
    "        if files:\n",
    "            print(\"  \" * level + f\"📄 파일 {len(files)}개:\")\n",
    "            for i, f in enumerate(sorted(files)):\n",
    "                if i >= max_files:\n",
    "                    print(\"  \" * (level + 1) + f\"... 외 {len(files) - max_files}개\")\n",
    "                    break\n",
    "                print(\"  \" * (level + 1) + f)\n",
    "    except PermissionError:\n",
    "        print(\"  \" * level + \"❌ 접근 권한 없음\")\n",
    "    except Exception as e:\n",
    "        print(\"  \" * level + f\"❌ 오류: {e}\")\n",
    "\n",
    "# 확인하고 싶은 디렉토리 경로 (원하는 경로로 수정)\n",
    "# 예: data_directory = \"unzipped_Speech\"\n",
    "# 또는 base_directory에 지정된 경로를 사용\n",
    "data_directory = \"PreprocessData\"\n",
    "print(f\"\\n{data_directory} 디렉토리 구조:\")\n",
    "print_directory_structure(data_directory)\n",
    "\n",
    "# KsponSpeech_01 디렉토리가 있는지 구체적으로 확인\n",
    "target_dir = \"PreprocessData_Speech/KsponSpeech_01\"\n",
    "if os.path.exists(target_dir):\n",
    "    print(f\"\\n{target_dir}가 존재합니다.\")\n",
    "    print(f\"{target_dir} 디렉토리 구조:\")\n",
    "    print_directory_structure(target_dir)\n",
    "else:\n",
    "    print(f\"\\n❌ {target_dir}가 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c67ef062-e996-4e47-8c10-16686046936b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"whisper_finetuned/saved_whisper_model.pt\"\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b61f6cf9-c406-46d0-bfc6-e71f1c92bb9e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.8/dist-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (0.30.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: torch; extra == \"torch\" in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (2.4.1+cu118)\n",
      "Requirement already satisfied: accelerate>=0.26.0; extra == \"torch\" in /usr/local/lib/python3.8/dist-packages (from transformers[torch]) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.13.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (11.8.89)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (11.8.86)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (11.7.5.86)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (11.8.89)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (2.20.5)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (11.11.3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch; extra == \"torch\"->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0; extra == \"torch\"->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch; extra == \"torch\"->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch; extra == \"torch\"->transformers[torch]) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b509ffa-797c-46c3-a999-987b0af778ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-25 01:10:28.596516: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-25 01:10:28.645096: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-25 01:10:29.371019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0001\n",
      "찾은 파일 수: 100\n",
      "찾은 파일 수: 200\n",
      "찾은 파일 수: 300\n",
      "찾은 파일 수: 400\n",
      "찾은 파일 수: 500\n",
      "찾은 파일 수: 600\n",
      "찾은 파일 수: 700\n",
      "찾은 파일 수: 800\n",
      "찾은 파일 수: 900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0002\n",
      "찾은 파일 수: 1000\n",
      "찾은 파일 수: 1100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0003\n",
      "찾은 파일 수: 1200\n",
      "찾은 파일 수: 1300\n",
      "찾은 파일 수: 1400\n",
      "찾은 파일 수: 1500\n",
      "찾은 파일 수: 1600\n",
      "찾은 파일 수: 1700\n",
      "찾은 파일 수: 1800\n",
      "찾은 파일 수: 1900\n",
      "찾은 파일 수: 2000\n",
      "찾은 파일 수: 2100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0004\n",
      "찾은 파일 수: 2200\n",
      "찾은 파일 수: 2300\n",
      "찾은 파일 수: 2400\n",
      "찾은 파일 수: 2500\n",
      "찾은 파일 수: 2600\n",
      "찾은 파일 수: 2700\n",
      "찾은 파일 수: 2800\n",
      "찾은 파일 수: 2900\n",
      "찾은 파일 수: 3000\n",
      "찾은 파일 수: 3100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0005\n",
      "찾은 파일 수: 3200\n",
      "찾은 파일 수: 3300\n",
      "찾은 파일 수: 3400\n",
      "찾은 파일 수: 3500\n",
      "찾은 파일 수: 3600\n",
      "찾은 파일 수: 3700\n",
      "찾은 파일 수: 3800\n",
      "찾은 파일 수: 3900\n",
      "찾은 파일 수: 4000\n",
      "찾은 파일 수: 4100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0006\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0007\n",
      "찾은 파일 수: 4200\n",
      "찾은 파일 수: 4300\n",
      "찾은 파일 수: 4400\n",
      "찾은 파일 수: 4500\n",
      "찾은 파일 수: 4600\n",
      "찾은 파일 수: 4700\n",
      "찾은 파일 수: 4800\n",
      "찾은 파일 수: 4900\n",
      "찾은 파일 수: 5000\n",
      "찾은 파일 수: 5100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0008\n",
      "찾은 파일 수: 5200\n",
      "찾은 파일 수: 5300\n",
      "찾은 파일 수: 5400\n",
      "찾은 파일 수: 5500\n",
      "찾은 파일 수: 5600\n",
      "찾은 파일 수: 5700\n",
      "찾은 파일 수: 5800\n",
      "찾은 파일 수: 5900\n",
      "찾은 파일 수: 6000\n",
      "찾은 파일 수: 6100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0009\n",
      "찾은 파일 수: 6200\n",
      "찾은 파일 수: 6300\n",
      "찾은 파일 수: 6400\n",
      "찾은 파일 수: 6500\n",
      "찾은 파일 수: 6600\n",
      "찾은 파일 수: 6700\n",
      "찾은 파일 수: 6800\n",
      "찾은 파일 수: 6900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0010\n",
      "찾은 파일 수: 7000\n",
      "찾은 파일 수: 7100\n",
      "찾은 파일 수: 7200\n",
      "찾은 파일 수: 7300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0011\n",
      "찾은 파일 수: 7400\n",
      "찾은 파일 수: 7500\n",
      "찾은 파일 수: 7600\n",
      "찾은 파일 수: 7700\n",
      "찾은 파일 수: 7800\n",
      "찾은 파일 수: 7900\n",
      "찾은 파일 수: 8000\n",
      "찾은 파일 수: 8100\n",
      "찾은 파일 수: 8200\n",
      "찾은 파일 수: 8300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0012\n",
      "찾은 파일 수: 8400\n",
      "찾은 파일 수: 8500\n",
      "찾은 파일 수: 8600\n",
      "찾은 파일 수: 8700\n",
      "찾은 파일 수: 8800\n",
      "찾은 파일 수: 8900\n",
      "찾은 파일 수: 9000\n",
      "찾은 파일 수: 9100\n",
      "찾은 파일 수: 9200\n",
      "찾은 파일 수: 9300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0013\n",
      "찾은 파일 수: 9400\n",
      "찾은 파일 수: 9500\n",
      "찾은 파일 수: 9600\n",
      "찾은 파일 수: 9700\n",
      "찾은 파일 수: 9800\n",
      "찾은 파일 수: 9900\n",
      "찾은 파일 수: 10000\n",
      "찾은 파일 수: 10100\n",
      "찾은 파일 수: 10200\n",
      "찾은 파일 수: 10300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0014\n",
      "찾은 파일 수: 10400\n",
      "찾은 파일 수: 10500\n",
      "찾은 파일 수: 10600\n",
      "찾은 파일 수: 10700\n",
      "찾은 파일 수: 10800\n",
      "찾은 파일 수: 10900\n",
      "찾은 파일 수: 11000\n",
      "찾은 파일 수: 11100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0015\n",
      "찾은 파일 수: 11200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0016\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0017\n",
      "찾은 파일 수: 11300\n",
      "찾은 파일 수: 11400\n",
      "찾은 파일 수: 11500\n",
      "찾은 파일 수: 11600\n",
      "찾은 파일 수: 11700\n",
      "찾은 파일 수: 11800\n",
      "찾은 파일 수: 11900\n",
      "찾은 파일 수: 12000\n",
      "찾은 파일 수: 12100\n",
      "찾은 파일 수: 12200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0018\n",
      "찾은 파일 수: 12300\n",
      "찾은 파일 수: 12400\n",
      "찾은 파일 수: 12500\n",
      "찾은 파일 수: 12600\n",
      "찾은 파일 수: 12700\n",
      "총 12714개의 NP 파일을 찾았습니다.\n",
      "경고: 텍스트 파일이 존재하지 않습니다: PreprocessData/KsponSpeech_01/KsponSpeech_0018_g2p/KsponSpeech_017485.txt\n",
      "총 12713개의 텍스트 파일을 매핑했습니다.\n",
      "총 12714개의 오디오 파일과 12713개의 텍스트 파일이 로드되었습니다.\n",
      "경고: 오디오 파일 수(12714)와 텍스트 파일 수(12713)가 일치하지 않습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 12713/12713 [00:06<00:00, 2054.02 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10170/10170 [35:13<00:00,  4.81 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1272/1272 [04:23<00:00,  4.84 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1271/1271 [04:24<00:00,  4.80 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_18222/1156997083.py:292: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 5:42:01, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>10.502600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>7.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.573500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.127000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>4.910000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.763700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>4.537200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>4.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>4.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>3.640800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.620100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>3.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.503700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>3.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>3.402400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.394400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>3.411600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.343000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>3.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>3.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>3.203100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>3.222000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.156500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>3.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.147800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>3.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.148900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>3.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.138700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>3.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>3.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>3.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>3.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>3.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>3.045100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.122800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>3.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.902200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>2.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.911700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>2.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.882500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>2.903700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>2.884200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>2.882800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.852100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>2.875700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>2.959800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.888300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>2.918700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.909800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>2.906700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>2.949700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.872400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>2.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.871900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>2.844500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.844300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>2.754100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.729400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>2.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.742700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>2.702900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.713100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>2.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>2.734600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.712800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>2.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.747300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>2.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>2.709500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>2.736400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.671000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>2.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>2.650400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>2.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>2.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.667500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>2.509300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>2.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.543500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>2.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.524900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>2.601400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.573700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>2.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.570100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>2.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>2.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>2.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>2.560400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>2.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.550300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>2.552100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.567100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>2.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3175</td>\n",
       "      <td>2.545100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>2.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3275</td>\n",
       "      <td>2.365800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.428000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3325</td>\n",
       "      <td>2.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.407500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3375</td>\n",
       "      <td>2.422200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3425</td>\n",
       "      <td>2.460100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.464600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3475</td>\n",
       "      <td>2.462100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.464700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3525</td>\n",
       "      <td>2.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.427200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3575</td>\n",
       "      <td>2.481700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3625</td>\n",
       "      <td>2.415100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.406400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>2.486300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.390300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3725</td>\n",
       "      <td>2.428800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.410300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3775</td>\n",
       "      <td>2.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3825</td>\n",
       "      <td>2.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>2.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3875</td>\n",
       "      <td>2.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3925</td>\n",
       "      <td>2.394500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>2.333700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3975</td>\n",
       "      <td>2.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.365500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1272' max='1272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1272/1272 02:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 결과: {'eval_loss': 3.0094687938690186, 'eval_runtime': 165.0981, 'eval_samples_per_second': 7.705, 'eval_steps_per_second': 7.705, 'epoch': 6.289308176100629}\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0001\n",
      "찾은 파일 수: 100\n",
      "찾은 파일 수: 200\n",
      "찾은 파일 수: 300\n",
      "찾은 파일 수: 400\n",
      "찾은 파일 수: 500\n",
      "찾은 파일 수: 600\n",
      "찾은 파일 수: 700\n",
      "찾은 파일 수: 800\n",
      "찾은 파일 수: 900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0002\n",
      "찾은 파일 수: 1000\n",
      "찾은 파일 수: 1100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0003\n",
      "찾은 파일 수: 1200\n",
      "찾은 파일 수: 1300\n",
      "찾은 파일 수: 1400\n",
      "찾은 파일 수: 1500\n",
      "찾은 파일 수: 1600\n",
      "찾은 파일 수: 1700\n",
      "찾은 파일 수: 1800\n",
      "찾은 파일 수: 1900\n",
      "찾은 파일 수: 2000\n",
      "찾은 파일 수: 2100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0004\n",
      "찾은 파일 수: 2200\n",
      "찾은 파일 수: 2300\n",
      "찾은 파일 수: 2400\n",
      "찾은 파일 수: 2500\n",
      "찾은 파일 수: 2600\n",
      "찾은 파일 수: 2700\n",
      "찾은 파일 수: 2800\n",
      "찾은 파일 수: 2900\n",
      "찾은 파일 수: 3000\n",
      "찾은 파일 수: 3100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0005\n",
      "찾은 파일 수: 3200\n",
      "찾은 파일 수: 3300\n",
      "찾은 파일 수: 3400\n",
      "찾은 파일 수: 3500\n",
      "찾은 파일 수: 3600\n",
      "찾은 파일 수: 3700\n",
      "찾은 파일 수: 3800\n",
      "찾은 파일 수: 3900\n",
      "찾은 파일 수: 4000\n",
      "찾은 파일 수: 4100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0006\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0007\n",
      "찾은 파일 수: 4200\n",
      "찾은 파일 수: 4300\n",
      "찾은 파일 수: 4400\n",
      "찾은 파일 수: 4500\n",
      "찾은 파일 수: 4600\n",
      "찾은 파일 수: 4700\n",
      "찾은 파일 수: 4800\n",
      "찾은 파일 수: 4900\n",
      "찾은 파일 수: 5000\n",
      "찾은 파일 수: 5100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0008\n",
      "찾은 파일 수: 5200\n",
      "찾은 파일 수: 5300\n",
      "찾은 파일 수: 5400\n",
      "찾은 파일 수: 5500\n",
      "찾은 파일 수: 5600\n",
      "찾은 파일 수: 5700\n",
      "찾은 파일 수: 5800\n",
      "찾은 파일 수: 5900\n",
      "찾은 파일 수: 6000\n",
      "찾은 파일 수: 6100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0009\n",
      "찾은 파일 수: 6200\n",
      "찾은 파일 수: 6300\n",
      "찾은 파일 수: 6400\n",
      "찾은 파일 수: 6500\n",
      "찾은 파일 수: 6600\n",
      "찾은 파일 수: 6700\n",
      "찾은 파일 수: 6800\n",
      "찾은 파일 수: 6900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0010\n",
      "찾은 파일 수: 7000\n",
      "찾은 파일 수: 7100\n",
      "찾은 파일 수: 7200\n",
      "찾은 파일 수: 7300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0011\n",
      "찾은 파일 수: 7400\n",
      "찾은 파일 수: 7500\n",
      "찾은 파일 수: 7600\n",
      "찾은 파일 수: 7700\n",
      "찾은 파일 수: 7800\n",
      "찾은 파일 수: 7900\n",
      "찾은 파일 수: 8000\n",
      "찾은 파일 수: 8100\n",
      "찾은 파일 수: 8200\n",
      "찾은 파일 수: 8300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0012\n",
      "찾은 파일 수: 8400\n",
      "찾은 파일 수: 8500\n",
      "찾은 파일 수: 8600\n",
      "찾은 파일 수: 8700\n",
      "찾은 파일 수: 8800\n",
      "찾은 파일 수: 8900\n",
      "찾은 파일 수: 9000\n",
      "찾은 파일 수: 9100\n",
      "찾은 파일 수: 9200\n",
      "찾은 파일 수: 9300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0013\n",
      "찾은 파일 수: 9400\n",
      "찾은 파일 수: 9500\n",
      "찾은 파일 수: 9600\n",
      "찾은 파일 수: 9700\n",
      "찾은 파일 수: 9800\n",
      "찾은 파일 수: 9900\n",
      "찾은 파일 수: 10000\n",
      "찾은 파일 수: 10100\n",
      "찾은 파일 수: 10200\n",
      "찾은 파일 수: 10300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0014\n",
      "찾은 파일 수: 10400\n",
      "찾은 파일 수: 10500\n",
      "찾은 파일 수: 10600\n",
      "찾은 파일 수: 10700\n",
      "찾은 파일 수: 10800\n",
      "찾은 파일 수: 10900\n",
      "찾은 파일 수: 11000\n",
      "찾은 파일 수: 11100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0015\n",
      "찾은 파일 수: 11200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0016\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0017\n",
      "찾은 파일 수: 11300\n",
      "찾은 파일 수: 11400\n",
      "찾은 파일 수: 11500\n",
      "찾은 파일 수: 11600\n",
      "찾은 파일 수: 11700\n",
      "찾은 파일 수: 11800\n",
      "찾은 파일 수: 11900\n",
      "찾은 파일 수: 12000\n",
      "찾은 파일 수: 12100\n",
      "찾은 파일 수: 12200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0018\n",
      "찾은 파일 수: 12300\n",
      "찾은 파일 수: 12400\n",
      "찾은 파일 수: 12500\n",
      "찾은 파일 수: 12600\n",
      "찾은 파일 수: 12700\n",
      "총 12714개의 NP 파일을 찾았습니다.\n",
      "경고: 텍스트 파일이 존재하지 않습니다: PreprocessData/KsponSpeech_01/KsponSpeech_0018_g2p/KsponSpeech_017485.txt\n",
      "총 12713개의 텍스트 파일을 매핑했습니다.\n",
      "테스트할 파일: 12714개\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (float) and bias type (c10::Half) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 381\u001b[0m\n\u001b[1;32m    378\u001b[0m test_audio \u001b[38;5;241m=\u001b[39m test_npy_files[i]\n\u001b[1;32m    379\u001b[0m reference_text \u001b[38;5;241m=\u001b[39m load_text(test_txt_files[i])\n\u001b[0;32m--> 381\u001b[0m transcription \u001b[38;5;241m=\u001b[39m \u001b[43mtranscribe_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m파일: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(test_audio)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m원본 텍스트: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreference_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 343\u001b[0m, in \u001b[0;36mtranscribe_audio\u001b[0;34m(model, processor, audio_file)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;66;03m# 모델을 통한 예측\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 343\u001b[0m     predicted_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# 예측된 토큰을 텍스트로 디코딩\u001b[39;00m\n\u001b[1;32m    346\u001b[0m transcription \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(\n\u001b[1;32m    347\u001b[0m     predicted_ids,\n\u001b[1;32m    348\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    349\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/generation_whisper.py:555\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.generate\u001b[0;34m(self, input_features, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, return_timestamps, task, language, is_multilingual, prompt_ids, prompt_condition_type, condition_on_prev_tokens, temperature, compression_ratio_threshold, logprob_threshold, no_speech_threshold, num_segment_frames, attention_mask, time_precision, return_token_timestamps, return_segments, return_dict_in_generate, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_prompt_condition_type(\n\u001b[1;32m    550\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m    551\u001b[0m     prompt_condition_type\u001b[38;5;241m=\u001b[39mprompt_condition_type,\n\u001b[1;32m    552\u001b[0m )\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# pass self.config for backward compatibility\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m init_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retrieve_init_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# passing `decoder_input_ids` is deprecated - the only exception is for assisted generation\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# where the input ids are handled explicitly by the generate method\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_decoder_input_ids(kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/generation_whisper.py:1370\u001b[0m, in \u001b[0;36mWhisperGenerationMixin._retrieve_init_tokens\u001b[0;34m(self, input_features, batch_size, generation_config, config, num_segment_frames, kwargs)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     lang_ids \u001b[38;5;241m=\u001b[39m [language_to_id(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m languages]\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(generation_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang_to_id\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_lang_id_undefined:\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;66;03m# language is not defined or intentially set to `None` to trigger language detection\u001b[39;00m\n\u001b[0;32m-> 1370\u001b[0m     lang_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_language\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_outputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_segment_frames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_segment_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lang_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;66;03m# append or replace lang_ids to init_tokens\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(init_tokens)):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/generation_whisper.py:1474\u001b[0m, in \u001b[0;36mWhisperGenerationMixin.detect_language\u001b[0;34m(self, input_features, encoder_outputs, generation_config, num_segment_frames)\u001b[0m\n\u001b[1;32m   1468\u001b[0m decoder_input_ids \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1469\u001b[0m     torch\u001b[38;5;241m.\u001b[39mones((batch_size, \u001b[38;5;241m1\u001b[39m), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m   1470\u001b[0m     \u001b[38;5;241m*\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1471\u001b[0m )\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m-> 1474\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1476\u001b[0m non_lang_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(logits[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m   1477\u001b[0m non_lang_mask[\u001b[38;5;28mlist\u001b[39m(generation_config\u001b[38;5;241m.\u001b[39mlang_to_id\u001b[38;5;241m.\u001b[39mvalues())] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/utils/operations.py:820\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/accelerate/utils/operations.py:808\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/amp/autocast_mode.py:43\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[0;32m---> 43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py:1767\u001b[0m, in \u001b[0;36mWhisperForConditionalGeneration.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1763\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[1;32m   1764\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[1;32m   1765\u001b[0m         )\n\u001b[0;32m-> 1767\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1769\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1771\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1772\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1779\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1785\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1787\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py:1618\u001b[0m, in \u001b[0;36mWhisperModel.forward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, decoder_position_ids, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1616\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mask_input_features(input_features, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[0;32m-> 1618\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;66;03m# If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\u001b[39;00m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/modeling_whisper.py:1027\u001b[0m, in \u001b[0;36mWhisperEncoder.forward\u001b[0;34m(self, input_features, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1023\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1024\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[1;32m   1025\u001b[0m )\n\u001b[1;32m   1026\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1027\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1028\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(inputs_embeds))\n\u001b[1;32m   1030\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:308\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/conv.py:304\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    302\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    303\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (float) and bias type (c10::Half) should be the same"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "def load_text(file_path):\n",
    "    # 일반적인 한국어 인코딩 시도\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "\n",
    "def get_file_paths_npy(base_directory):\n",
    "    \"\"\"\n",
    "    디렉토리에서 NP 파일(combined_features.npy)의 경로를 가져옵니다.\n",
    "    \"\"\"\n",
    "    npy_files = []\n",
    "\n",
    "    # 디렉토리 구조에 맞게 폴더 목록 생성\n",
    "    base_folders = []\n",
    "    for i in range(1, 19):\n",
    "        if i > 9:\n",
    "            base_folders.append(f\"KsponSpeech_00{i}\")\n",
    "        else:\n",
    "            base_folders.append(f\"KsponSpeech_000{i}\")\n",
    "\n",
    "    # 모든 폴더 순회\n",
    "    for folder in base_folders:\n",
    "        folder_path = os.path.join(base_directory, folder)\n",
    "        \n",
    "        # 폴더가 존재하는지 확인\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"폴더 검색 중: {folder_path}\")\n",
    "            \n",
    "            # 폴더 내의 파일 검색\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('_combined_features.npy'):\n",
    "                        npy_path = os.path.join(root, file)\n",
    "                        npy_files.append(npy_path)\n",
    "                        if len(npy_files) % 100 == 0:  # 100개마다 로그 출력\n",
    "                            print(f\"찾은 파일 수: {len(npy_files)}\")\n",
    "        else:\n",
    "            print(f\"폴더가 존재하지 않음: {folder_path}\")\n",
    "    \n",
    "    print(f\"총 {len(npy_files)}개의 NP 파일을 찾았습니다.\")\n",
    "    return npy_files\n",
    "\n",
    "\n",
    "def get_file_paths_txt(npy_files):\n",
    "    \"\"\"\n",
    "    NP 파일 경로로부터 해당하는 텍스트 파일 경로를 생성합니다.\n",
    "    \"\"\"\n",
    "    txt_files = []\n",
    "    \n",
    "    for npy_path in npy_files:\n",
    "        # 파일명 추출 (경로와 확장자 제거)\n",
    "        file_name = os.path.basename(npy_path).replace('_combined_features.npy', '')\n",
    "        \n",
    "        # 디렉토리 구조 분석\n",
    "        dir_path = os.path.dirname(npy_path)\n",
    "        parent_dir = os.path.basename(dir_path)\n",
    "        \n",
    "        # 해당 G2P 폴더 경로 생성\n",
    "        g2p_dir = parent_dir + \"_g2p\"\n",
    "        g2p_dir_path = os.path.join(os.path.dirname(dir_path), g2p_dir)\n",
    "        \n",
    "        # 텍스트 파일 경로 생성\n",
    "        txt_path = os.path.join(g2p_dir_path, file_name + \".txt\")\n",
    "        \n",
    "        # 파일이 존재하는지 확인 (선택적)\n",
    "        if os.path.exists(txt_path):\n",
    "            txt_files.append(txt_path)\n",
    "        else:\n",
    "            print(f\"경고: 텍스트 파일이 존재하지 않습니다: {txt_path}\")\n",
    "    \n",
    "    print(f\"총 {len(txt_files)}개의 텍스트 파일을 매핑했습니다.\")\n",
    "    return txt_files\n",
    "\n",
    "\n",
    "def create_dataset(npy_files, txt_files, max_samples=None):\n",
    "    \"\"\"\n",
    "    NP 파일과 텍스트 파일로부터 데이터셋을 생성합니다.\n",
    "    \"\"\"\n",
    "    data = {\"audio\": [], \"text\": []}\n",
    "    # 최대 샘플 수 제한\n",
    "    if max_samples is not None:\n",
    "        npy_files = npy_files[:max_samples]\n",
    "        txt_files = txt_files[:max_samples]\n",
    "\n",
    "    for npy_file, txt_file in zip(npy_files, txt_files):\n",
    "        data[\"audio\"].append(npy_file)\n",
    "        data[\"text\"].append(load_text(txt_file))\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def train_whisper_model():\n",
    "    # 경로 수정\n",
    "    base_directory = \"PreprocessData/KsponSpeech_01\"  # unzipped_Speech에서 PreprocessData로 변경\n",
    "    output_dir = \"whisper_finetuned\"\n",
    "    \n",
    "    # 파일 경로 가져오기 함수 이름 변경\n",
    "    npy_files = get_file_paths_npy(base_directory)\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        print(\"파일을 찾을 수 없습니다. 경로와 파일 패턴을 확인하세요.\")\n",
    "        return None, None\n",
    "    \n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    print(f\"총 {len(npy_files)}개의 오디오 파일과 {len(txt_files)}개의 텍스트 파일이 로드되었습니다.\")\n",
    "    \n",
    "    # 파일 수가 일치하지 않으면 경고\n",
    "    if len(npy_files) != len(txt_files):\n",
    "        print(f\"경고: 오디오 파일 수({len(npy_files)})와 텍스트 파일 수({len(txt_files)})가 일치하지 않습니다.\")\n",
    "        # 일치하는 파일만 사용\n",
    "        if len(npy_files) > len(txt_files):\n",
    "            npy_files = npy_files[:len(txt_files)]\n",
    "        else:\n",
    "            txt_files = txt_files[:len(npy_files)]\n",
    "    \n",
    "    dataset = create_dataset(npy_files, txt_files)\n",
    "\n",
    "    def map_to_array(batch):\n",
    "        arrays = []\n",
    "        rates = []\n",
    "\n",
    "        for audio_path in batch[\"audio\"]:\n",
    "            # npy 파일 로드\n",
    "            audio_array = np.load(audio_path)\n",
    "            # float32로 변환하고 정규화 (필요한 경우)\n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "            sampling_rate = 16000\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(sampling_rate)\n",
    "\n",
    "        batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "        return batch\n",
    "\n",
    "    # 데이터셋에 Audio 형식 적용 (배치 처리)\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "\n",
    "    # 훈련/검증/테스트 세트 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    # Whisper 모델 및 프로세서 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-small\", \n",
    "    use_cache=False,\n",
    "    low_cpu_mem_usage=True\n",
    "    # torch_dtype=torch.float16 제거\n",
    "    )\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        # 오디오 처리\n",
    "        audio = batch[\"audio\"]\n",
    "    \n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "    \n",
    "        # NumPy 배열을 PyTorch 텐서로 변환\n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "    \n",
    "        # 텐서 타입 변환 (중요: 타입을 명시적으로 float16으로 설정하지 않음)\n",
    "        # input_features = input_features.to(model.dtype)  # 이 줄을 제거하거나 아래와 같이 수정\n",
    "    \n",
    "        # 텍스트 처리\n",
    "        labels = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    # 데이터셋 전처리 적용 (비배치 함수)\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    # 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "\n",
    "        # pred_ids가 튜플인 경우 (일반적으로 첫 번째 요소가 예측값)\n",
    "        if isinstance(pred_ids, tuple):\n",
    "            pred_ids = pred_ids[0]\n",
    "\n",
    "        # 이제 pred_ids가 텐서/배열인지 확인하고 3차원인 경우 처리\n",
    "        if hasattr(pred_ids, 'shape') and len(pred_ids.shape) > 2:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "        # 패딩 토큰 무시\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "        # 예측과 레이블을 텍스트로 디코딩\n",
    "        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # WER 계산 \n",
    "        # 참고: jiwer 라이브러리가 임포트되었는지 확인\n",
    "        import jiwer  # 이 라인 추가\n",
    "        wer = jiwer.wer(label_str, pred_str)\n",
    "\n",
    "        return {\"wer\": wer}\n",
    "\n",
    "    # 훈련 인자 설정\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=1e-5,\n",
    "        warmup_steps=500,\n",
    "        max_steps=4000,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,  # 혼합 정밀도는 여전히 활성화\n",
    "        # fp16_full_eval=True 제거 (평가를 하지 않으므로 불필요)\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_steps=500,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "        # 필요하지 않은 옵션들 제거\n",
    "    )\n",
    "\n",
    "    # 훈련기 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        # eval_dataset=processed_datasets[\"validation\"],  # 이 줄 제거 또는 주석 처리\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        # compute_metrics=compute_metrics,  # 이 줄 제거 또는 주석 처리\n",
    "    )\n",
    "\n",
    "    # 모델 훈련\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 저장\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    # 테스트 평가\n",
    "    test_results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "    print(f\"테스트 결과: {test_results}\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# 모델 추론 함수\n",
    "def transcribe_audio(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    # NP 파일 로드\n",
    "    audio_array = np.load(audio_file)\n",
    "    \n",
    "    # float32로 변환하고 정규화 (필요한 경우)\n",
    "    if audio_array.dtype in [np.int16, np.int8]:\n",
    "        max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "        audio_array = audio_array.astype(np.float32) / max_value\n",
    "    elif audio_array.dtype != np.float32:\n",
    "        audio_array = audio_array.astype(np.float32)\n",
    "    \n",
    "    # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "    sr = 16000\n",
    "\n",
    "    # 특성 추출\n",
    "    input_features = processor.feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "\n",
    "    # 모델을 통한 예측\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    # 예측된 토큰을 텍스트로 디코딩\n",
    "    transcription = processor.tokenizer.batch_decode(\n",
    "        predicted_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 훈련\n",
    "    model, processor = train_whisper_model()\n",
    "    \n",
    "    if model is None or processor is None:\n",
    "        print(\"모델 훈련에 실패했습니다.\")\n",
    "        exit()\n",
    "    \n",
    "    # 테스트할 디렉터리 설정\n",
    "    test_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    \n",
    "    # 테스트 파일 가져오기 - 함수 이름 변경\n",
    "    npy_files = get_file_paths_npy(test_directory)\n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    \n",
    "    print(f\"테스트할 파일: {len(npy_files)}개\")\n",
    "    \n",
    "    # 테스트할 파일 수 제한 (선택적)\n",
    "    max_test_files = 10\n",
    "    test_npy_files = npy_files[:max_test_files]\n",
    "    test_txt_files = txt_files[:max_test_files]\n",
    "    \n",
    "    # 모든 파일 테스트\n",
    "    for i in range(len(test_npy_files)):\n",
    "        test_audio = test_npy_files[i]\n",
    "        reference_text = load_text(test_txt_files[i])\n",
    "        \n",
    "        transcription = transcribe_audio(model, processor, test_audio)\n",
    "        \n",
    "        print(f\"파일: {os.path.basename(test_audio)}\")\n",
    "        print(f\"원본 텍스트: {reference_text}\")\n",
    "        print(f\"변환 결과: {transcription}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f75050-8e21-465e-8d1f-0a4f54faefef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    # 일반적인 한국어 인코딩 시도\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "\n",
    "def get_file_paths_npy(base_directory):\n",
    "    \"\"\"\n",
    "    디렉토리에서 NP 파일(combined_features.npy)의 경로를 가져옵니다.\n",
    "    \"\"\"\n",
    "    npy_files = []\n",
    "\n",
    "    # 디렉토리 구조에 맞게 폴더 목록 생성\n",
    "    base_folders = []\n",
    "    for i in range(1, 19):\n",
    "        if i > 9:\n",
    "            base_folders.append(f\"KsponSpeech_00{i}\")\n",
    "        else:\n",
    "            base_folders.append(f\"KsponSpeech_000{i}\")\n",
    "\n",
    "    # 모든 폴더 순회\n",
    "    for folder in base_folders:\n",
    "        folder_path = os.path.join(base_directory, folder)\n",
    "        \n",
    "        # 폴더가 존재하는지 확인\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"폴더 검색 중: {folder_path}\")\n",
    "            \n",
    "            # 폴더 내의 파일 검색\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('_combined_features.npy'):\n",
    "                        npy_path = os.path.join(root, file)\n",
    "                        npy_files.append(npy_path)\n",
    "                        if len(npy_files) % 100 == 0:  # 100개마다 로그 출력\n",
    "                            print(f\"찾은 파일 수: {len(npy_files)}\")\n",
    "        else:\n",
    "            print(f\"폴더가 존재하지 않음: {folder_path}\")\n",
    "    \n",
    "    print(f\"총 {len(npy_files)}개의 NP 파일을 찾았습니다.\")\n",
    "    return npy_files\n",
    "\n",
    "\n",
    "def get_file_paths_txt(npy_files):\n",
    "    \"\"\"\n",
    "    NP 파일 경로로부터 해당하는 텍스트 파일 경로를 생성합니다.\n",
    "    \"\"\"\n",
    "    txt_files = []\n",
    "    \n",
    "    for npy_path in npy_files:\n",
    "        # 파일명 추출 (경로와 확장자 제거)\n",
    "        file_name = os.path.basename(npy_path).replace('_combined_features.npy', '')\n",
    "        \n",
    "        # 디렉토리 구조 분석\n",
    "        dir_path = os.path.dirname(npy_path)\n",
    "        parent_dir = os.path.basename(dir_path)\n",
    "        \n",
    "        # 해당 G2P 폴더 경로 생성\n",
    "        g2p_dir = parent_dir + \"_g2p\"\n",
    "        g2p_dir_path = os.path.join(os.path.dirname(dir_path), g2p_dir)\n",
    "        \n",
    "        # 텍스트 파일 경로 생성\n",
    "        txt_path = os.path.join(g2p_dir_path, file_name + \".txt\")\n",
    "        \n",
    "        # 파일이 존재하는지 확인 (선택적)\n",
    "        if os.path.exists(txt_path):\n",
    "            txt_files.append(txt_path)\n",
    "        else:\n",
    "            print(f\"경고: 텍스트 파일이 존재하지 않습니다: {txt_path}\")\n",
    "    \n",
    "    print(f\"총 {len(txt_files)}개의 텍스트 파일을 매핑했습니다.\")\n",
    "    return txt_files\n",
    "\n",
    "\n",
    "def create_dataset(npy_files, txt_files, max_samples=None):\n",
    "    \"\"\"\n",
    "    NP 파일과 텍스트 파일로부터 데이터셋을 생성합니다.\n",
    "    \"\"\"\n",
    "    data = {\"audio\": [], \"text\": []}\n",
    "    \n",
    "    # 최대 샘플 수 제한\n",
    "    if max_samples is not None:\n",
    "        npy_files = npy_files[:max_samples]\n",
    "        txt_files = txt_files[:max_samples]\n",
    "\n",
    "    for npy_file, txt_file in zip(npy_files, txt_files):\n",
    "        try:\n",
    "            # 파일 존재 확인\n",
    "            if os.path.exists(npy_file) and os.path.exists(txt_file):\n",
    "                # npy 파일 로드 테스트\n",
    "                test_array = np.load(npy_file)\n",
    "                if len(test_array) > 0:  # 빈 배열 체크\n",
    "                    data[\"audio\"].append(npy_file)\n",
    "                    data[\"text\"].append(load_text(txt_file))\n",
    "                else:\n",
    "                    print(f\"빈 오디오 파일 건너뜀: {npy_file}\")\n",
    "            else:\n",
    "                print(f\"파일 없음: {npy_file} 또는 {txt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"파일 로드 오류: {npy_file}, 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def train_whisper_model():\n",
    "    # 경로 수정\n",
    "    base_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    output_dir = \"whisper_finetuned\"\n",
    "    \n",
    "    # 파일 경로 가져오기\n",
    "    npy_files = get_file_paths_npy(base_directory)\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        print(\"파일을 찾을 수 없습니다. 경로와 파일 패턴을 확인하세요.\")\n",
    "        return None, None\n",
    "    \n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    print(f\"총 {len(npy_files)}개의 오디오 파일과 {len(txt_files)}개의 텍스트 파일이 로드되었습니다.\")\n",
    "    \n",
    "    # 파일 수가 일치하지 않으면 경고\n",
    "    if len(npy_files) != len(txt_files):\n",
    "        print(f\"경고: 오디오 파일 수({len(npy_files)})와 텍스트 파일 수({len(txt_files)})가 일치하지 않습니다.\")\n",
    "        # 일치하는 파일만 사용\n",
    "        if len(npy_files) > len(txt_files):\n",
    "            npy_files = npy_files[:len(txt_files)]\n",
    "        else:\n",
    "            txt_files = txt_files[:len(npy_files)]\n",
    "    \n",
    "    dataset = create_dataset(npy_files, txt_files)\n",
    "\n",
    "    def map_to_array(batch):\n",
    "        arrays = []\n",
    "        rates = []\n",
    "\n",
    "        for audio_path in batch[\"audio\"]:\n",
    "            # npy 파일 로드\n",
    "            audio_array = np.load(audio_path)\n",
    "            # float32로 변환하고 정규화 (필요한 경우)\n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "            sampling_rate = 16000\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(sampling_rate)\n",
    "\n",
    "        batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "        return batch\n",
    "\n",
    "    # 데이터셋에 Audio 형식 적용 (배치 처리)\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "\n",
    "    # 훈련/검증/테스트 세트 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    # Whisper 모델 및 프로세서 로드 (fp16 사용)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16  # fp16 사용\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        # 오디오 처리\n",
    "        audio = batch[\"audio\"]\n",
    "    \n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "    \n",
    "        # NumPy 배열을 PyTorch 텐서로 변환\n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "    \n",
    "        # 타입을 모델과 일치시킴 (fp16)\n",
    "        input_features = input_features.to(model.dtype)\n",
    "    \n",
    "        # 텍스트 처리\n",
    "        labels = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    # 데이터셋 전처리 적용\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    # 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "\n",
    "        # pred_ids가 튜플인 경우 (일반적으로 첫 번째 요소가 예측값)\n",
    "        if isinstance(pred_ids, tuple):\n",
    "            pred_ids = pred_ids[0]\n",
    "\n",
    "        # 이제 pred_ids가 텐서/배열인지 확인하고 3차원인 경우 처리\n",
    "        if hasattr(pred_ids, 'shape') and len(pred_ids.shape) > 2:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "        # 패딩 토큰 무시\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "        # 예측과 레이블을 텍스트로 디코딩\n",
    "        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # WER 계산\n",
    "        try:\n",
    "            import jiwer\n",
    "            wer = jiwer.wer(label_str, pred_str)\n",
    "        except ImportError:\n",
    "            print(\"jiwer 라이브러리가 설치되지 않았습니다. WER 계산을 건너뜁니다.\")\n",
    "            wer = 0.0\n",
    "\n",
    "        return {\"wer\": wer}\n",
    "\n",
    "    # 훈련 인자 설정 (안정적인 fp16 설정)\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=5e-6,  # 학습률 낮춤 (원래 1e-5에서)\n",
    "        warmup_steps=500,\n",
    "        max_steps=4000,\n",
    "        gradient_checkpointing=True,\n",
    "        fp16=True,\n",
    "        max_grad_norm=1.0,  # gradient clipping 적당히 설정\n",
    "        dataloader_pin_memory=False,\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_steps=500,\n",
    "        logging_steps=25,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # 훈련기 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "\n",
    "    # 모델 훈련\n",
    "    print(\"모델 훈련을 시작합니다...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 저장\n",
    "    print(\"모델을 저장합니다...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    # 테스트 평가\n",
    "    print(\"테스트 평가를 수행합니다...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "        print(f\"테스트 결과: {test_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"테스트 평가 중 오류 발생: {e}\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def transcribe_audio(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NP 파일 로드\n",
    "        audio_array = np.load(audio_file)\n",
    "        \n",
    "        # float32로 변환하고 정규화 (필요한 경우)\n",
    "        if audio_array.dtype in [np.int16, np.int8]:\n",
    "            max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "            audio_array = audio_array.astype(np.float32) / max_value\n",
    "        elif audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "        sr = 16000\n",
    "\n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        # 타입을 모델과 일치시킴\n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # GPU 사용하는 경우\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # 모델을 통한 예측\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                language=\"korean\",  # 한국어 명시\n",
    "                task=\"transcribe\"\n",
    "            )\n",
    "\n",
    "        # 예측된 토큰을 텍스트로 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 오류: {e}\")\n",
    "        return \"음성 인식 실패\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Whisper 한국어 파인튜닝을 시작합니다...\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model, processor = train_whisper_model()\n",
    "    \n",
    "    if model is None or processor is None:\n",
    "        print(\"모델 훈련에 실패했습니다.\")\n",
    "        exit()\n",
    "    \n",
    "    print(\"모델 훈련이 완료되었습니다!\")\n",
    "    \n",
    "    # 테스트할 디렉터리 설정\n",
    "    test_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    \n",
    "    # 테스트 파일 가져오기\n",
    "    npy_files = get_file_paths_npy(test_directory)\n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    \n",
    "    print(f\"테스트할 파일: {len(npy_files)}개\")\n",
    "    \n",
    "    # 테스트할 파일 수 제한 (선택적)\n",
    "    max_test_files = 10\n",
    "    test_npy_files = npy_files[:max_test_files]\n",
    "    test_txt_files = txt_files[:max_test_files]\n",
    "    \n",
    "    print(\"테스트를 시작합니다...\")\n",
    "    \n",
    "    # 모든 파일 테스트\n",
    "    for i in range(len(test_npy_files)):\n",
    "        test_audio = test_npy_files[i]\n",
    "        reference_text = load_text(test_txt_files[i])\n",
    "        \n",
    "        transcription = transcribe_audio(model, processor, test_audio)\n",
    "        \n",
    "        print(f\"파일: {os.path.basename(test_audio)}\")\n",
    "        print(f\"원본 텍스트: {reference_text}\")\n",
    "        print(f\"변환 결과: {transcription}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"모든 작업이 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a74e3319-90a9-403f-bb29-70539811a2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-26 15:06:10.895768: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-26 15:06:10.943293: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-26 15:06:11.655394: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper 한국어 파인튜닝을 시작합니다...\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0001\n",
      "찾은 파일 수: 100\n",
      "찾은 파일 수: 200\n",
      "찾은 파일 수: 300\n",
      "찾은 파일 수: 400\n",
      "찾은 파일 수: 500\n",
      "찾은 파일 수: 600\n",
      "찾은 파일 수: 700\n",
      "찾은 파일 수: 800\n",
      "찾은 파일 수: 900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0002\n",
      "찾은 파일 수: 1000\n",
      "찾은 파일 수: 1100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0003\n",
      "찾은 파일 수: 1200\n",
      "찾은 파일 수: 1300\n",
      "찾은 파일 수: 1400\n",
      "찾은 파일 수: 1500\n",
      "찾은 파일 수: 1600\n",
      "찾은 파일 수: 1700\n",
      "찾은 파일 수: 1800\n",
      "찾은 파일 수: 1900\n",
      "찾은 파일 수: 2000\n",
      "찾은 파일 수: 2100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0004\n",
      "찾은 파일 수: 2200\n",
      "찾은 파일 수: 2300\n",
      "찾은 파일 수: 2400\n",
      "찾은 파일 수: 2500\n",
      "찾은 파일 수: 2600\n",
      "찾은 파일 수: 2700\n",
      "찾은 파일 수: 2800\n",
      "찾은 파일 수: 2900\n",
      "찾은 파일 수: 3000\n",
      "찾은 파일 수: 3100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0005\n",
      "찾은 파일 수: 3200\n",
      "찾은 파일 수: 3300\n",
      "찾은 파일 수: 3400\n",
      "찾은 파일 수: 3500\n",
      "찾은 파일 수: 3600\n",
      "찾은 파일 수: 3700\n",
      "찾은 파일 수: 3800\n",
      "찾은 파일 수: 3900\n",
      "찾은 파일 수: 4000\n",
      "찾은 파일 수: 4100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0006\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0007\n",
      "찾은 파일 수: 4200\n",
      "찾은 파일 수: 4300\n",
      "찾은 파일 수: 4400\n",
      "찾은 파일 수: 4500\n",
      "찾은 파일 수: 4600\n",
      "찾은 파일 수: 4700\n",
      "찾은 파일 수: 4800\n",
      "찾은 파일 수: 4900\n",
      "찾은 파일 수: 5000\n",
      "찾은 파일 수: 5100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0008\n",
      "찾은 파일 수: 5200\n",
      "찾은 파일 수: 5300\n",
      "찾은 파일 수: 5400\n",
      "찾은 파일 수: 5500\n",
      "찾은 파일 수: 5600\n",
      "찾은 파일 수: 5700\n",
      "찾은 파일 수: 5800\n",
      "찾은 파일 수: 5900\n",
      "찾은 파일 수: 6000\n",
      "찾은 파일 수: 6100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0009\n",
      "찾은 파일 수: 6200\n",
      "찾은 파일 수: 6300\n",
      "찾은 파일 수: 6400\n",
      "찾은 파일 수: 6500\n",
      "찾은 파일 수: 6600\n",
      "찾은 파일 수: 6700\n",
      "찾은 파일 수: 6800\n",
      "찾은 파일 수: 6900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0010\n",
      "찾은 파일 수: 7000\n",
      "찾은 파일 수: 7100\n",
      "찾은 파일 수: 7200\n",
      "찾은 파일 수: 7300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0011\n",
      "찾은 파일 수: 7400\n",
      "찾은 파일 수: 7500\n",
      "찾은 파일 수: 7600\n",
      "찾은 파일 수: 7700\n",
      "찾은 파일 수: 7800\n",
      "찾은 파일 수: 7900\n",
      "찾은 파일 수: 8000\n",
      "찾은 파일 수: 8100\n",
      "찾은 파일 수: 8200\n",
      "찾은 파일 수: 8300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0012\n",
      "찾은 파일 수: 8400\n",
      "찾은 파일 수: 8500\n",
      "찾은 파일 수: 8600\n",
      "찾은 파일 수: 8700\n",
      "찾은 파일 수: 8800\n",
      "찾은 파일 수: 8900\n",
      "찾은 파일 수: 9000\n",
      "찾은 파일 수: 9100\n",
      "찾은 파일 수: 9200\n",
      "찾은 파일 수: 9300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0013\n",
      "찾은 파일 수: 9400\n",
      "찾은 파일 수: 9500\n",
      "찾은 파일 수: 9600\n",
      "찾은 파일 수: 9700\n",
      "찾은 파일 수: 9800\n",
      "찾은 파일 수: 9900\n",
      "찾은 파일 수: 10000\n",
      "찾은 파일 수: 10100\n",
      "찾은 파일 수: 10200\n",
      "찾은 파일 수: 10300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0014\n",
      "찾은 파일 수: 10400\n",
      "찾은 파일 수: 10500\n",
      "찾은 파일 수: 10600\n",
      "찾은 파일 수: 10700\n",
      "찾은 파일 수: 10800\n",
      "찾은 파일 수: 10900\n",
      "찾은 파일 수: 11000\n",
      "찾은 파일 수: 11100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0015\n",
      "찾은 파일 수: 11200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0016\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0017\n",
      "찾은 파일 수: 11300\n",
      "찾은 파일 수: 11400\n",
      "찾은 파일 수: 11500\n",
      "찾은 파일 수: 11600\n",
      "찾은 파일 수: 11700\n",
      "찾은 파일 수: 11800\n",
      "찾은 파일 수: 11900\n",
      "찾은 파일 수: 12000\n",
      "찾은 파일 수: 12100\n",
      "찾은 파일 수: 12200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0018\n",
      "찾은 파일 수: 12300\n",
      "찾은 파일 수: 12400\n",
      "찾은 파일 수: 12500\n",
      "찾은 파일 수: 12600\n",
      "찾은 파일 수: 12700\n",
      "총 12714개의 NP 파일을 찾았습니다.\n",
      "경고: 텍스트 파일이 존재하지 않습니다: PreprocessData/KsponSpeech_01/KsponSpeech_0018_g2p/KsponSpeech_017485.txt\n",
      "총 12713개의 텍스트 파일을 매핑했습니다.\n",
      "총 12714개의 오디오 파일과 12713개의 텍스트 파일이 로드되었습니다.\n",
      "경고: 오디오 파일 수(12714)와 텍스트 파일 수(12713)가 일치하지 않습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 12713/12713 [00:05<00:00, 2151.98 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10170/10170 [35:18<00:00,  4.80 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1272/1272 [04:27<00:00,  4.76 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1271/1271 [04:27<00:00,  4.76 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 능력: (8, 6), BF16 지원: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24804/282252810.py:339: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 훈련을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4000' max='4000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4000/4000 4:53:34, Epoch 6/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>9.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>6.884500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.506800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>4.860200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.657900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>4.520100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.439200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>4.248900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.072400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>3.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>3.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>3.619800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>3.607500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>3.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>3.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.411900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>3.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>3.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.352400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>3.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>3.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>3.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>3.191800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>3.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>3.171500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.169300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>3.090700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>3.128800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>3.160500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>3.125500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.092100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>3.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>3.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>3.079500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>3.042600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>2.880300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.898800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>2.885600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>2.944400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.950900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>2.916500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.869200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>2.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.869900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>2.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>2.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.901100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>2.928000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.869300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>2.894300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.933800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>2.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.910700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>2.880500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.882700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>2.899100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>2.793700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>2.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.665800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>2.717500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>2.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>2.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.716700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>2.680600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.745700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>2.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.730800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>2.676100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>2.669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>2.721400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.699900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>2.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.710700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>2.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.697700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>2.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>2.516100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.523600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>2.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>2.575100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>2.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>2.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>2.641600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.508100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>2.573600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>2.571200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.568900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>2.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.551800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>2.509000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.548000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>2.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>2.615300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.537400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3175</td>\n",
       "      <td>2.569400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>2.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.452100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3275</td>\n",
       "      <td>2.399000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.444100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3325</td>\n",
       "      <td>2.432400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3375</td>\n",
       "      <td>2.458900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3425</td>\n",
       "      <td>2.413500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3475</td>\n",
       "      <td>2.438200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.405000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3525</td>\n",
       "      <td>2.437200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.412200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3575</td>\n",
       "      <td>2.396100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.433000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3625</td>\n",
       "      <td>2.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.449600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>2.421100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.469200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3725</td>\n",
       "      <td>2.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.418400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3775</td>\n",
       "      <td>2.469300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3825</td>\n",
       "      <td>2.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>2.354300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3875</td>\n",
       "      <td>2.347100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.365200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3925</td>\n",
       "      <td>2.341500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>2.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3975</td>\n",
       "      <td>2.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.337800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델을 저장합니다...\n",
      "테스트 평가를 수행합니다...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1272' max='1272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1272/1272 02:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 결과: {'eval_loss': 3.03531551361084, 'eval_runtime': 163.0837, 'eval_samples_per_second': 7.8, 'eval_steps_per_second': 7.8, 'epoch': 6.289308176100629}\n",
      "모델 훈련이 완료되었습니다!\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0001\n",
      "찾은 파일 수: 100\n",
      "찾은 파일 수: 200\n",
      "찾은 파일 수: 300\n",
      "찾은 파일 수: 400\n",
      "찾은 파일 수: 500\n",
      "찾은 파일 수: 600\n",
      "찾은 파일 수: 700\n",
      "찾은 파일 수: 800\n",
      "찾은 파일 수: 900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0002\n",
      "찾은 파일 수: 1000\n",
      "찾은 파일 수: 1100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0003\n",
      "찾은 파일 수: 1200\n",
      "찾은 파일 수: 1300\n",
      "찾은 파일 수: 1400\n",
      "찾은 파일 수: 1500\n",
      "찾은 파일 수: 1600\n",
      "찾은 파일 수: 1700\n",
      "찾은 파일 수: 1800\n",
      "찾은 파일 수: 1900\n",
      "찾은 파일 수: 2000\n",
      "찾은 파일 수: 2100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0004\n",
      "찾은 파일 수: 2200\n",
      "찾은 파일 수: 2300\n",
      "찾은 파일 수: 2400\n",
      "찾은 파일 수: 2500\n",
      "찾은 파일 수: 2600\n",
      "찾은 파일 수: 2700\n",
      "찾은 파일 수: 2800\n",
      "찾은 파일 수: 2900\n",
      "찾은 파일 수: 3000\n",
      "찾은 파일 수: 3100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0005\n",
      "찾은 파일 수: 3200\n",
      "찾은 파일 수: 3300\n",
      "찾은 파일 수: 3400\n",
      "찾은 파일 수: 3500\n",
      "찾은 파일 수: 3600\n",
      "찾은 파일 수: 3700\n",
      "찾은 파일 수: 3800\n",
      "찾은 파일 수: 3900\n",
      "찾은 파일 수: 4000\n",
      "찾은 파일 수: 4100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0006\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0007\n",
      "찾은 파일 수: 4200\n",
      "찾은 파일 수: 4300\n",
      "찾은 파일 수: 4400\n",
      "찾은 파일 수: 4500\n",
      "찾은 파일 수: 4600\n",
      "찾은 파일 수: 4700\n",
      "찾은 파일 수: 4800\n",
      "찾은 파일 수: 4900\n",
      "찾은 파일 수: 5000\n",
      "찾은 파일 수: 5100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0008\n",
      "찾은 파일 수: 5200\n",
      "찾은 파일 수: 5300\n",
      "찾은 파일 수: 5400\n",
      "찾은 파일 수: 5500\n",
      "찾은 파일 수: 5600\n",
      "찾은 파일 수: 5700\n",
      "찾은 파일 수: 5800\n",
      "찾은 파일 수: 5900\n",
      "찾은 파일 수: 6000\n",
      "찾은 파일 수: 6100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0009\n",
      "찾은 파일 수: 6200\n",
      "찾은 파일 수: 6300\n",
      "찾은 파일 수: 6400\n",
      "찾은 파일 수: 6500\n",
      "찾은 파일 수: 6600\n",
      "찾은 파일 수: 6700\n",
      "찾은 파일 수: 6800\n",
      "찾은 파일 수: 6900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0010\n",
      "찾은 파일 수: 7000\n",
      "찾은 파일 수: 7100\n",
      "찾은 파일 수: 7200\n",
      "찾은 파일 수: 7300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0011\n",
      "찾은 파일 수: 7400\n",
      "찾은 파일 수: 7500\n",
      "찾은 파일 수: 7600\n",
      "찾은 파일 수: 7700\n",
      "찾은 파일 수: 7800\n",
      "찾은 파일 수: 7900\n",
      "찾은 파일 수: 8000\n",
      "찾은 파일 수: 8100\n",
      "찾은 파일 수: 8200\n",
      "찾은 파일 수: 8300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0012\n",
      "찾은 파일 수: 8400\n",
      "찾은 파일 수: 8500\n",
      "찾은 파일 수: 8600\n",
      "찾은 파일 수: 8700\n",
      "찾은 파일 수: 8800\n",
      "찾은 파일 수: 8900\n",
      "찾은 파일 수: 9000\n",
      "찾은 파일 수: 9100\n",
      "찾은 파일 수: 9200\n",
      "찾은 파일 수: 9300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0013\n",
      "찾은 파일 수: 9400\n",
      "찾은 파일 수: 9500\n",
      "찾은 파일 수: 9600\n",
      "찾은 파일 수: 9700\n",
      "찾은 파일 수: 9800\n",
      "찾은 파일 수: 9900\n",
      "찾은 파일 수: 10000\n",
      "찾은 파일 수: 10100\n",
      "찾은 파일 수: 10200\n",
      "찾은 파일 수: 10300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0014\n",
      "찾은 파일 수: 10400\n",
      "찾은 파일 수: 10500\n",
      "찾은 파일 수: 10600\n",
      "찾은 파일 수: 10700\n",
      "찾은 파일 수: 10800\n",
      "찾은 파일 수: 10900\n",
      "찾은 파일 수: 11000\n",
      "찾은 파일 수: 11100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0015\n",
      "찾은 파일 수: 11200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0016\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0017\n",
      "찾은 파일 수: 11300\n",
      "찾은 파일 수: 11400\n",
      "찾은 파일 수: 11500\n",
      "찾은 파일 수: 11600\n",
      "찾은 파일 수: 11700\n",
      "찾은 파일 수: 11800\n",
      "찾은 파일 수: 11900\n",
      "찾은 파일 수: 12000\n",
      "찾은 파일 수: 12100\n",
      "찾은 파일 수: 12200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0018\n",
      "찾은 파일 수: 12300\n",
      "찾은 파일 수: 12400\n",
      "찾은 파일 수: 12500\n",
      "찾은 파일 수: 12600\n",
      "찾은 파일 수: 12700\n",
      "총 12714개의 NP 파일을 찾았습니다.\n",
      "경고: 텍스트 파일이 존재하지 않습니다: PreprocessData/KsponSpeech_01/KsponSpeech_0018_g2p/KsponSpeech_017485.txt\n",
      "총 12713개의 텍스트 파일을 매핑했습니다.\n",
      "테스트할 파일: 12714개\n",
      "테스트를 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파일: KsponSpeech_000455_combined_features.npy\n",
      "원본 텍스트: 오/ 근데 마 눤짜리 끼면 누 나파가지고. 비/\n",
      "변환 결과: 오늘 러무한 사람한테는 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스탐. 비스태스 러무한 사람드리에 대해서는 비스태을 러무한 사람드리 비스태�를 러무한 사람드리에 대해서는 비스태�를 러무한 사람드리에 대해서는 비스태�를 러무한 사람드리에 대해서는 비스태��만 러무한 사람드리에 대해서는 비스태��만 러무한 사람드리에 대해서는 비스태마니르 러무한지 러무한지 러무한지 러무한지 러무한지 러무한지 러마나 러맄마나 러마나 러마나 러마나 마나 마나 마나 마나 마나 마나 마나 마나\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000336_combined_features.npy\n",
      "원본 텍스트: 오/ 하, 근데, 그러케 하면 또, 보고시픈 공여니나 영화이쓰면 몯 뽀고 이러니까,\n",
      "변환 결과: 오늘 래가 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지?지?지?지? 뭐지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000639_combined_features.npy\n",
      "원본 텍스트: 엔/ 아파트 별로라고 하지 아나써? 엔/\n",
      "변환 결과: 오늘 롤러스에다가 뭐지? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000961_combined_features.npy\n",
      "원본 텍스트: 아니. 프라하가 가고 십따면서 갑짜기 가고 십 삐/ 엔/\n",
      "변환 결과: 오늘 롤러스에다가 뭐지? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤젤\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000124_combined_features.npy\n",
      "원본 텍스트: 금 그 뒤쪼길꺼 라 걔 이번 주 모교+ 모교일 랄 학꾜 간다 그랜는데.\n",
      "변환 결과: 오늘 러무한 사람한테는 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리 비스태스 러무한 사람드리에 대해서는 비스태�레서는 비스태을 러무한 사람드리에 대해서는 비스태�레서는 비스태�래요. 비스태�래서는 비스태�래요. 비스태�래서는 비스태�래요. 비스태레서는 비스태�래요. 비스태레서는 비스태�래요. 비스태레서는 비스태레서는 비스태�래요. 비스태레서는 비스태레서는 비스태레 인제니르 럴�리니르 럴�리니르 럳���� 럼�르 럳�\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000139_combined_features.npy\n",
      "원본 텍스트: 올라도 탈 사람 다 타. 맨날 다들 비/ 어/ 이제 몯 탄다, 몯 타겓따 그러는데 비/ 담배만 해도 봐봐. 담배 비/ 어 (이배)/(두 배)가 올란는데\n",
      "변환 결과: 오늘 래가 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지?지?지? 뭐지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000753_combined_features.npy\n",
      "원본 텍스트: 아니야. 나 그때 거의 실세+ 실세여써.\n",
      "변환 결과: 오늘 롤러스에다가 뭐지? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤젤? 엔젤? 엔젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤젤? 엔젤젤젤\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000212_combined_features.npy\n",
      "원본 텍스트: 너 소아과 가면 진짜 애들 때리고 날리도 아니겓따.\n",
      "변환 결과: 오늘 롤러스에다가 뭐지? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤? 엔젤젤? 엔젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤? 엔젤젤젤? 엔젤젤젤\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000238_combined_features.npy\n",
      "원본 텍스트: 오/ 친해질 뭐/ 노력또 하지 안는다 이딴 시그로 얘기해서 비/ 아/ 걔 볼 때마다 정말 러무 민망하더라고, 음/ 그래써.\n",
      "변환 결과: 오늘 래가 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지? 뭐지?지? 뭐지?지? 뭐지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지?지\n",
      "--------------------------------------------------\n",
      "파일: KsponSpeech_000233_combined_features.npy\n",
      "원본 텍스트: 그/ 약깐 뭐라 해야 되지? 다 마자야 된다는 그런 관념?\n",
      "변환 결과: 오늘 롤러스 러떠케 얘기하자고? 엘, 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행기니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행니까. 비행�까는 가워는 가워는 가워는 가워는 가워는 가워는 가워는 가워\n",
      "--------------------------------------------------\n",
      "모든 작업이 완료되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "# 오전 12시 6분 밤에 돌린거\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    # 일반적인 한국어 인코딩 시도\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "\n",
    "def get_file_paths_npy(base_directory):\n",
    "    \"\"\"\n",
    "    디렉토리에서 NP 파일(combined_features.npy)의 경로를 가져옵니다.\n",
    "    \"\"\"\n",
    "    npy_files = []\n",
    "\n",
    "    # 디렉토리 구조에 맞게 폴더 목록 생성\n",
    "    base_folders = []\n",
    "    for i in range(1, 19):\n",
    "        if i > 9:\n",
    "            base_folders.append(f\"KsponSpeech_00{i}\")\n",
    "        else:\n",
    "            base_folders.append(f\"KsponSpeech_000{i}\")\n",
    "\n",
    "    # 모든 폴더 순회\n",
    "    for folder in base_folders:\n",
    "        folder_path = os.path.join(base_directory, folder)\n",
    "        \n",
    "        # 폴더가 존재하는지 확인\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"폴더 검색 중: {folder_path}\")\n",
    "            \n",
    "            # 폴더 내의 파일 검색\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('_combined_features.npy'):\n",
    "                        npy_path = os.path.join(root, file)\n",
    "                        npy_files.append(npy_path)\n",
    "                        if len(npy_files) % 100 == 0:  # 100개마다 로그 출력\n",
    "                            print(f\"찾은 파일 수: {len(npy_files)}\")\n",
    "        else:\n",
    "            print(f\"폴더가 존재하지 않음: {folder_path}\")\n",
    "    \n",
    "    print(f\"총 {len(npy_files)}개의 NP 파일을 찾았습니다.\")\n",
    "    return npy_files\n",
    "\n",
    "\n",
    "def get_file_paths_txt(npy_files):\n",
    "    \"\"\"\n",
    "    NP 파일 경로로부터 해당하는 텍스트 파일 경로를 생성합니다.\n",
    "    \"\"\"\n",
    "    txt_files = []\n",
    "    \n",
    "    for npy_path in npy_files:\n",
    "        # 파일명 추출 (경로와 확장자 제거)\n",
    "        file_name = os.path.basename(npy_path).replace('_combined_features.npy', '')\n",
    "        \n",
    "        # 디렉토리 구조 분석\n",
    "        dir_path = os.path.dirname(npy_path)\n",
    "        parent_dir = os.path.basename(dir_path)\n",
    "        \n",
    "        # 해당 G2P 폴더 경로 생성\n",
    "        g2p_dir = parent_dir + \"_g2p\"\n",
    "        g2p_dir_path = os.path.join(os.path.dirname(dir_path), g2p_dir)\n",
    "        \n",
    "        # 텍스트 파일 경로 생성\n",
    "        txt_path = os.path.join(g2p_dir_path, file_name + \".txt\")\n",
    "        \n",
    "        # 파일이 존재하는지 확인 (선택적)\n",
    "        if os.path.exists(txt_path):\n",
    "            txt_files.append(txt_path)\n",
    "        else:\n",
    "            print(f\"경고: 텍스트 파일이 존재하지 않습니다: {txt_path}\")\n",
    "    \n",
    "    print(f\"총 {len(txt_files)}개의 텍스트 파일을 매핑했습니다.\")\n",
    "    return txt_files\n",
    "\n",
    "\n",
    "def create_dataset(npy_files, txt_files, max_samples=None):\n",
    "    \"\"\"\n",
    "    NP 파일과 텍스트 파일로부터 데이터셋을 생성합니다.\n",
    "    \"\"\"\n",
    "    data = {\"audio\": [], \"text\": []}\n",
    "    \n",
    "    # 최대 샘플 수 제한\n",
    "    if max_samples is not None:\n",
    "        npy_files = npy_files[:max_samples]\n",
    "        txt_files = txt_files[:max_samples]\n",
    "\n",
    "    for npy_file, txt_file in zip(npy_files, txt_files):\n",
    "        try:\n",
    "            # 파일 존재 확인\n",
    "            if os.path.exists(npy_file) and os.path.exists(txt_file):\n",
    "                # npy 파일 로드 테스트\n",
    "                test_array = np.load(npy_file)\n",
    "                if len(test_array) > 0:  # 빈 배열 체크\n",
    "                    data[\"audio\"].append(npy_file)\n",
    "                    data[\"text\"].append(load_text(txt_file))\n",
    "                else:\n",
    "                    print(f\"빈 오디오 파일 건너뜀: {npy_file}\")\n",
    "            else:\n",
    "                print(f\"파일 없음: {npy_file} 또는 {txt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"파일 로드 오류: {npy_file}, 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def train_whisper_model():\n",
    "    # 경로 수정\n",
    "    base_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    output_dir = \"whisper_finetuned\"\n",
    "    \n",
    "    # 파일 경로 가져오기\n",
    "    npy_files = get_file_paths_npy(base_directory)\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        print(\"파일을 찾을 수 없습니다. 경로와 파일 패턴을 확인하세요.\")\n",
    "        return None, None\n",
    "    \n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    print(f\"총 {len(npy_files)}개의 오디오 파일과 {len(txt_files)}개의 텍스트 파일이 로드되었습니다.\")\n",
    "    \n",
    "    # 파일 수가 일치하지 않으면 경고\n",
    "    if len(npy_files) != len(txt_files):\n",
    "        print(f\"경고: 오디오 파일 수({len(npy_files)})와 텍스트 파일 수({len(txt_files)})가 일치하지 않습니다.\")\n",
    "        # 일치하는 파일만 사용\n",
    "        if len(npy_files) > len(txt_files):\n",
    "            npy_files = npy_files[:len(txt_files)]\n",
    "        else:\n",
    "            txt_files = txt_files[:len(npy_files)]\n",
    "    \n",
    "    dataset = create_dataset(npy_files, txt_files)\n",
    "\n",
    "    def map_to_array(batch):\n",
    "        arrays = []\n",
    "        rates = []\n",
    "\n",
    "        for audio_path in batch[\"audio\"]:\n",
    "            # npy 파일 로드\n",
    "            audio_array = np.load(audio_path)\n",
    "            # float32로 변환하고 정규화 (필요한 경우)\n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "            sampling_rate = 16000\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(sampling_rate)\n",
    "\n",
    "        batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "        return batch\n",
    "\n",
    "    # 데이터셋에 Audio 형식 적용 (배치 처리)\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "\n",
    "    # 훈련/검증/테스트 세트 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    # Whisper 모델 및 프로세서 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "        # torch_dtype는 자동으로 결정되도록 함\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        # 오디오 처리\n",
    "        audio = batch[\"audio\"]\n",
    "    \n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "    \n",
    "        # NumPy 배열을 PyTorch 텐서로 변환\n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "    \n",
    "        # 타입을 모델과 일치시킴 (fp16)\n",
    "        input_features = input_features.to(model.dtype)\n",
    "    \n",
    "        # 텍스트 처리\n",
    "        labels = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    # 데이터셋 전처리 적용\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    # 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "\n",
    "        # pred_ids가 튜플인 경우 (일반적으로 첫 번째 요소가 예측값)\n",
    "        if isinstance(pred_ids, tuple):\n",
    "            pred_ids = pred_ids[0]\n",
    "\n",
    "        # 이제 pred_ids가 텐서/배열인지 확인하고 3차원인 경우 처리\n",
    "        if hasattr(pred_ids, 'shape') and len(pred_ids.shape) > 2:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "        # 패딩 토큰 무시\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "        # 예측과 레이블을 텍스트로 디코딩\n",
    "        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # WER 계산\n",
    "        try:\n",
    "            import jiwer\n",
    "            wer = jiwer.wer(label_str, pred_str)\n",
    "        except ImportError:\n",
    "            print(\"jiwer 라이브러리가 설치되지 않았습니다. WER 계산을 건너뜁니다.\")\n",
    "            wer = 0.0\n",
    "\n",
    "        return {\"wer\": wer}\n",
    "\n",
    "    # GPU 능력 확인하여 최적 설정 선택\n",
    "    device_capability = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    supports_bf16 = device_capability >= (8, 0)  # Ampere 이상\n",
    "    \n",
    "    print(f\"GPU 능력: {device_capability}, BF16 지원: {supports_bf16}\")\n",
    "    \n",
    "    # 훈련 인자 설정 (GPU에 따라 자동 선택)\n",
    "    if supports_bf16:\n",
    "        # BF16 사용 (Ampere GPU)\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=2,\n",
    "            learning_rate=1e-5,\n",
    "            warmup_steps=500,\n",
    "            max_steps=4000,\n",
    "            gradient_checkpointing=True,\n",
    "            bf16=True,\n",
    "            fp16=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_steps=500,\n",
    "            logging_steps=25,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "    else:\n",
    "        # Float32 사용 (안정적이지만 느림)\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=4,  # 배치 크기 줄임\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=4,  # 그래디언트 누적 늘림\n",
    "            learning_rate=1e-5,\n",
    "            warmup_steps=500,\n",
    "            max_steps=4000,\n",
    "            gradient_checkpointing=True,\n",
    "            bf16=False,\n",
    "            fp16=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_steps=500,\n",
    "            logging_steps=25,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "\n",
    "    # 훈련기 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "\n",
    "    # 모델 훈련\n",
    "    print(\"모델 훈련을 시작합니다...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 저장\n",
    "    print(\"모델을 저장합니다...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    # 테스트 평가\n",
    "    print(\"테스트 평가를 수행합니다...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "        print(f\"테스트 결과: {test_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"테스트 평가 중 오류 발생: {e}\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def transcribe_audio(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NP 파일 로드\n",
    "        audio_array = np.load(audio_file)\n",
    "        \n",
    "        # float32로 변환하고 정규화 (필요한 경우)\n",
    "        if audio_array.dtype in [np.int16, np.int8]:\n",
    "            max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "            audio_array = audio_array.astype(np.float32) / max_value\n",
    "        elif audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "        sr = 16000\n",
    "\n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        # 타입을 모델과 일치시킴\n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # GPU 사용하는 경우\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # 모델을 통한 예측\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                language=\"korean\",  # 한국어 명시\n",
    "                task=\"transcribe\"\n",
    "            )\n",
    "\n",
    "        # 예측된 토큰을 텍스트로 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 오류: {e}\")\n",
    "        return \"음성 인식 실패\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Whisper 한국어 파인튜닝을 시작합니다...\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model, processor = train_whisper_model()\n",
    "    \n",
    "    if model is None or processor is None:\n",
    "        print(\"모델 훈련에 실패했습니다.\")\n",
    "        exit()\n",
    "    \n",
    "    print(\"모델 훈련이 완료되었습니다!\")\n",
    "    \n",
    "    # 테스트할 디렉터리 설정\n",
    "    test_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    \n",
    "    # 테스트 파일 가져오기\n",
    "    npy_files = get_file_paths_npy(test_directory)\n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    \n",
    "    print(f\"테스트할 파일: {len(npy_files)}개\")\n",
    "    \n",
    "    # 테스트할 파일 수 제한 (선택적)\n",
    "    max_test_files = 10\n",
    "    test_npy_files = npy_files[:max_test_files]\n",
    "    test_txt_files = txt_files[:max_test_files]\n",
    "    \n",
    "    print(\"테스트를 시작합니다...\")\n",
    "    \n",
    "    # 모든 파일 테스트\n",
    "    for i in range(len(test_npy_files)):\n",
    "        test_audio = test_npy_files[i]\n",
    "        reference_text = load_text(test_txt_files[i])\n",
    "        \n",
    "        transcription = transcribe_audio(model, processor, test_audio)\n",
    "        \n",
    "        print(f\"파일: {os.path.basename(test_audio)}\")\n",
    "        print(f\"원본 텍스트: {reference_text}\")\n",
    "        print(f\"변환 결과: {transcription}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"모든 작업이 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ca83402-3b4d-4204-bdb6-95416da4ff0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 기본 버전 ===\n",
      "원본 오디오 shape: (129280,)\n",
      "처리된 오디오 shape: (129280,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 입력 features shape: torch.Size([1, 80, 3000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 토큰 수: 58\n",
      "Raw tokens (처음 20개): [50258, 50264, 50359, 50363, 8880, 24915, 36265, 14173, 18016, 4673, 1955, 1098, 48735, 5905, 24902, 12092, 5963, 117, 19041, 4130]\n",
      "\n",
      "=== 간단 버전 ===\n",
      "결과:  오늘 교육대토로는 특별히 청년특집으로 계획되는 거자나 청년들의 경험을 잘 듣고 여기에서 공감할 수 있는 자리가 마련되었으면 합니다.\n",
      "\n",
      "=== 고급 버전 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결과:  오늘 교육대토로는 특별리 청년특집으로 계획되는 거자나요. 청년들의 경업을 잘 듣고 여기에서 공감할 수 있는 자리가 마련되 었으면 합니다\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def transcribe_audio(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    반복 생성 문제 해결 버전\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NP 파일 로드\n",
    "        audio_array = np.load(audio_file)\n",
    "        \n",
    "        # 데이터 차원 확인 및 처리\n",
    "        print(f\"원본 오디오 shape: {audio_array.shape}\")\n",
    "        \n",
    "        # 2D 배열인 경우 1D로 변환 (시간축만 남기기)\n",
    "        if len(audio_array.shape) == 2:\n",
    "            # 보통 (time, features) 또는 (features, time) 형태\n",
    "            if audio_array.shape[0] < audio_array.shape[1]:\n",
    "                audio_array = audio_array.flatten()  # 또는 적절한 축 선택\n",
    "            else:\n",
    "                audio_array = audio_array.flatten()  # 또는 적절한 축 선택\n",
    "        \n",
    "        print(f\"처리된 오디오 shape: {audio_array.shape}\")\n",
    "        \n",
    "        # float32로 변환하고 정규화\n",
    "        if audio_array.dtype in [np.int16, np.int8]:\n",
    "            max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "            audio_array = audio_array.astype(np.float32) / max_value\n",
    "        elif audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "        sr = 16000\n",
    "\n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        # 타입을 모델과 일치시킴\n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # GPU 사용하는 경우\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # 디버깅 정보 출력\n",
    "        print(f\"최종 입력 features shape: {input_features.shape}\")\n",
    "\n",
    "        # 모델을 통한 예측 (경고 해결)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=448,\n",
    "                num_beams=1,              # beam search 끄기\n",
    "                do_sample=False,          # 샘플링 끄기  \n",
    "                early_stopping=False,     # num_beams=1일 때 False로 설정\n",
    "                repetition_penalty=2.0,   # 반복 패널티 강화\n",
    "                no_repeat_ngram_size=3,   # 3-gram 반복 방지\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                language=\"ko\",            # 한국어 명시 (언어 감지 경고 해결)\n",
    "                task=\"transcribe\",        # 명시적으로 전사 작업 지정\n",
    "            )\n",
    "\n",
    "        # 디버깅 정보 출력\n",
    "        print(f\"생성된 토큰 수: {len(predicted_ids[0])}\")\n",
    "        print(f\"Raw tokens (처음 20개): {predicted_ids[0][:20].tolist()}\")\n",
    "\n",
    "        # 예측된 토큰을 텍스트로 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 오류: {e}\")\n",
    "        return \"음성 인식 실패\"\n",
    "\n",
    "\n",
    "def transcribe_audio_simple(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    가장 간단한 버전 - 기본 설정만 사용\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NP 파일 로드\n",
    "        audio_array = np.load(audio_file)\n",
    "        \n",
    "        # float32로 변환하고 정규화\n",
    "        if audio_array.dtype in [np.int16, np.int8]:\n",
    "            max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "            audio_array = audio_array.astype(np.float32) / max_value\n",
    "        elif audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        # 타입 맞추기\n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # GPU 사용\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # 가장 기본적인 생성\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(input_features)\n",
    "\n",
    "        # 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 오류: {e}\")\n",
    "        return \"음성 인식 실패\"\n",
    "\n",
    "\n",
    "def transcribe_audio_advanced(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    고급 설정 버전 - 더 세밀한 제어\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NP 파일 로드\n",
    "        audio_array = np.load(audio_file)\n",
    "        \n",
    "        # float32로 변환하고 정규화\n",
    "        if audio_array.dtype in [np.int16, np.int8]:\n",
    "            max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "            audio_array = audio_array.astype(np.float32) / max_value\n",
    "        elif audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        # 타입 맞추기\n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # GPU 사용\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # 고급 설정으로 생성\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=224,           # 길이 더 제한\n",
    "                min_length=1,             # 최소 길이\n",
    "                num_beams=2,              # beam search 활성화\n",
    "                do_sample=True,           # 샘플링 활성화\n",
    "                temperature=0.7,          # 온도 낮춤\n",
    "                top_p=0.9,               # nucleus sampling\n",
    "                repetition_penalty=1.5,   # 반복 패널티\n",
    "                no_repeat_ngram_size=4,   # 4-gram 반복 방지\n",
    "                early_stopping=True,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 오류: {e}\")\n",
    "        return \"음성 인식 실패\"\n",
    "\n",
    "\n",
    "# 사용 예시\n",
    "if __name__ == \"__main__\":\n",
    "    # 모델 로드 (이미 훈련된 모델)\n",
    "    from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "    \n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"whisper_finetuned\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"whisper_finetuned\")\n",
    "\n",
    "\n",
    "        # NPZ에서 오디오 추출\n",
    "    npz_data = np.load(\"output_npz/example_data.npz\")\n",
    "    audio_data = npz_data['audio_data']  # 원본 오디오\n",
    "    np.save(\"temp_audio.npy\", audio_data)\n",
    "    \n",
    "    # 이제 NPY 파일 사용\n",
    "    test_audio = \"temp_audio.npy\"\n",
    "    \n",
    "    # 3가지 방법으로 테스트\n",
    "    print(\"=== 기본 버전 ===\")\n",
    "    result1 = transcribe_audio(model, processor, test_audio)\n",
    "\n",
    "    '''\n",
    "    # 테스트 파일\n",
    "    test_audio = \"output_npz/example_data.wav\"\n",
    "    # 3가지 방법으로 테스트\n",
    "    print(\"=== 기본 버전 ===\")\n",
    "    result1 = transcribe_audio(model, processor, test_audio)\n",
    "    print(f\"결과: {result1}\")\n",
    "    '''\n",
    "    print(\"\\n=== 간단 버전 ===\")\n",
    "    result2 = transcribe_audio_simple(model, processor, test_audio)\n",
    "    print(f\"결과: {result2}\")\n",
    "    \n",
    "    print(\"\\n=== 고급 버전 ===\")\n",
    "    result3 = transcribe_audio_advanced(model, processor, test_audio)\n",
    "    print(f\"결과: {result3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a85493-6fb5-4084-8a9d-c2e7ba24cf7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mExampleData\u001b[0m/            example.txt       preprocessing_delete_noise.ipynb\n",
      "GPU테스트_유저03.ipynb  example_data.pcm  preprocessing_g2p.ipynb\n",
      "\u001b[01;34mPreprocessData\u001b[0m/         \u001b[00;36mexample_data.wav\u001b[0m  preprocessing_mfcc.ipynb\n",
      "\u001b[01;34mTrainData\u001b[0m/              \u001b[00;36mnews_data.mp3\u001b[0m     \u001b[01;34mwhisper_finetuned\u001b[0m/\n",
      "data.ipynb              news_data.pcm\n",
      "\u001b[01;34mdependency_library\u001b[0m/     \u001b[00;36mnews_data.wav\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c93fc-a9b0-4565-9852-9ccc8cc0ac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-27 03:00:55.360279: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-27 03:00:55.407883: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-27 03:00:56.128477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whisper 한국어 파인튜닝을 시작합니다...\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0001\n",
      "찾은 파일 수: 100\n",
      "찾은 파일 수: 200\n",
      "찾은 파일 수: 300\n",
      "찾은 파일 수: 400\n",
      "찾은 파일 수: 500\n",
      "찾은 파일 수: 600\n",
      "찾은 파일 수: 700\n",
      "찾은 파일 수: 800\n",
      "찾은 파일 수: 900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0002\n",
      "찾은 파일 수: 1000\n",
      "찾은 파일 수: 1100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0003\n",
      "찾은 파일 수: 1200\n",
      "찾은 파일 수: 1300\n",
      "찾은 파일 수: 1400\n",
      "찾은 파일 수: 1500\n",
      "찾은 파일 수: 1600\n",
      "찾은 파일 수: 1700\n",
      "찾은 파일 수: 1800\n",
      "찾은 파일 수: 1900\n",
      "찾은 파일 수: 2000\n",
      "찾은 파일 수: 2100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0004\n",
      "찾은 파일 수: 2200\n",
      "찾은 파일 수: 2300\n",
      "찾은 파일 수: 2400\n",
      "찾은 파일 수: 2500\n",
      "찾은 파일 수: 2600\n",
      "찾은 파일 수: 2700\n",
      "찾은 파일 수: 2800\n",
      "찾은 파일 수: 2900\n",
      "찾은 파일 수: 3000\n",
      "찾은 파일 수: 3100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0005\n",
      "찾은 파일 수: 3200\n",
      "찾은 파일 수: 3300\n",
      "찾은 파일 수: 3400\n",
      "찾은 파일 수: 3500\n",
      "찾은 파일 수: 3600\n",
      "찾은 파일 수: 3700\n",
      "찾은 파일 수: 3800\n",
      "찾은 파일 수: 3900\n",
      "찾은 파일 수: 4000\n",
      "찾은 파일 수: 4100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0006\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0007\n",
      "찾은 파일 수: 4200\n",
      "찾은 파일 수: 4300\n",
      "찾은 파일 수: 4400\n",
      "찾은 파일 수: 4500\n",
      "찾은 파일 수: 4600\n",
      "찾은 파일 수: 4700\n",
      "찾은 파일 수: 4800\n",
      "찾은 파일 수: 4900\n",
      "찾은 파일 수: 5000\n",
      "찾은 파일 수: 5100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0008\n",
      "찾은 파일 수: 5200\n",
      "찾은 파일 수: 5300\n",
      "찾은 파일 수: 5400\n",
      "찾은 파일 수: 5500\n",
      "찾은 파일 수: 5600\n",
      "찾은 파일 수: 5700\n",
      "찾은 파일 수: 5800\n",
      "찾은 파일 수: 5900\n",
      "찾은 파일 수: 6000\n",
      "찾은 파일 수: 6100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0009\n",
      "찾은 파일 수: 6200\n",
      "찾은 파일 수: 6300\n",
      "찾은 파일 수: 6400\n",
      "찾은 파일 수: 6500\n",
      "찾은 파일 수: 6600\n",
      "찾은 파일 수: 6700\n",
      "찾은 파일 수: 6800\n",
      "찾은 파일 수: 6900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0010\n",
      "찾은 파일 수: 7000\n",
      "찾은 파일 수: 7100\n",
      "찾은 파일 수: 7200\n",
      "찾은 파일 수: 7300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0011\n",
      "찾은 파일 수: 7400\n",
      "찾은 파일 수: 7500\n",
      "찾은 파일 수: 7600\n",
      "찾은 파일 수: 7700\n",
      "찾은 파일 수: 7800\n",
      "찾은 파일 수: 7900\n",
      "찾은 파일 수: 8000\n",
      "찾은 파일 수: 8100\n",
      "찾은 파일 수: 8200\n",
      "찾은 파일 수: 8300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0012\n",
      "찾은 파일 수: 8400\n",
      "찾은 파일 수: 8500\n",
      "찾은 파일 수: 8600\n",
      "찾은 파일 수: 8700\n",
      "찾은 파일 수: 8800\n",
      "찾은 파일 수: 8900\n",
      "찾은 파일 수: 9000\n",
      "찾은 파일 수: 9100\n",
      "찾은 파일 수: 9200\n",
      "찾은 파일 수: 9300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0013\n",
      "찾은 파일 수: 9400\n",
      "찾은 파일 수: 9500\n",
      "찾은 파일 수: 9600\n",
      "찾은 파일 수: 9700\n",
      "찾은 파일 수: 9800\n",
      "찾은 파일 수: 9900\n",
      "찾은 파일 수: 10000\n",
      "찾은 파일 수: 10100\n",
      "찾은 파일 수: 10200\n",
      "찾은 파일 수: 10300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0014\n",
      "찾은 파일 수: 10400\n",
      "찾은 파일 수: 10500\n",
      "찾은 파일 수: 10600\n",
      "찾은 파일 수: 10700\n",
      "찾은 파일 수: 10800\n",
      "찾은 파일 수: 10900\n",
      "찾은 파일 수: 11000\n",
      "찾은 파일 수: 11100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0015\n",
      "찾은 파일 수: 11200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0016\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0017\n",
      "찾은 파일 수: 11300\n",
      "찾은 파일 수: 11400\n",
      "찾은 파일 수: 11500\n",
      "찾은 파일 수: 11600\n",
      "찾은 파일 수: 11700\n",
      "찾은 파일 수: 11800\n",
      "찾은 파일 수: 11900\n",
      "찾은 파일 수: 12000\n",
      "찾은 파일 수: 12100\n",
      "찾은 파일 수: 12200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0018\n",
      "찾은 파일 수: 12300\n",
      "찾은 파일 수: 12400\n",
      "찾은 파일 수: 12500\n",
      "찾은 파일 수: 12600\n",
      "찾은 파일 수: 12700\n",
      "총 12714개의 NP 파일을 찾았습니다.\n",
      "경고: 텍스트 파일이 존재하지 않습니다: PreprocessData/KsponSpeech_01/KsponSpeech_0018_g2p/KsponSpeech_017485.txt\n",
      "총 12713개의 텍스트 파일을 매핑했습니다.\n",
      "총 12714개의 오디오 파일과 12713개의 텍스트 파일이 로드되었습니다.\n",
      "경고: 오디오 파일 수(12714)와 텍스트 파일 수(12713)가 일치하지 않습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 2071.84 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 800/800 [02:47<00:00,  4.77 examples/s]\n",
      "Map:  58%|████████████████████████████████████████████████████████████▎                                           | 58/100 [00:12<00:08,  4.71 examples/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict, Audio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "# 5월 27일 (화) 오후 12시 시도\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    # 일반적인 한국어 인코딩 시도\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "\n",
    "def get_file_paths_npy(base_directory):\n",
    "    \"\"\"\n",
    "    디렉토리에서 NP 파일(combined_features.npy)의 경로를 가져옵니다.\n",
    "    \"\"\"\n",
    "    npy_files = []\n",
    "\n",
    "    # 디렉토리 구조에 맞게 폴더 목록 생성\n",
    "    base_folders = []\n",
    "    for i in range(1, 19):\n",
    "        if i > 9:\n",
    "            base_folders.append(f\"KsponSpeech_00{i}\")\n",
    "        else:\n",
    "            base_folders.append(f\"KsponSpeech_000{i}\")\n",
    "\n",
    "    # 모든 폴더 순회\n",
    "    for folder in base_folders:\n",
    "        folder_path = os.path.join(base_directory, folder)\n",
    "        \n",
    "        # 폴더가 존재하는지 확인\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"폴더 검색 중: {folder_path}\")\n",
    "            \n",
    "            # 폴더 내의 파일 검색\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('_combined_features.npy'):\n",
    "                        npy_path = os.path.join(root, file)\n",
    "                        npy_files.append(npy_path)\n",
    "                        if len(npy_files) % 100 == 0:  # 100개마다 로그 출력\n",
    "                            print(f\"찾은 파일 수: {len(npy_files)}\")\n",
    "        else:\n",
    "            print(f\"폴더가 존재하지 않음: {folder_path}\")\n",
    "    \n",
    "    print(f\"총 {len(npy_files)}개의 NP 파일을 찾았습니다.\")\n",
    "    return npy_files\n",
    "\n",
    "\n",
    "def get_file_paths_txt(npy_files):\n",
    "    \"\"\"\n",
    "    NP 파일 경로로부터 해당하는 텍스트 파일 경로를 생성합니다.\n",
    "    \"\"\"\n",
    "    txt_files = []\n",
    "    \n",
    "    for npy_path in npy_files:\n",
    "        # 파일명 추출 (경로와 확장자 제거)\n",
    "        file_name = os.path.basename(npy_path).replace('_combined_features.npy', '')\n",
    "        \n",
    "        # 디렉토리 구조 분석\n",
    "        dir_path = os.path.dirname(npy_path)\n",
    "        parent_dir = os.path.basename(dir_path)\n",
    "        \n",
    "        # 해당 G2P 폴더 경로 생성\n",
    "        g2p_dir = parent_dir + \"_g2p\"\n",
    "        g2p_dir_path = os.path.join(os.path.dirname(dir_path), g2p_dir)\n",
    "        \n",
    "        # 텍스트 파일 경로 생성\n",
    "        txt_path = os.path.join(g2p_dir_path, file_name + \".txt\")\n",
    "        \n",
    "        # 파일이 존재하는지 확인 (선택적)\n",
    "        if os.path.exists(txt_path):\n",
    "            txt_files.append(txt_path)\n",
    "        else:\n",
    "            print(f\"경고: 텍스트 파일이 존재하지 않습니다: {txt_path}\")\n",
    "    \n",
    "    print(f\"총 {len(txt_files)}개의 텍스트 파일을 매핑했습니다.\")\n",
    "    return txt_files\n",
    "\n",
    "\n",
    "def create_dataset(npy_files, txt_files, max_samples=None):\n",
    "    \"\"\"\n",
    "    NP 파일과 텍스트 파일로부터 데이터셋을 생성합니다.\n",
    "    \"\"\"\n",
    "    data = {\"audio\": [], \"text\": []}\n",
    "    \n",
    "    # 최대 샘플 수 제한\n",
    "    if max_samples is not None:\n",
    "        npy_files = npy_files[:max_samples]\n",
    "        txt_files = txt_files[:max_samples]\n",
    "\n",
    "    for npy_file, txt_file in zip(npy_files, txt_files):\n",
    "        try:\n",
    "            # 파일 존재 확인\n",
    "            if os.path.exists(npy_file) and os.path.exists(txt_file):\n",
    "                # npy 파일 로드 테스트\n",
    "                test_array = np.load(npy_file)\n",
    "                if len(test_array) > 0:  # 빈 배열 체크\n",
    "                    data[\"audio\"].append(npy_file)\n",
    "                    data[\"text\"].append(load_text(txt_file))\n",
    "                else:\n",
    "                    print(f\"빈 오디오 파일 건너뜀: {npy_file}\")\n",
    "            else:\n",
    "                print(f\"파일 없음: {npy_file} 또는 {txt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"파일 로드 오류: {npy_file}, 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch\n",
    "\n",
    "\n",
    "def train_whisper_model():\n",
    "    # 경로 수정\n",
    "    base_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    output_dir = \"whisper_finetuned\"\n",
    "    \n",
    "    # 파일 경로 가져오기\n",
    "    npy_files = get_file_paths_npy(base_directory)\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        print(\"파일을 찾을 수 없습니다. 경로와 파일 패턴을 확인하세요.\")\n",
    "        return None, None\n",
    "    \n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    print(f\"총 {len(npy_files)}개의 오디오 파일과 {len(txt_files)}개의 텍스트 파일이 로드되었습니다.\")\n",
    "    \n",
    "    # 파일 수가 일치하지 않으면 경고\n",
    "    if len(npy_files) != len(txt_files):\n",
    "        print(f\"경고: 오디오 파일 수({len(npy_files)})와 텍스트 파일 수({len(txt_files)})가 일치하지 않습니다.\")\n",
    "        # 일치하는 파일만 사용\n",
    "        if len(npy_files) > len(txt_files):\n",
    "            npy_files = npy_files[:len(txt_files)]\n",
    "        else:\n",
    "            txt_files = txt_files[:len(npy_files)]\n",
    "    \n",
    "    # 데이터셋 크기 제한 (테스트용)\n",
    "    max_samples = 1000  # 작은 데이터셋으로 먼저 테스트\n",
    "    dataset = create_dataset(npy_files[:max_samples], txt_files[:max_samples])\n",
    "\n",
    "    def map_to_array(batch):\n",
    "        arrays = []\n",
    "        rates = []\n",
    "\n",
    "        for audio_path in batch[\"audio\"]:\n",
    "            # npy 파일 로드\n",
    "            audio_array = np.load(audio_path)\n",
    "            # float32로 변환하고 정규화 (필요한 경우)\n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "            sampling_rate = 16000\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(sampling_rate)\n",
    "\n",
    "        batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "        return batch\n",
    "\n",
    "    # 데이터셋에 Audio 형식 적용 (배치 처리)\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "\n",
    "    # 훈련/검증/테스트 세트 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    # Whisper 모델 및 프로세서 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "        # torch_dtype는 자동으로 결정되도록 함\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        # 오디오 처리\n",
    "        audio = batch[\"audio\"]\n",
    "    \n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "    \n",
    "        # NumPy 배열을 PyTorch 텐서로 변환\n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "    \n",
    "        # 타입을 모델과 일치시킴 (fp16)\n",
    "        input_features = input_features.to(model.dtype)\n",
    "    \n",
    "        # 텍스트 처리\n",
    "        labels = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "    \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    # 데이터셋 전처리 적용\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    # 데이터 콜레이터 설정\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        pred_ids = pred.predictions\n",
    "        label_ids = pred.label_ids\n",
    "\n",
    "        # pred_ids가 튜플인 경우 (일반적으로 첫 번째 요소가 예측값)\n",
    "        if isinstance(pred_ids, tuple):\n",
    "            pred_ids = pred_ids[0]\n",
    "\n",
    "        # 이제 pred_ids가 텐서/배열인지 확인하고 3차원인 경우 처리\n",
    "        if hasattr(pred_ids, 'shape') and len(pred_ids.shape) > 2:\n",
    "            pred_ids = np.argmax(pred_ids, axis=-1)\n",
    "\n",
    "        # 패딩 토큰 무시\n",
    "        label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "        # 예측과 레이블을 텍스트로 디코딩\n",
    "        pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "        # WER 계산\n",
    "        try:\n",
    "            import jiwer\n",
    "            wer = jiwer.wer(label_str, pred_str)\n",
    "        except ImportError:\n",
    "            print(\"jiwer 라이브러리가 설치되지 않았습니다. WER 계산을 건너뜁니다.\")\n",
    "            wer = 0.0\n",
    "\n",
    "        return {\"wer\": wer}\n",
    "\n",
    "    # GPU 능력 확인하여 최적 설정 선택\n",
    "    device_capability = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    supports_bf16 = device_capability >= (8, 0)  # Ampere 이상\n",
    "    \n",
    "    print(f\"GPU 능력: {device_capability}, BF16 지원: {supports_bf16}\")\n",
    "    \n",
    "    # 훈련 인자 설정 (GPU에 따라 자동 선택)\n",
    "    if supports_bf16:\n",
    "        # BF16 사용 (Ampere GPU)\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=2,\n",
    "            learning_rate=5e-6,  # 학습률을 더 낮춤\n",
    "            warmup_steps=100,   # warmup 단계 줄임\n",
    "            max_steps=1000,     # 총 스텝 줄임 (테스트용)\n",
    "            gradient_checkpointing=True,\n",
    "            bf16=True,\n",
    "            fp16=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_steps=500,\n",
    "            logging_steps=25,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "    else:\n",
    "        # Float32 사용 (안정적이지만 느림)\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=4,  # 배치 크기 줄임\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=4,  # 그래디언트 누적 늘림\n",
    "            learning_rate=1e-5,\n",
    "            warmup_steps=500,\n",
    "            max_steps=4000,\n",
    "            gradient_checkpointing=True,\n",
    "            bf16=False,\n",
    "            fp16=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_steps=500,\n",
    "            logging_steps=25,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "\n",
    "    # 훈련기 설정\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "\n",
    "    # 모델 훈련\n",
    "    print(\"모델 훈련을 시작합니다...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 저장\n",
    "    print(\"모델을 저장합니다...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    # 테스트 평가\n",
    "    print(\"테스트 평가를 수행합니다...\")\n",
    "    try:\n",
    "        test_results = trainer.evaluate(processed_datasets[\"test\"])\n",
    "        print(f\"테스트 결과: {test_results}\")\n",
    "    except Exception as e:\n",
    "        print(f\"테스트 평가 중 오류 발생: {e}\")\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def transcribe_audio(model, processor, audio_file):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # NP 파일 로드\n",
    "        audio_array = np.load(audio_file)\n",
    "        \n",
    "        # float32로 변환하고 정규화 (필요한 경우)\n",
    "        if audio_array.dtype in [np.int16, np.int8]:\n",
    "            max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "            audio_array = audio_array.astype(np.float32) / max_value\n",
    "        elif audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        # 샘플링 레이트는 고정 (KsponSpeech는 16kHz)\n",
    "        sr = 16000\n",
    "\n",
    "        # 특성 추출\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        # 타입을 모델과 일치시킴\n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # GPU 사용하는 경우\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # 모델을 통한 예측 (반복 방지 설정 추가)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                language=\"korean\",  # 한국어 명시\n",
    "                task=\"transcribe\",\n",
    "                max_length=448,  # 최대 길이 제한\n",
    "                num_beams=1,  # beam search 비활성화 (빠른 생성)\n",
    "                do_sample=False,  # 샘플링 비활성화\n",
    "                temperature=1.0,\n",
    "                repetition_penalty=1.2,  # 반복 패널티 추가\n",
    "                no_repeat_ngram_size=3,  # 3-gram 반복 방지\n",
    "                early_stopping=True,\n",
    "                pad_token_id=processor.tokenizer.pad_token_id,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        # 예측된 토큰을 텍스트로 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 오류: {e}\")\n",
    "        return \"음성 인식 실패\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Whisper 한국어 파인튜닝을 시작합니다...\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    model, processor = train_whisper_model()\n",
    "    \n",
    "    if model is None or processor is None:\n",
    "        print(\"모델 훈련에 실패했습니다.\")\n",
    "        exit()\n",
    "    \n",
    "    print(\"모델 훈련이 완료되었습니다!\")\n",
    "    \n",
    "    # 테스트할 디렉터리 설정\n",
    "    test_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    \n",
    "    # 테스트 파일 가져오기\n",
    "    npy_files = get_file_paths_npy(test_directory)\n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    \n",
    "    print(f\"테스트할 파일: {len(npy_files)}개\")\n",
    "    \n",
    "    # 테스트할 파일 수 제한 (선택적)\n",
    "    max_test_files = 10\n",
    "    test_npy_files = npy_files[:max_test_files]\n",
    "    test_txt_files = txt_files[:max_test_files]\n",
    "    \n",
    "    print(\"테스트를 시작합니다...\")\n",
    "    \n",
    "    # 모든 파일 테스트\n",
    "    for i in range(len(test_npy_files)):\n",
    "        test_audio = test_npy_files[i]\n",
    "        reference_text = load_text(test_txt_files[i])\n",
    "        \n",
    "        transcription = transcribe_audio(model, processor, test_audio)\n",
    "        \n",
    "        print(f\"파일: {os.path.basename(test_audio)}\")\n",
    "        print(f\"원본 텍스트: {reference_text}\")\n",
    "        print(f\"변환 결과: {transcription}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"모든 작업이 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42208bcf-e2a4-435d-b39f-177fe3f433f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 디렉토리: /data/TrainData\n",
      "상위 디렉토리: /data\n",
      "상위 디렉토리 내용: ['.Trash-0', 'GPU테스트_유저03.ipynb', 'TrainData', 'data.ipynb', '.ipynb_checkpoints']\n",
      "TrainData 경로 존재 여부: False\n",
      "\n",
      "실제 존재하는 디렉토리 탐색:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "walk() got an unexpected keyword argument 'maxdepth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 실제 존재하는 디렉토리 목록 출력\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m실제 존재하는 디렉토리 탐색:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m root, dirs, files \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwalk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopdown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dir_name \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKspon\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dir_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpeech\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dir_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dir_name:\n",
      "\u001b[0;31mTypeError\u001b[0m: walk() got an unexpected keyword argument 'maxdepth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 현재 디렉토리 확인\n",
    "current_dir = os.getcwd()\n",
    "print(f\"현재 디렉토리: {current_dir}\")\n",
    "\n",
    "# 상위 디렉토리 내용 확인\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "print(f\"상위 디렉토리: {parent_dir}\")\n",
    "if os.path.exists(parent_dir):\n",
    "    print(f\"상위 디렉토리 내용: {os.listdir(parent_dir)[:10]}\")  # 처음 10개만\n",
    "\n",
    "# TrainData 디렉토리가 있는지 확인\n",
    "train_data_dir = os.path.join(current_dir, \"TrainData\")\n",
    "print(f\"TrainData 경로 존재 여부: {os.path.exists(train_data_dir)}\")\n",
    "\n",
    "# 실제 존재하는 디렉토리 목록 출력\n",
    "print(\"\\n실제 존재하는 디렉토리 탐색:\")\n",
    "for root, dirs, files in os.walk(current_dir, topdown=True, maxdepth=3):\n",
    "    for dir_name in dirs:\n",
    "        if \"Kspon\" in dir_name or \"Speech\" in dir_name or \"Train\" in dir_name:\n",
    "            full_path = os.path.join(root, dir_name)\n",
    "            print(f\"발견: {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "92b001b4-8871-47c9-a441-10fe6badd877",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.8/dist-packages (1.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0) (0.30.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0) (2.4.1+cu118)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0) (1.24.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.13.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.26.0) (4.13.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.11.3.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.8.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.8.87)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.3.0.86)\n",
      "Requirement already satisfied: triton==3.0.0; platform_system == \"Linux\" and platform_machine == \"x86_64\" and python_version < \"3.13\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (3.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (10.9.0.58)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48; platform_system == \"Linux\" and platform_machine == \"x86_64\" in /usr/local/lib/python3.8/dist-packages (from torch>=1.10.0->accelerate>=0.26.0) (11.4.1.48)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate>=0.26.0) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/dist-packages (from jinja2->torch>=1.10.0->accelerate>=0.26.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from sympy->torch>=1.10.0->accelerate>=0.26.0) (1.3.0)\n",
      "accelerate                   1.0.1         \n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'\n",
    "!pip list | grep accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f21351be-a61d-4955-bfbd-09b19baaf805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "할당된 메모리: 10.88 GB\n",
      "캐시된 메모리: 16.41 GB\n",
      "최대 할당된 메모리: 18.72 GB\n",
      "GPU 0: NVIDIA GeForce RTX 3090\n",
      "  메모리 할당: 10.88 GB\n",
      "  메모리 예약: 16.41 GB\n"
     ]
    }
   ],
   "source": [
    "# PyTorch로 GPU 메모리 사용량 확인\n",
    "import torch\n",
    "\n",
    "# 현재 할당된 메모리 확인\n",
    "print(f\"할당된 메모리: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"캐시된 메모리: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "print(f\"최대 할당된 메모리: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB\")\n",
    "\n",
    "# GPU별 메모리 통계 자세히 보기 (여러 GPU가 있는 경우)\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "    print(f\"  메모리 할당: {torch.cuda.memory_allocated(i) / 1024**3:.2f} GB\")\n",
    "    print(f\"  메모리 예약: {torch.cuda.memory_reserved(i) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79123090-fb9b-4687-934d-dc9a4af74ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/__init__.py:836: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  return isinstance(obj, torch.Tensor)\n",
      "/tmp/ipykernel_628/1513626050.py:8: FutureWarning: `torch.distributed.reduce_op` is deprecated, please use `torch.distributed.ReduceOp` instead\n",
      "  if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([4]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([4, 50]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([200, 1500, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([4, 1500, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([4, 50, 51865]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([200, 138, 51865]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([204]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([200, 138]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([8, 80, 3000]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([8, 55]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([4, 80, 3000]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([4, 50]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([8, 80, 3000]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([8, 124]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([8, 80, 3000]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([8, 72]) cpu\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([51865, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 80, 3]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768, 3]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([448, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 80, 3]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 80, 3]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768, 3]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768, 3]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([51865, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([51865, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([448, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([448, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072, 768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768, 3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([3072]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([]) cpu\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.Tensor'> torch.Size([768]) cuda:0\n",
      "<class 'torch.nn.parameter.Parameter'> torch.Size([1500, 768]) cuda:0\n",
      "정리 후 할당된 메모리: 8.98 GB\n",
      "정리 후 캐시된 메모리: 12.48 GB\n"
     ]
    }
   ],
   "source": [
    "# 명시적으로 모든 대형 객체 제거\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# 모든 변수 확인 (옵션)\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size(), obj.device)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 각 변수를 개별적으로 삭제\n",
    "try:\n",
    "    del model, processor, trainer, training_args, datasets, processed_datasets\n",
    "    del dataset, train_test_valid, test_valid\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# 강제 GC 및 캐시 정리\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 다시 확인\n",
    "print(f\"정리 후 할당된 메모리: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"정리 후 캐시된 메모리: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d265ec47-3383-49b6-bea6-e7ccb92bf423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== example1.pcm 파일 테스트 ===\n",
      "오류: example1.pcm 파일을 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# PCM 파일 로드 함수\n",
    "def read_pcm(file_path, sr=16000, bit_depth=16):\n",
    "    \"\"\"\n",
    "    PCM 파일을 읽어 numpy 배열로 반환합니다.\n",
    "    \"\"\"\n",
    "    # PCM 파일 읽기 (16비트 또는 8비트 부호 있는 정수 가정)\n",
    "    if bit_depth == 16:\n",
    "        raw_data = np.fromfile(file_path, dtype=np.int16)\n",
    "    elif bit_depth == 8:\n",
    "        raw_data = np.fromfile(file_path, dtype=np.int8)\n",
    "    else:\n",
    "        raw_data = np.fromfile(file_path, dtype=np.int16)  # 기본값\n",
    "\n",
    "    # [-1, 1] 범위로 정규화\n",
    "    max_value = float(2 ** (bit_depth - 1))\n",
    "    normalized_data = raw_data.astype(np.float32) / max_value\n",
    "\n",
    "    return normalized_data, sr\n",
    "\n",
    "# 모델 추론 함수\n",
    "def transcribe_audio(model, processor, audio_file, device):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    # PCM 파일 로드\n",
    "    audio_array, sr = read_pcm(audio_file)\n",
    "\n",
    "    # 특성 추출\n",
    "    input_features = processor.feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # 입력 특성을 모델과 동일한 장치로 이동\n",
    "    input_features = input_features.to(device)\n",
    "\n",
    "    # 모델을 통한 예측\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "\n",
    "    # 예측된 토큰을 텍스트로 디코딩\n",
    "    transcription = processor.tokenizer.batch_decode(\n",
    "        predicted_ids,\n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "def main():\n",
    "    print(\"=== example1.pcm 파일 테스트 ===\")\n",
    "    \n",
    "    # 테스트할 파일 경로\n",
    "    test_file = \"example1.pcm\"  # 이미 찾은 경로 사용\n",
    "    \n",
    "    # 파일 존재 확인\n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"오류: {test_file} 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"테스트 파일: {test_file}\")\n",
    "    \n",
    "    # 텍스트 파일 경로 (있는 경우)\n",
    "    txt_file = test_file.replace('.pcm', '.txt')\n",
    "    if os.path.exists(txt_file):\n",
    "        with open(txt_file, 'r', encoding='utf-8') as f:\n",
    "            reference_text = f.read().strip()\n",
    "        print(f\"참조 텍스트: {reference_text}\")\n",
    "    else:\n",
    "        print(\"참조 텍스트 파일이 없습니다.\")\n",
    "    \n",
    "    # 기기 설정 - CUDA 문제 해결을 위한 방법\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        print(\"CUDA 사용 가능 - GPU 사용\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"CUDA 사용 불가 - CPU 사용\")\n",
    "    \n",
    "    # 기본 모델 로드\n",
    "    print(\"기본 Whisper 모델을 로드합니다...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "    \n",
    "    # 모델을 장치로 명시적 이동\n",
    "    model = model.to(device)\n",
    "    print(f\"모델을 {device}로 이동했습니다.\")\n",
    "    \n",
    "    # 저장된 모델 파일을 찾아 로드 시도\n",
    "    model_dir = \"saved_whisper_model\"\n",
    "    \n",
    "    # PT 파일 찾기\n",
    "    pt_files = []\n",
    "    \n",
    "    # 현재 디렉토리에서 저장된 모델 파일 찾기\n",
    "    if os.path.exists(model_dir):\n",
    "        for root, _, files in os.walk(model_dir):\n",
    "            for file in files:\n",
    "                if file.endswith('.pt'):\n",
    "                    pt_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # 다른 일반적인 위치도 확인\n",
    "    for location in [\".\", \"./saved_model\", \"./models\"]:\n",
    "        if os.path.exists(location):\n",
    "            for file in os.listdir(location):\n",
    "                if file.endswith('.pt'):\n",
    "                    pt_files.append(os.path.join(location, file))\n",
    "    \n",
    "    # pt 파일 찾았다면 로드 시도\n",
    "    if pt_files:\n",
    "        print(f\"{len(pt_files)}개의 PT 파일을 찾았습니다:\")\n",
    "        for i, pt_file in enumerate(pt_files):\n",
    "            print(f\"  {i+1}. {pt_file}\")\n",
    "            \n",
    "        try:\n",
    "            # 첫 번째 파일 로드 시도\n",
    "            model_file = pt_files[0]\n",
    "            print(f\"모델 파일 로드 시도: {model_file}\")\n",
    "            \n",
    "            # PT 파일을 장치에 맞게 로드\n",
    "            state_dict = torch.load(model_file, map_location=device)\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"모델 파일 로드 성공!\")\n",
    "        except Exception as e:\n",
    "            print(f\"모델 파일 로드 실패: {str(e)}\")\n",
    "            print(\"기본 Whisper 모델을 사용합니다.\")\n",
    "    else:\n",
    "        print(\"저장된 PT 파일을 찾을 수 없습니다. 기본 Whisper 모델을 사용합니다.\")\n",
    "    \n",
    "    # 모델을 평가 모드로 설정\n",
    "    model.eval()\n",
    "    \n",
    "    # 음성 인식 수행\n",
    "    print(\"\\n음성 인식 중...\")\n",
    "    try:\n",
    "        # 장치 정보를 transcribe_audio 함수에 전달\n",
    "        transcription = transcribe_audio(model, processor, test_file, device)\n",
    "        print(\"\\n=== 인식 결과 ===\")\n",
    "        print(transcription)\n",
    "        \n",
    "        # 참조 텍스트가 있는 경우 WER 계산\n",
    "        if os.path.exists(txt_file) and 'reference_text' in locals():\n",
    "            try:\n",
    "                import jiwer\n",
    "                wer = jiwer.wer(reference_text, transcription)\n",
    "                print(f\"\\nWER (Word Error Rate): {wer:.4f}\")\n",
    "            except ImportError:\n",
    "                print(\"\\nWER 계산을 위해 jiwer 라이브러리가 필요합니다.\")\n",
    "                print(\"pip install jiwer 명령으로 설치할 수 있습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 중 오류 발생: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()  # 더 자세한 오류 정보 출력\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fac0ae34-4e09-4e71-a0a9-1d544630ecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mExampleData\u001b[0m/            \u001b[01;34mTrainData\u001b[0m/  \u001b[01;34mwhisper_finetuned\u001b[0m/\n",
      "GPU테스트_유저03.ipynb  data.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de4dd876-be9e-4f29-b6ae-58b44532d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/whisper_finetuned\n"
     ]
    }
   ],
   "source": [
    "cd /data/whisper_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e763aaf-8f41-4d7a-b447-616e2897bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /data/whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4cf69ec9-54d5-4591-8dcd-51e5537de084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== example1.pcm 파일 테스트 (한국어 모드) ===\n",
      "테스트 파일: example1.pcm\n",
      "장치: cpu (문제 해결을 위해 CPU만 사용)\n",
      "기본 Whisper 모델을 로드합니다...\n",
      "\n",
      "----- 언어 자동 감지 모드 -----\n",
      "음성 인식 중...\n",
      "PCM 파일 정보:\n",
      "- 샘플 수: 4605952\n",
      "- 샘플링 레이트: 16000Hz\n",
      "- 오디오 길이: 287.87초\n",
      "- 최소값: -0.2336\n",
      "- 최대값: 0.2339\n",
      "- 평균값: 0.0000\n",
      "- 표준편차: 0.0556\n",
      "\n",
      "=== 자동 언어 감지 결과 ===\n",
      " You\n",
      "\n",
      "----- 한국어 강제 모드 -----\n",
      "음성 인식 중...\n",
      "PCM 파일 정보:\n",
      "- 샘플 수: 4605952\n",
      "- 샘플링 레이트: 16000Hz\n",
      "- 오디오 길이: 287.87초\n",
      "- 최소값: -0.2336\n",
      "- 최대값: 0.2339\n",
      "- 평균값: 0.0000\n",
      "- 표준편차: 0.0556\n",
      "언어를 ko로 강제 설정합니다.\n",
      "\n",
      "=== 한국어 강제 모드 결과 ===\n",
      " 이곳은 이곳에서 가장 큰 도움이 되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# PCM 파일 로드 함수\n",
    "def read_pcm(file_path, sr=16000, bit_depth=16):\n",
    "    \"\"\"\n",
    "    PCM 파일을 읽어 numpy 배열로 반환합니다.\n",
    "    \"\"\"\n",
    "    # PCM 파일 읽기 (16비트 또는 8비트 부호 있는 정수 가정)\n",
    "    if bit_depth == 16:\n",
    "        raw_data = np.fromfile(file_path, dtype=np.int16)\n",
    "    elif bit_depth == 8:\n",
    "        raw_data = np.fromfile(file_path, dtype=np.int8)\n",
    "    else:\n",
    "        raw_data = np.fromfile(file_path, dtype=np.int16)  # 기본값\n",
    "\n",
    "    # [-1, 1] 범위로 정규화\n",
    "    max_value = float(2 ** (bit_depth - 1))\n",
    "    normalized_data = raw_data.astype(np.float32) / max_value\n",
    "\n",
    "    # 짧은 음성 샘플 출력\n",
    "    print(f\"PCM 파일 정보:\")\n",
    "    print(f\"- 샘플 수: {len(normalized_data)}\")\n",
    "    print(f\"- 샘플링 레이트: {sr}Hz\")\n",
    "    print(f\"- 오디오 길이: {len(normalized_data)/sr:.2f}초\")\n",
    "    \n",
    "    # 비정상적으로 짧거나 긴 경우 경고\n",
    "    if len(normalized_data)/sr < 0.1:\n",
    "        print(\"경고: 오디오가 너무 짧습니다 (0.1초 미만)\")\n",
    "    elif len(normalized_data)/sr > 100000:\n",
    "        print(\"경고: 오디오가 너무 깁니다 (초과)\")\n",
    "    \n",
    "    # 오디오 신호 통계 출력\n",
    "    print(f\"- 최소값: {normalized_data.min():.4f}\")\n",
    "    print(f\"- 최대값: {normalized_data.max():.4f}\")\n",
    "    print(f\"- 평균값: {normalized_data.mean():.4f}\")\n",
    "    print(f\"- 표준편차: {normalized_data.std():.4f}\")\n",
    "    \n",
    "    # 모두 0이거나 일정한 값인지 확인\n",
    "    if np.all(normalized_data == 0):\n",
    "        print(\"경고: 오디오 데이터가 모두 0입니다!\")\n",
    "    elif np.std(normalized_data) < 0.001:\n",
    "        print(\"경고: 오디오 데이터의 변동이 거의 없습니다!\")\n",
    "\n",
    "    return normalized_data, sr\n",
    "\n",
    "# 모델 추론 함수\n",
    "def transcribe_audio(model, processor, audio_file, device, forced_language=None):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용하여 오디오 파일을 텍스트로 변환합니다.\n",
    "    \"\"\"\n",
    "    # PCM 파일 로드\n",
    "    audio_array, sr = read_pcm(audio_file)\n",
    "\n",
    "    # 특성 추출\n",
    "    input_features = processor.feature_extractor(\n",
    "        audio_array,\n",
    "        sampling_rate=sr,\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    # 입력 특성을 모델과 동일한 장치로 이동\n",
    "    input_features = input_features.to(device)\n",
    "\n",
    "    # 생성 옵션 설정 (한국어 강제 설정)\n",
    "    generation_kwargs = {}\n",
    "    \n",
    "    if forced_language:\n",
    "        # 강제 언어 설정\n",
    "        forced_decoder_ids = processor.get_decoder_prompt_ids(language=forced_language, task=\"transcribe\")\n",
    "        generation_kwargs[\"forced_decoder_ids\"] = forced_decoder_ids\n",
    "        print(f\"언어를 {forced_language}로 강제 설정합니다.\")\n",
    "\n",
    "    # 모델을 통한 예측\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, **generation_kwargs)\n",
    "\n",
    "    # 예측된 토큰을 텍스트로 디코딩\n",
    "    transcription = processor.tokenizer.batch_decode(\n",
    "        predicted_ids, \n",
    "        skip_special_tokens=True\n",
    "    )[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "def main():\n",
    "    print(\"=== example1.pcm 파일 테스트 (한국어 모드) ===\")\n",
    "    \n",
    "    # 테스트할 파일 경로\n",
    "    test_file = \"example1.pcm\"  # 이미 찾은 경로 사용\n",
    "    \n",
    "    # 파일 존재 확인\n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"오류: {test_file} 파일을 찾을 수 없습니다.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"테스트 파일: {test_file}\")\n",
    "    \n",
    "    # CPU 장치 강제 설정 (CUDA 문제 회피)\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"장치: {device} (문제 해결을 위해 CPU만 사용)\")\n",
    "    \n",
    "    # 기본 Whisper 모델 로드\n",
    "    print(\"기본 Whisper 모델을 로드합니다...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(device)\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "    \n",
    "    # 모델 추론\n",
    "    print(\"\\n----- 언어 자동 감지 모드 -----\")\n",
    "    print(\"음성 인식 중...\")\n",
    "    try:\n",
    "        auto_transcription = transcribe_audio(model, processor, test_file, device)\n",
    "        print(\"\\n=== 자동 언어 감지 결과 ===\")\n",
    "        print(auto_transcription)\n",
    "    except Exception as e:\n",
    "        print(f\"자동 언어 모드 오류: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n----- 한국어 강제 모드 -----\")\n",
    "    print(\"음성 인식 중...\")\n",
    "    try:\n",
    "        # 언어를 한국어(ko)로 강제 지정\n",
    "        korean_transcription = transcribe_audio(model, processor, test_file, device, forced_language=\"ko\")\n",
    "        print(\"\\n=== 한국어 강제 모드 결과 ===\")\n",
    "        print(korean_transcription)\n",
    "    except Exception as e:\n",
    "        print(f\"한국어 강제 모드 오류: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9380691-9570-43bf-8e7b-34062db5757c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whisper_finetuned 내용:\n",
      "  example1.pcm\n",
      "  preprocessor_config.json\n",
      "  checkpoint-2000\n",
      "  whisper_finetuned\n",
      "  checkpoint-3500\n",
      "  tokenizer_config.json\n",
      "  training_args.bin\n",
      "  model.safetensors\n",
      "  merges.txt\n",
      "  checkpoint-3000\n",
      "  .ipynb_checkpoints\n",
      "  checkpoint-500\n",
      "  added_tokens.json\n",
      "  checkpoint-1000\n",
      "  special_tokens_map.json\n",
      "  checkpoint-1500\n",
      "  saved_whisper_model.pt\n",
      "  KsponSpeech_128001.pcm\n",
      "  runs\n",
      "  config.json\n",
      "  normalizer.json\n",
      "  checkpoint-2500\n",
      "  vocab.json\n",
      "  generation_config.json\n",
      "  checkpoint-4000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"whisper_finetuned 내용:\")\n",
    "for file in os.listdir(\"whisper_finetuned\"):\n",
    "    print(f\"  {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eff679-3150-4ac3-ad2a-85f181a76c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-05-27 15:15:46.117660: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-27 15:15:46.165587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-27 15:15:46.959438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발음 기준 Whisper 파인튜닝을 시작합니다...\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0001\n",
      "찾은 파일 수: 100\n",
      "찾은 파일 수: 200\n",
      "찾은 파일 수: 300\n",
      "찾은 파일 수: 400\n",
      "찾은 파일 수: 500\n",
      "찾은 파일 수: 600\n",
      "찾은 파일 수: 700\n",
      "찾은 파일 수: 800\n",
      "찾은 파일 수: 900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0002\n",
      "찾은 파일 수: 1000\n",
      "찾은 파일 수: 1100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0003\n",
      "찾은 파일 수: 1200\n",
      "찾은 파일 수: 1300\n",
      "찾은 파일 수: 1400\n",
      "찾은 파일 수: 1500\n",
      "찾은 파일 수: 1600\n",
      "찾은 파일 수: 1700\n",
      "찾은 파일 수: 1800\n",
      "찾은 파일 수: 1900\n",
      "찾은 파일 수: 2000\n",
      "찾은 파일 수: 2100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0004\n",
      "찾은 파일 수: 2200\n",
      "찾은 파일 수: 2300\n",
      "찾은 파일 수: 2400\n",
      "찾은 파일 수: 2500\n",
      "찾은 파일 수: 2600\n",
      "찾은 파일 수: 2700\n",
      "찾은 파일 수: 2800\n",
      "찾은 파일 수: 2900\n",
      "찾은 파일 수: 3000\n",
      "찾은 파일 수: 3100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0005\n",
      "찾은 파일 수: 3200\n",
      "찾은 파일 수: 3300\n",
      "찾은 파일 수: 3400\n",
      "찾은 파일 수: 3500\n",
      "찾은 파일 수: 3600\n",
      "찾은 파일 수: 3700\n",
      "찾은 파일 수: 3800\n",
      "찾은 파일 수: 3900\n",
      "찾은 파일 수: 4000\n",
      "찾은 파일 수: 4100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0006\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0007\n",
      "찾은 파일 수: 4200\n",
      "찾은 파일 수: 4300\n",
      "찾은 파일 수: 4400\n",
      "찾은 파일 수: 4500\n",
      "찾은 파일 수: 4600\n",
      "찾은 파일 수: 4700\n",
      "찾은 파일 수: 4800\n",
      "찾은 파일 수: 4900\n",
      "찾은 파일 수: 5000\n",
      "찾은 파일 수: 5100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0008\n",
      "찾은 파일 수: 5200\n",
      "찾은 파일 수: 5300\n",
      "찾은 파일 수: 5400\n",
      "찾은 파일 수: 5500\n",
      "찾은 파일 수: 5600\n",
      "찾은 파일 수: 5700\n",
      "찾은 파일 수: 5800\n",
      "찾은 파일 수: 5900\n",
      "찾은 파일 수: 6000\n",
      "찾은 파일 수: 6100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0009\n",
      "찾은 파일 수: 6200\n",
      "찾은 파일 수: 6300\n",
      "찾은 파일 수: 6400\n",
      "찾은 파일 수: 6500\n",
      "찾은 파일 수: 6600\n",
      "찾은 파일 수: 6700\n",
      "찾은 파일 수: 6800\n",
      "찾은 파일 수: 6900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0010\n",
      "찾은 파일 수: 7000\n",
      "찾은 파일 수: 7100\n",
      "찾은 파일 수: 7200\n",
      "찾은 파일 수: 7300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0011\n",
      "찾은 파일 수: 7400\n",
      "찾은 파일 수: 7500\n",
      "찾은 파일 수: 7600\n",
      "찾은 파일 수: 7700\n",
      "찾은 파일 수: 7800\n",
      "찾은 파일 수: 7900\n",
      "찾은 파일 수: 8000\n",
      "찾은 파일 수: 8100\n",
      "찾은 파일 수: 8200\n",
      "찾은 파일 수: 8300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0012\n",
      "찾은 파일 수: 8400\n",
      "찾은 파일 수: 8500\n",
      "찾은 파일 수: 8600\n",
      "찾은 파일 수: 8700\n",
      "찾은 파일 수: 8800\n",
      "찾은 파일 수: 8900\n",
      "찾은 파일 수: 9000\n",
      "찾은 파일 수: 9100\n",
      "찾은 파일 수: 9200\n",
      "찾은 파일 수: 9300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0013\n",
      "찾은 파일 수: 9400\n",
      "찾은 파일 수: 9500\n",
      "찾은 파일 수: 9600\n",
      "찾은 파일 수: 9700\n",
      "찾은 파일 수: 9800\n",
      "찾은 파일 수: 9900\n",
      "찾은 파일 수: 10000\n",
      "찾은 파일 수: 10100\n",
      "찾은 파일 수: 10200\n",
      "찾은 파일 수: 10300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0014\n",
      "찾은 파일 수: 10400\n",
      "찾은 파일 수: 10500\n",
      "찾은 파일 수: 10600\n",
      "찾은 파일 수: 10700\n",
      "찾은 파일 수: 10800\n",
      "찾은 파일 수: 10900\n",
      "찾은 파일 수: 11000\n",
      "찾은 파일 수: 11100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0015\n",
      "찾은 파일 수: 11200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0016\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0017\n",
      "찾은 파일 수: 11300\n",
      "찾은 파일 수: 11400\n",
      "찾은 파일 수: 11500\n",
      "찾은 파일 수: 11600\n",
      "찾은 파일 수: 11700\n",
      "찾은 파일 수: 11800\n",
      "찾은 파일 수: 11900\n",
      "찾은 파일 수: 12000\n",
      "찾은 파일 수: 12100\n",
      "찾은 파일 수: 12200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0018\n",
      "찾은 파일 수: 12300\n",
      "찾은 파일 수: 12400\n",
      "찾은 파일 수: 12500\n",
      "찾은 파일 수: 12600\n",
      "찾은 파일 수: 12700\n",
      "총 12714개의 NP 파일을 찾았습니다.\n",
      "경고: 텍스트 파일이 존재하지 않습니다: PreprocessData/KsponSpeech_01/KsponSpeech_0018_g2p/KsponSpeech_017485.txt\n",
      "총 12713개의 텍스트 파일을 매핑했습니다.\n",
      "총 12714개의 오디오 파일과 12713개의 텍스트 파일이 로드되었습니다.\n",
      "경고: 파일 수 불일치\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 12713/12713 [00:05<00:00, 2169.01 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저 어휘 확장 중...\n",
      "기존 어휘 크기: 51865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 10170/10170 [35:41<00:00,  4.75 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1272/1272 [04:29<00:00,  4.73 examples/s]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 1271/1271 [04:27<00:00,  4.75 examples/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 능력: (8, 6), BF16 지원: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29713/3254085371.py:300: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "발음 기준 모델 훈련을 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 7:16:52, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>9.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>7.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.848900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.999500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5.666000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>5.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.164900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>4.945200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.821700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>4.675300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.525500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>4.425300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4.382000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>4.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.155800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>4.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>3.951100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>3.723500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>3.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>3.519200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>3.540900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>3.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>3.477400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>3.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>3.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>3.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>3.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>3.356400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>3.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>3.308100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>3.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>3.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>3.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>3.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>3.271800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>3.238200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>3.312100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.253000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>3.254200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>3.233000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>3.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>3.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>3.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>3.223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>3.203400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>3.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>3.202000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>3.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>3.208300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>3.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>3.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>3.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>3.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>3.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>3.031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>3.093500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>3.102500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>3.036100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>3.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>3.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>3.088100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>3.036500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>3.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>3.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>3.090600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>3.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>3.024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>3.082000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>3.045700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>3.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>3.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>3.003600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>2.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>2.972200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>2.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.925600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025</td>\n",
       "      <td>2.961600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>2.936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2075</td>\n",
       "      <td>2.912700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.937900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2125</td>\n",
       "      <td>2.855100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>2.948700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2175</td>\n",
       "      <td>2.908800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.889100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2225</td>\n",
       "      <td>2.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>2.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2275</td>\n",
       "      <td>2.914300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.946500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2325</td>\n",
       "      <td>2.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>2.919000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>2.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.854000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2425</td>\n",
       "      <td>2.917800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>2.934200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2475</td>\n",
       "      <td>2.927300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.911900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2525</td>\n",
       "      <td>2.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>2.869500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2575</td>\n",
       "      <td>2.780200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.803100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2625</td>\n",
       "      <td>2.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>2.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2675</td>\n",
       "      <td>2.838200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2725</td>\n",
       "      <td>2.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2775</td>\n",
       "      <td>2.806900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.804600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2825</td>\n",
       "      <td>2.840900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>2.828600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2875</td>\n",
       "      <td>2.769000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2925</td>\n",
       "      <td>2.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>2.875500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2975</td>\n",
       "      <td>2.825700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3025</td>\n",
       "      <td>2.826000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>2.801900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3075</td>\n",
       "      <td>2.856600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.849500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3125</td>\n",
       "      <td>2.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>2.776900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3175</td>\n",
       "      <td>2.832000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.742000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3225</td>\n",
       "      <td>2.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>2.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3275</td>\n",
       "      <td>2.740400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.719200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3325</td>\n",
       "      <td>2.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>2.659100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3375</td>\n",
       "      <td>2.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3425</td>\n",
       "      <td>2.704100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>2.797900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3475</td>\n",
       "      <td>2.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.627200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3525</td>\n",
       "      <td>2.720700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>2.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3575</td>\n",
       "      <td>2.747900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.727100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3625</td>\n",
       "      <td>2.682100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>2.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3675</td>\n",
       "      <td>2.699400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.654000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3725</td>\n",
       "      <td>2.757600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>2.735600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3775</td>\n",
       "      <td>2.671400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.764900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3825</td>\n",
       "      <td>2.653200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>2.612300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3875</td>\n",
       "      <td>2.621400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3925</td>\n",
       "      <td>2.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>2.639700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3975</td>\n",
       "      <td>2.635200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.639000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4025</td>\n",
       "      <td>2.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>2.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4075</td>\n",
       "      <td>2.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.647200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4125</td>\n",
       "      <td>2.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>2.604100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4175</td>\n",
       "      <td>2.666800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4225</td>\n",
       "      <td>2.669200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>2.658400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4275</td>\n",
       "      <td>2.642100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4325</td>\n",
       "      <td>2.622800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>2.638000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4375</td>\n",
       "      <td>2.636300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.662900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4425</td>\n",
       "      <td>2.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>2.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4475</td>\n",
       "      <td>2.567600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4525</td>\n",
       "      <td>2.532700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>2.598700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4575</td>\n",
       "      <td>2.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4625</td>\n",
       "      <td>2.581600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>2.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4675</td>\n",
       "      <td>2.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4725</td>\n",
       "      <td>2.565100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>2.548100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4775</td>\n",
       "      <td>2.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.578700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4825</td>\n",
       "      <td>2.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>2.575800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4875</td>\n",
       "      <td>2.579200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4925</td>\n",
       "      <td>2.554300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>2.530300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4975</td>\n",
       "      <td>2.548600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.598600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5025</td>\n",
       "      <td>2.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>2.596200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5075</td>\n",
       "      <td>2.566600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.589100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5125</td>\n",
       "      <td>2.516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>2.526500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5175</td>\n",
       "      <td>2.570800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.527800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5225</td>\n",
       "      <td>2.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>2.512100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5275</td>\n",
       "      <td>2.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5325</td>\n",
       "      <td>2.532100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>2.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5375</td>\n",
       "      <td>2.513400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5425</td>\n",
       "      <td>2.495800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>2.499000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5475</td>\n",
       "      <td>2.531400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5525</td>\n",
       "      <td>2.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>2.533900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5575</td>\n",
       "      <td>2.541000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5625</td>\n",
       "      <td>2.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>2.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5675</td>\n",
       "      <td>2.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.542100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5725</td>\n",
       "      <td>2.492200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>2.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5775</td>\n",
       "      <td>2.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.521700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5825</td>\n",
       "      <td>2.477000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>2.463300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5875</td>\n",
       "      <td>2.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5925</td>\n",
       "      <td>2.503900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>2.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5975</td>\n",
       "      <td>2.474100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.471500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델을 저장합니다...\n",
      "발음 기준 모델 훈련이 완료되었습니다!\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0001\n",
      "찾은 파일 수: 100\n",
      "찾은 파일 수: 200\n",
      "찾은 파일 수: 300\n",
      "찾은 파일 수: 400\n",
      "찾은 파일 수: 500\n",
      "찾은 파일 수: 600\n",
      "찾은 파일 수: 700\n",
      "찾은 파일 수: 800\n",
      "찾은 파일 수: 900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0002\n",
      "찾은 파일 수: 1000\n",
      "찾은 파일 수: 1100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0003\n",
      "찾은 파일 수: 1200\n",
      "찾은 파일 수: 1300\n",
      "찾은 파일 수: 1400\n",
      "찾은 파일 수: 1500\n",
      "찾은 파일 수: 1600\n",
      "찾은 파일 수: 1700\n",
      "찾은 파일 수: 1800\n",
      "찾은 파일 수: 1900\n",
      "찾은 파일 수: 2000\n",
      "찾은 파일 수: 2100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0004\n",
      "찾은 파일 수: 2200\n",
      "찾은 파일 수: 2300\n",
      "찾은 파일 수: 2400\n",
      "찾은 파일 수: 2500\n",
      "찾은 파일 수: 2600\n",
      "찾은 파일 수: 2700\n",
      "찾은 파일 수: 2800\n",
      "찾은 파일 수: 2900\n",
      "찾은 파일 수: 3000\n",
      "찾은 파일 수: 3100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0005\n",
      "찾은 파일 수: 3200\n",
      "찾은 파일 수: 3300\n",
      "찾은 파일 수: 3400\n",
      "찾은 파일 수: 3500\n",
      "찾은 파일 수: 3600\n",
      "찾은 파일 수: 3700\n",
      "찾은 파일 수: 3800\n",
      "찾은 파일 수: 3900\n",
      "찾은 파일 수: 4000\n",
      "찾은 파일 수: 4100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0006\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0007\n",
      "찾은 파일 수: 4200\n",
      "찾은 파일 수: 4300\n",
      "찾은 파일 수: 4400\n",
      "찾은 파일 수: 4500\n",
      "찾은 파일 수: 4600\n",
      "찾은 파일 수: 4700\n",
      "찾은 파일 수: 4800\n",
      "찾은 파일 수: 4900\n",
      "찾은 파일 수: 5000\n",
      "찾은 파일 수: 5100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0008\n",
      "찾은 파일 수: 5200\n",
      "찾은 파일 수: 5300\n",
      "찾은 파일 수: 5400\n",
      "찾은 파일 수: 5500\n",
      "찾은 파일 수: 5600\n",
      "찾은 파일 수: 5700\n",
      "찾은 파일 수: 5800\n",
      "찾은 파일 수: 5900\n",
      "찾은 파일 수: 6000\n",
      "찾은 파일 수: 6100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0009\n",
      "찾은 파일 수: 6200\n",
      "찾은 파일 수: 6300\n",
      "찾은 파일 수: 6400\n",
      "찾은 파일 수: 6500\n",
      "찾은 파일 수: 6600\n",
      "찾은 파일 수: 6700\n",
      "찾은 파일 수: 6800\n",
      "찾은 파일 수: 6900\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0010\n",
      "찾은 파일 수: 7000\n",
      "찾은 파일 수: 7100\n",
      "찾은 파일 수: 7200\n",
      "찾은 파일 수: 7300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0011\n",
      "찾은 파일 수: 7400\n",
      "찾은 파일 수: 7500\n",
      "찾은 파일 수: 7600\n",
      "찾은 파일 수: 7700\n",
      "찾은 파일 수: 7800\n",
      "찾은 파일 수: 7900\n",
      "찾은 파일 수: 8000\n",
      "찾은 파일 수: 8100\n",
      "찾은 파일 수: 8200\n",
      "찾은 파일 수: 8300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0012\n",
      "찾은 파일 수: 8400\n",
      "찾은 파일 수: 8500\n",
      "찾은 파일 수: 8600\n",
      "찾은 파일 수: 8700\n",
      "찾은 파일 수: 8800\n",
      "찾은 파일 수: 8900\n",
      "찾은 파일 수: 9000\n",
      "찾은 파일 수: 9100\n",
      "찾은 파일 수: 9200\n",
      "찾은 파일 수: 9300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0013\n",
      "찾은 파일 수: 9400\n",
      "찾은 파일 수: 9500\n",
      "찾은 파일 수: 9600\n",
      "찾은 파일 수: 9700\n",
      "찾은 파일 수: 9800\n",
      "찾은 파일 수: 9900\n",
      "찾은 파일 수: 10000\n",
      "찾은 파일 수: 10100\n",
      "찾은 파일 수: 10200\n",
      "찾은 파일 수: 10300\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0014\n",
      "찾은 파일 수: 10400\n",
      "찾은 파일 수: 10500\n",
      "찾은 파일 수: 10600\n",
      "찾은 파일 수: 10700\n",
      "찾은 파일 수: 10800\n",
      "찾은 파일 수: 10900\n",
      "찾은 파일 수: 11000\n",
      "찾은 파일 수: 11100\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0015\n",
      "찾은 파일 수: 11200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0016\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0017\n",
      "찾은 파일 수: 11300\n",
      "찾은 파일 수: 11400\n",
      "찾은 파일 수: 11500\n",
      "찾은 파일 수: 11600\n",
      "찾은 파일 수: 11700\n",
      "찾은 파일 수: 11800\n",
      "찾은 파일 수: 11900\n",
      "찾은 파일 수: 12000\n",
      "찾은 파일 수: 12100\n",
      "찾은 파일 수: 12200\n",
      "폴더 검색 중: PreprocessData/KsponSpeech_01/KsponSpeech_0018\n",
      "찾은 파일 수: 12300\n",
      "찾은 파일 수: 12400\n",
      "찾은 파일 수: 12500\n",
      "찾은 파일 수: 12600\n",
      "찾은 파일 수: 12700\n",
      "총 12714개의 NP 파일을 찾았습니다.\n",
      "경고: 텍스트 파일이 존재하지 않습니다: PreprocessData/KsponSpeech_01/KsponSpeech_0018_g2p/KsponSpeech_017485.txt\n",
      "총 12713개의 텍스트 파일을 매핑했습니다.\n",
      "발음 기준 테스트를 시작합니다...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import librosa\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import json\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return \"텍스트 로드 실패\"\n",
    "\n",
    "def get_file_paths_npy(base_directory):\n",
    "    npy_files = []\n",
    "    base_folders = []\n",
    "    for i in range(1, 19):\n",
    "        if i > 9:\n",
    "            base_folders.append(f\"KsponSpeech_00{i}\")\n",
    "        else:\n",
    "            base_folders.append(f\"KsponSpeech_000{i}\")\n",
    "\n",
    "    for folder in base_folders:\n",
    "        folder_path = os.path.join(base_directory, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            print(f\"폴더 검색 중: {folder_path}\")\n",
    "            for root, _, files in os.walk(folder_path):\n",
    "                for file in files:\n",
    "                    if file.endswith('_combined_features.npy'):\n",
    "                        npy_path = os.path.join(root, file)\n",
    "                        npy_files.append(npy_path)\n",
    "                        if len(npy_files) % 100 == 0:\n",
    "                            print(f\"찾은 파일 수: {len(npy_files)}\")\n",
    "        else:\n",
    "            print(f\"폴더가 존재하지 않음: {folder_path}\")\n",
    "    \n",
    "    print(f\"총 {len(npy_files)}개의 NP 파일을 찾았습니다.\")\n",
    "    return npy_files\n",
    "\n",
    "def get_file_paths_txt(npy_files):\n",
    "    txt_files = []\n",
    "    for npy_path in npy_files:\n",
    "        file_name = os.path.basename(npy_path).replace('_combined_features.npy', '')\n",
    "        dir_path = os.path.dirname(npy_path)\n",
    "        parent_dir = os.path.basename(dir_path)\n",
    "        g2p_dir = parent_dir + \"_g2p\"\n",
    "        g2p_dir_path = os.path.join(os.path.dirname(dir_path), g2p_dir)\n",
    "        txt_path = os.path.join(g2p_dir_path, file_name + \".txt\")\n",
    "        \n",
    "        if os.path.exists(txt_path):\n",
    "            txt_files.append(txt_path)\n",
    "        else:\n",
    "            print(f\"경고: 텍스트 파일이 존재하지 않습니다: {txt_path}\")\n",
    "    \n",
    "    print(f\"총 {len(txt_files)}개의 텍스트 파일을 매핑했습니다.\")\n",
    "    return txt_files\n",
    "\n",
    "def create_dataset(npy_files, txt_files, max_samples=None):\n",
    "    data = {\"audio\": [], \"text\": []}\n",
    "    \n",
    "    if max_samples is not None:\n",
    "        npy_files = npy_files[:max_samples]\n",
    "        txt_files = txt_files[:max_samples]\n",
    "\n",
    "    for npy_file, txt_file in zip(npy_files, txt_files):\n",
    "        try:\n",
    "            if os.path.exists(npy_file) and os.path.exists(txt_file):\n",
    "                test_array = np.load(npy_file)\n",
    "                if len(test_array) > 0:\n",
    "                    data[\"audio\"].append(npy_file)\n",
    "                    # 발음 텍스트 그대로 저장 (정규화 X)\n",
    "                    original_text = load_text(txt_file)\n",
    "                    data[\"text\"].append(original_text)\n",
    "                else:\n",
    "                    print(f\"빈 오디오 파일 건너뜀: {npy_file}\")\n",
    "            else:\n",
    "                print(f\"파일 없음: {npy_file} 또는 {txt_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"파일 로드 오류: {npy_file}, 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def extend_tokenizer_vocabulary(processor, texts):\n",
    "    \"\"\"\n",
    "    발음 기준 텍스트에 맞춰 토크나이저 어휘 확장\n",
    "    \"\"\"\n",
    "    print(\"토크나이저 어휘 확장 중...\")\n",
    "    \n",
    "    # 기존 어휘 크기\n",
    "    original_vocab_size = len(processor.tokenizer)\n",
    "    print(f\"기존 어휘 크기: {original_vocab_size}\")\n",
    "    \n",
    "    # 발음 텍스트에서 새로운 토큰 추출\n",
    "    new_tokens = set()\n",
    "    for text in texts[:1000]:  # 샘플링해서 확인\n",
    "        # 발음 기호들 추출\n",
    "        if \"/\" in text:\n",
    "            new_tokens.update([\"/\"])\n",
    "        if \"+\" in text:\n",
    "            new_tokens.update([\"+\"])\n",
    "        # 기타 특수 기호들...\n",
    "    \n",
    "    # 새 토큰 추가\n",
    "    new_tokens = list(new_tokens - set(processor.tokenizer.get_vocab().keys()))\n",
    "    if new_tokens:\n",
    "        print(f\"새로 추가할 토큰: {new_tokens}\")\n",
    "        processor.tokenizer.add_tokens(new_tokens)\n",
    "        print(f\"확장된 어휘 크기: {len(processor.tokenizer)}\")\n",
    "    \n",
    "    return processor\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def train_whisper_pronunciation_model():\n",
    "    base_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    output_dir = \"whisper_pronunciation_finetuned\"\n",
    "    \n",
    "    # 파일 경로 가져오기\n",
    "    npy_files = get_file_paths_npy(base_directory)\n",
    "    \n",
    "    if len(npy_files) == 0:\n",
    "        print(\"파일을 찾을 수 없습니다.\")\n",
    "        return None, None\n",
    "    \n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    print(f\"총 {len(npy_files)}개의 오디오 파일과 {len(txt_files)}개의 텍스트 파일이 로드되었습니다.\")\n",
    "    \n",
    "    # 파일 수 맞추기\n",
    "    if len(npy_files) != len(txt_files):\n",
    "        print(f\"경고: 파일 수 불일치\")\n",
    "        min_len = min(len(npy_files), len(txt_files))\n",
    "        npy_files = npy_files[:min_len]\n",
    "        txt_files = txt_files[:min_len]\n",
    "    \n",
    "    dataset = create_dataset(npy_files, txt_files)\n",
    "\n",
    "    def map_to_array(batch):\n",
    "        arrays = []\n",
    "        rates = []\n",
    "\n",
    "        for audio_path in batch[\"audio\"]:\n",
    "            audio_array = np.load(audio_path)\n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            sampling_rate = 16000\n",
    "            arrays.append(audio_array)\n",
    "            rates.append(sampling_rate)\n",
    "\n",
    "        batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "        return batch\n",
    "\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "\n",
    "    # 데이터셋 분할\n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    # 모델 및 프로세서 로드\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "    \n",
    "    # 🎯 핵심: 토크나이저 어휘 확장\n",
    "    all_texts = [item[\"text\"] for item in dataset]\n",
    "    processor = extend_tokenizer_vocabulary(processor, all_texts)\n",
    "    \n",
    "    # 모델 임베딩 크기 조정 (새 토큰 추가 시)\n",
    "    if len(processor.tokenizer) > model.config.vocab_size:\n",
    "        print(\"모델 임베딩 크기 조정 중...\")\n",
    "        model.resize_token_embeddings(len(processor.tokenizer))\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        \n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        \n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "        \n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # 🎯 핵심: 발음 텍스트 그대로 토크나이징 (정규화 없음)\n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=448,\n",
    "            padding=False\n",
    "        ).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    # GPU 능력 확인\n",
    "    device_capability = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    supports_bf16 = device_capability >= (8, 0)\n",
    "    \n",
    "    print(f\"GPU 능력: {device_capability}, BF16 지원: {supports_bf16}\")\n",
    "    \n",
    "    # 훈련 설정\n",
    "    if supports_bf16:\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=2,\n",
    "            learning_rate=5e-6,  # 더 낮은 학습률 (발음 학습용)\n",
    "            warmup_steps=500,\n",
    "            max_steps=6000,      # 더 많은 스텝\n",
    "            gradient_checkpointing=True,\n",
    "            bf16=True,\n",
    "            fp16=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_steps=500,\n",
    "            logging_steps=25,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "    else:\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=4,\n",
    "            learning_rate=5e-6,\n",
    "            warmup_steps=500,\n",
    "            max_steps=6000,\n",
    "            gradient_checkpointing=True,\n",
    "            bf16=False,\n",
    "            fp16=False,\n",
    "            dataloader_pin_memory=False,\n",
    "            evaluation_strategy=\"no\",\n",
    "            save_steps=500,\n",
    "            logging_steps=25,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            push_to_hub=False,\n",
    "        )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"발음 기준 모델 훈련을 시작합니다...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"모델을 저장합니다...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    # 토크나이저 설정도 별도 저장\n",
    "    with open(os.path.join(output_dir, \"pronunciation_config.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"original_vocab_size\": model.config.vocab_size,\n",
    "            \"extended_vocab_size\": len(processor.tokenizer),\n",
    "            \"training_type\": \"pronunciation_based\"\n",
    "        }, f, indent=2)\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "# 🎯 발음 기준 추론 함수\n",
    "def transcribe_audio_pronunciation(model, processor, audio_file):\n",
    "    try:\n",
    "        audio_array = np.load(audio_file)\n",
    "        \n",
    "        if audio_array.dtype in [np.int16, np.int8]:\n",
    "            max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "            audio_array = audio_array.astype(np.float32) / max_value\n",
    "        elif audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        \n",
    "        sr = 16000\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=sr,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "            model = model.cuda()\n",
    "\n",
    "        # 🎯 발음 기준 생성 (최소한의 제약)\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=448,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                # language, task 파라미터 제거 (발음 출력 유도)\n",
    "            )\n",
    "\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"음성 인식 오류: {e}\")\n",
    "        return \"음성 인식 실패\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"발음 기준 Whisper 파인튜닝을 시작합니다...\")\n",
    "    \n",
    "    model, processor = train_whisper_pronunciation_model()\n",
    "    \n",
    "    if model is None or processor is None:\n",
    "        print(\"모델 훈련에 실패했습니다.\")\n",
    "        exit()\n",
    "    \n",
    "    print(\"발음 기준 모델 훈련이 완료되었습니다!\")\n",
    "    \n",
    "    # 테스트\n",
    "    test_directory = \"PreprocessData/KsponSpeech_01\"\n",
    "    npy_files = get_file_paths_npy(test_directory)\n",
    "    txt_files = get_file_paths_txt(npy_files)\n",
    "    \n",
    "    max_test_files = 5\n",
    "    test_npy_files = npy_files[:max_test_files]\n",
    "    test_txt_files = txt_files[:max_test_files]\n",
    "    \n",
    "    print(\"발음 기준 테스트를 시작합니다...\")\n",
    "    \n",
    "    for i in range(len(test_npy_files)):\n",
    "        test_audio = test_npy_files[i]\n",
    "        reference_text = load_text(test_txt_files[i])\n",
    "        \n",
    "        transcription = transcribe_audio_pronunciation(model, processor, test_audio)\n",
    "        \n",
    "        print(f\"파일: {os.path.basename(test_audio)}\")\n",
    "        print(f\"원본 텍스트: {reference_text}\")\n",
    "        print(f\"발음 변환 결과: {transcription}\")\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    print(\"모든 작업이 완료되었습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04f21d89-e83a-4bff-b07c-03e399fbe23a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 PCM 파일 발음 변환 테스트\n",
      "============================================================\n",
      "🔄 모델 로딩 중: whisper_pronunciation_finetuned\n",
      "🔥 GPU 사용 중\n",
      "✅ 모델 로드 완료!\n",
      "📁 처리할 파일: 000025.wav\n",
      "\n",
      "🎵 PCM 파일 처리 중: 000025.wav\n",
      "   📊 원본 PCM 데이터 길이: 193515 샘플\n",
      "   📊 원본 데이터 범위: [-7886, 32000]\n",
      "   📊 정규화 후 범위: [-0.246, 1.000]\n",
      "   📊 오디오 길이: 12.09초\n",
      "   📊 입력 특징 크기: torch.Size([1, 80, 3000])\n",
      "   🤖 발음 변환 결과:  거기서 아무런 그런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨들 쑤맨들 하죠. 그래서 부모한테 맨들 쑤맨들 하죠. 그래서 부모한테 맨들 쑨들 하죠. 부모니깨�라고. 부모니깨�라고. 부모니깨�맨�라고. 부모니깨�맨�맨�맨�맨�맨�맨�맨�맨��\n",
      "\n",
      "============================================================\n",
      "🎉 PCM -> 발음 변환 성공!\n",
      "📝 최종 결과:  거기서 아무런 그런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨날 애착을 하죠. 그래서 부모한테 맨들 쑤맨들 하죠. 그래서 부모한테 맨들 쑤맨들 하죠. 그래서 부모한테 맨들 쑨들 하죠. 부모니깨�라고. 부모니깨�라고. 부모니깨�맨�라고. 부모니깨�맨�맨�맨�맨�맨�맨�맨�맨��\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def load_trained_model(model_path=\"whisper_pronunciation_finetuned\"):\n",
    "    \"\"\"저장된 발음 모델 불러오기\"\"\"\n",
    "    print(f\"🔄 모델 로딩 중: {model_path}\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ 모델 폴더가 없습니다: {model_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "        processor = WhisperProcessor.from_pretrained(model_path)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            print(\"🔥 GPU 사용 중\")\n",
    "        else:\n",
    "            print(\"💻 CPU 사용 중\")\n",
    "            \n",
    "        print(\"✅ 모델 로드 완료!\")\n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로드 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def transcribe_real_audio(model, processor, audio_file_path):\n",
    "    \"\"\"실제 오디오 파일(.wav, .mp3 등)을 발음으로 변환\"\"\"\n",
    "    \n",
    "    print(f\"🎵 오디오 파일 처리 중: {os.path.basename(audio_file_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. 오디오 파일 로드 (Whisper가 하는 방식 그대로)\n",
    "        audio, sr = librosa.load(audio_file_path, sr=16000)  # 16kHz로 리샘플링\n",
    "        print(f\"   📊 오디오 길이: {len(audio)/sr:.2f}초\")\n",
    "        print(f\"   📊 샘플링 레이트: {sr}Hz\")\n",
    "        print(f\"   📊 오디오 범위: [{audio.min():.3f}, {audio.max():.3f}]\")\n",
    "        \n",
    "        # 2. Whisper 특징 추출 (일반 추론과 동일)\n",
    "        input_features = processor(\n",
    "            audio, \n",
    "            sampling_rate=sr, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        print(f\"   📊 입력 특징 크기: {input_features.shape}\")\n",
    "        \n",
    "        # 3. GPU로 이동\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "        \n",
    "        # 4. 발음 기준 추론\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=448,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                # language=\"ko\"  # 필요시 추가\n",
    "            )\n",
    "        \n",
    "        # 5. 텍스트 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        print(f\"   🤖 발음 변환 결과: {transcription}\")\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 처리 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_with_sample_audio():\n",
    "    \"\"\"샘플 오디오로 테스트\"\"\"\n",
    "    \n",
    "    print(\"🎯 실제 오디오 파일 -> 발음 변환 테스트\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 모델 로드\n",
    "    model, processor = load_trained_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 테스트할 오디오 파일들 찾기\n",
    "    audio_extensions = ['.wav', '.mp3', '.flac', '.m4a']\n",
    "    test_files = []\n",
    "    \n",
    "    # 현재 디렉토리와 하위 폴더에서 오디오 파일 찾기\n",
    "    for root, dirs, files in os.walk('.'):\n",
    "        for file in files:\n",
    "            if any(file.lower().endswith(ext) for ext in audio_extensions):\n",
    "                audio_path = os.path.join(root, file)\n",
    "                test_files.append(audio_path)\n",
    "                if len(test_files) >= 5:  # 최대 5개\n",
    "                    break\n",
    "        if len(test_files) >= 5:\n",
    "            break\n",
    "    \n",
    "    if not test_files:\n",
    "        print(\"❌ 테스트할 오디오 파일을 찾을 수 없습니다!\")\n",
    "        print(\"다음 형식의 파일을 현재 폴더에 넣어주세요: .wav, .mp3, .flac, .m4a\")\n",
    "        \n",
    "        # 간단한 테스트 음성 생성 (선택사항)\n",
    "        create_test_audio = input(\"테스트용 음성 파일을 생성하시겠습니까? (y/n): \")\n",
    "        if create_test_audio.lower() == 'y':\n",
    "            create_sample_audio()\n",
    "            return\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    print(f\"📁 {len(test_files)}개의 오디오 파일 발견\")\n",
    "    print()\n",
    "    \n",
    "    # 각 파일에 대해 추론 실행\n",
    "    for i, audio_file in enumerate(test_files, 1):\n",
    "        print(f\"🔍 테스트 {i}/{len(test_files)}\")\n",
    "        result = transcribe_real_audio(model, processor, audio_file)\n",
    "        \n",
    "        if result:\n",
    "            print(\"   ✅ 성공!\")\n",
    "        else:\n",
    "            print(\"   ❌ 실패!\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "def create_sample_audio():\n",
    "    \"\"\"테스트용 샘플 오디오 생성\"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        \n",
    "        print(\"🔊 테스트용 음성 파일 생성 중...\")\n",
    "        \n",
    "        # 간단한 사인파 생성 (440Hz, 2초)\n",
    "        sr = 16000\n",
    "        duration = 2.0\n",
    "        t = np.linspace(0, duration, int(sr * duration))\n",
    "        \n",
    "        # 440Hz 톤 (A4 음)\n",
    "        audio = 0.3 * np.sin(2 * np.pi * 440 * t)\n",
    "        \n",
    "        # 파일 저장\n",
    "        output_file = \"test_audio.wav\"\n",
    "        sf.write(output_file, audio, sr)\n",
    "        \n",
    "        print(f\"✅ 테스트 파일 생성 완료: {output_file}\")\n",
    "        print(\"이제 다시 테스트를 실행해보세요!\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ soundfile 라이브러리가 필요합니다: pip install soundfile\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 테스트 파일 생성 실패: {e}\")\n",
    "\n",
    "def test_single_file():\n",
    "    \"\"\"단일 파일 직접 지정해서 테스트\"\"\"\n",
    "    \n",
    "    print(\"📂 단일 파일 테스트\")\n",
    "    \n",
    "    # 파일 경로 직접 입력\n",
    "    audio_file = input(\"오디오 파일 경로를 입력하세요: \").strip()\n",
    "    \n",
    "    if not os.path.exists(audio_file):\n",
    "        print(f\"❌ 파일이 존재하지 않습니다: {audio_file}\")\n",
    "        return\n",
    "    \n",
    "    # 모델 로드\n",
    "    model, processor = load_trained_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # 추론 실행\n",
    "    print()\n",
    "    result = transcribe_real_audio(model, processor, audio_file)\n",
    "    \n",
    "    if result:\n",
    "        print(\"\\n🎉 추론 성공!\")\n",
    "        print(f\"결과: {result}\")\n",
    "    else:\n",
    "        print(\"\\n❌ 추론 실패!\")\n",
    "\n",
    "def load_pcm_file(pcm_file_path, sample_rate=16000):\n",
    "    \"\"\"PCM 파일을 로드하여 numpy 배열로 변환 (사용자 전처리 방식 적용)\"\"\"\n",
    "    try:\n",
    "        # PCM 파일을 16비트 정수로 읽기\n",
    "        with open(pcm_file_path, 'rb') as f:\n",
    "            pcm_data = f.read()\n",
    "        \n",
    "        # bytes를 numpy 배열로 변환 (16비트 정수 가정)\n",
    "        np_pcm = np.frombuffer(pcm_data, dtype=np.int16)\n",
    "        \n",
    "        print(f\"   📊 원본 PCM 데이터 길이: {len(np_pcm)} 샘플\")\n",
    "        print(f\"   📊 원본 데이터 범위: [{np_pcm.min()}, {np_pcm.max()}]\")\n",
    "        \n",
    "        # 🎯 사용자 전처리 방식 적용\n",
    "        normalized = np_pcm / np.max(np.abs(np_pcm))\n",
    "        \n",
    "        print(f\"   📊 정규화 후 범위: [{normalized.min():.3f}, {normalized.max():.3f}]\")\n",
    "        print(f\"   📊 오디오 길이: {len(normalized)/sample_rate:.2f}초\")\n",
    "        \n",
    "        return normalized.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ PCM 파일 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_pcm_file(model, processor, pcm_file_path):\n",
    "    \"\"\"PCM 파일을 발음으로 변환\"\"\"\n",
    "    \n",
    "    print(f\"🎵 PCM 파일 처리 중: {os.path.basename(pcm_file_path)}\")\n",
    "    \n",
    "    try:\n",
    "        # 1. PCM 파일 로드\n",
    "        audio = load_pcm_file(pcm_file_path)\n",
    "        if audio is None:\n",
    "            return None\n",
    "        \n",
    "        # 2. Whisper 특징 추출\n",
    "        input_features = processor(\n",
    "            audio, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        print(f\"   📊 입력 특징 크기: {input_features.shape}\")\n",
    "        \n",
    "        # 3. GPU로 이동\n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "        \n",
    "        # 4. 발음 기준 추론\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=448,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        \n",
    "        # 5. 텍스트 디코딩\n",
    "        transcription = processor.tokenizer.batch_decode(\n",
    "            predicted_ids, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        print(f\"   🤖 발음 변환 결과: {transcription}\")\n",
    "        return transcription\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ 처리 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 함수 - example_data.pcm 파일 처리\"\"\"\n",
    "    \n",
    "    print(\"🎤 PCM 파일 발음 변환 테스트\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # PCM 파일 경로\n",
    "    pcm_file = \"000025.wav\"\n",
    "    \n",
    "    # 파일 존재 확인\n",
    "    if not os.path.exists(pcm_file):\n",
    "        print(f\"❌ PCM 파일이 없습니다: {pcm_file}\")\n",
    "        print(\"example_data.pcm 파일을 현재 폴더에 넣어주세요!\")\n",
    "        return\n",
    "    \n",
    "    # 모델 로드\n",
    "    model, processor = load_trained_model()\n",
    "    if model is None:\n",
    "        print(\"❌ 모델 로드 실패!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"📁 처리할 파일: {pcm_file}\")\n",
    "    print()\n",
    "    \n",
    "    # PCM 파일 추론 실행\n",
    "    result = transcribe_pcm_file(model, processor, pcm_file)\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    if result:\n",
    "        print(\"🎉 PCM -> 발음 변환 성공!\")\n",
    "        print(f\"📝 최종 결과: {result}\")\n",
    "    else:\n",
    "        print(\"❌ PCM -> 발음 변환 실패!\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89a02ddb-31bc-459c-9e21-2b4e5d4e50b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Whisper 모델 .pt 변환 도구\n",
      "================================================================================\n",
      "🔄 Transformers 모델을 .pt 파일로 변환\n",
      "============================================================\n",
      "📋 필요한 파일들 확인:\n",
      "   ✅ config.json\n",
      "   ✅ model.safetensors\n",
      "\n",
      "🔄 모델 로딩 중...\n",
      "✅ 모델 로드 완료!\n",
      "📊 모델 정보:\n",
      "   - 모델 타입: WhisperForConditionalGeneration\n",
      "   - 어휘 크기: 51865\n",
      "   - 모델 크기: 241,734,912 parameters\n",
      "\n",
      "💾 .pt 파일로 저장 중: whisper_pronunciation_model.pt\n",
      "✅ 저장 완료!\n",
      "📊 파일 크기: 926.0 MB\n",
      "\n",
      "🔍 저장된 파일 검증 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29713/3837886129.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loaded_data = torch.load(output_pt_file, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 검증 완료! 포함된 데이터:\n",
      "   - model_state_dict\n",
      "   - model_config\n",
      "   - tokenizer\n",
      "   - feature_extractor\n",
      "   - processor_config\n",
      "   - training_info\n",
      "\n",
      "================================================================================\n",
      "🧪 변환된 .pt 모델 테스트\n",
      "============================================================\n",
      "🔄 .pt 파일에서 모델 로딩: whisper_pronunciation_model.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29713/3837886129.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pt_file_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ .pt 파일에서 모델 로드 완료!\n",
      "🔥 GPU로 모델 이동 완료\n",
      "🔍 더미 데이터로 추론 테스트...\n",
      "✅ 추론 테스트 성공!\n",
      "📝 더미 결과:  (static)\n",
      "\n",
      "================================================================================\n",
      "🎉 변환 완료!\n",
      "📁 사용할 파일: whisper_pronunciation_model.pt\n",
      "💡 이제 로컬에서 이 .pt 파일을 사용하세요!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "def convert_transformers_to_pt():\n",
    "    \"\"\"Transformers 모델을 .pt 파일로 변환\"\"\"\n",
    "    \n",
    "    print(\"🔄 Transformers 모델을 .pt 파일로 변환\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 입력 폴더 (Transformers 모델이 저장된 곳)\n",
    "    input_model_path = \"whisper_pronunciation_finetuned\"\n",
    "    \n",
    "    # 출력 파일명\n",
    "    output_pt_file = \"whisper_pronunciation_model.pt\"\n",
    "    \n",
    "    # 모델 폴더 확인\n",
    "    if not os.path.exists(input_model_path):\n",
    "        print(f\"❌ 모델 폴더가 없습니다: {input_model_path}\")\n",
    "        return\n",
    "    \n",
    "    # 필요한 파일들 확인\n",
    "    required_files = [\"config.json\"]\n",
    "    model_file = None\n",
    "    \n",
    "    # 모델 파일 찾기 (safetensors 또는 bin)\n",
    "    if os.path.exists(os.path.join(input_model_path, \"model.safetensors\")):\n",
    "        model_file = \"model.safetensors\"\n",
    "    elif os.path.exists(os.path.join(input_model_path, \"pytorch_model.bin\")):\n",
    "        model_file = \"pytorch_model.bin\"\n",
    "    \n",
    "    if model_file:\n",
    "        required_files.append(model_file)\n",
    "    \n",
    "    print(f\"📋 필요한 파일들 확인:\")\n",
    "    for file in required_files:\n",
    "        file_path = os.path.join(input_model_path, file)\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"   ✅ {file}\")\n",
    "        else:\n",
    "            print(f\"   ❌ {file} - 없음\")\n",
    "            return\n",
    "    \n",
    "    try:\n",
    "        # 1. 모델과 프로세서 로드\n",
    "        print(\"\\n🔄 모델 로딩 중...\")\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(input_model_path)\n",
    "        processor = WhisperProcessor.from_pretrained(input_model_path)\n",
    "        \n",
    "        print(\"✅ 모델 로드 완료!\")\n",
    "        \n",
    "        # 2. 모델 정보 출력\n",
    "        print(f\"📊 모델 정보:\")\n",
    "        print(f\"   - 모델 타입: {type(model).__name__}\")\n",
    "        print(f\"   - 어휘 크기: {len(processor.tokenizer)}\")\n",
    "        print(f\"   - 모델 크기: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "        \n",
    "        # 3. .pt 파일로 저장할 데이터 준비\n",
    "        save_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'model_config': model.config.to_dict(),\n",
    "            'tokenizer': processor.tokenizer,\n",
    "            'feature_extractor': processor.feature_extractor,\n",
    "            'processor_config': {\n",
    "                'tokenizer_class': processor.tokenizer.__class__.__name__,\n",
    "                'feature_extractor_class': processor.feature_extractor.__class__.__name__,\n",
    "            },\n",
    "            'training_info': {\n",
    "                'model_type': 'whisper_pronunciation_finetuned',\n",
    "                'base_model': 'openai/whisper-small',\n",
    "                'task': 'pronunciation_transcription'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 4. .pt 파일로 저장\n",
    "        print(f\"\\n💾 .pt 파일로 저장 중: {output_pt_file}\")\n",
    "        torch.save(save_data, output_pt_file)\n",
    "        \n",
    "        # 5. 저장된 파일 크기 확인\n",
    "        file_size = os.path.getsize(output_pt_file) / (1024 * 1024)  # MB\n",
    "        print(f\"✅ 저장 완료!\")\n",
    "        print(f\"📊 파일 크기: {file_size:.1f} MB\")\n",
    "        \n",
    "        # 6. 검증을 위한 로드 테스트\n",
    "        print(f\"\\n🔍 저장된 파일 검증 중...\")\n",
    "        loaded_data = torch.load(output_pt_file, map_location='cpu')\n",
    "        \n",
    "        print(\"✅ 검증 완료! 포함된 데이터:\")\n",
    "        for key in loaded_data.keys():\n",
    "            print(f\"   - {key}\")\n",
    "        \n",
    "        return output_pt_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 변환 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_pt_model(pt_file_path):\n",
    "    \"\"\"저장된 .pt 파일에서 모델 로드\"\"\"\n",
    "    \n",
    "    print(f\"🔄 .pt 파일에서 모델 로딩: {pt_file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # .pt 파일 로드\n",
    "        checkpoint = torch.load(pt_file_path, map_location='cpu')\n",
    "        \n",
    "        # 모델 생성\n",
    "        from transformers import WhisperConfig\n",
    "        config = WhisperConfig.from_dict(checkpoint['model_config'])\n",
    "        model = WhisperForConditionalGeneration(config)\n",
    "        \n",
    "        # 모델 가중치 로드\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # 프로세서 복원\n",
    "        tokenizer = checkpoint['tokenizer']\n",
    "        feature_extractor = checkpoint['feature_extractor']\n",
    "        processor = WhisperProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n",
    "        \n",
    "        print(\"✅ .pt 파일에서 모델 로드 완료!\")\n",
    "        \n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ .pt 파일 로드 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def test_pt_model():\n",
    "    \"\"\"변환된 .pt 모델 테스트\"\"\"\n",
    "    \n",
    "    print(\"🧪 변환된 .pt 모델 테스트\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    pt_file = \"whisper_pronunciation_model.pt\"\n",
    "    \n",
    "    if not os.path.exists(pt_file):\n",
    "        print(f\"❌ .pt 파일이 없습니다: {pt_file}\")\n",
    "        return\n",
    "    \n",
    "    # .pt 파일에서 모델 로드\n",
    "    model, processor = load_pt_model(pt_file)\n",
    "    \n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # GPU 사용 가능하면 이동\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        print(\"🔥 GPU로 모델 이동 완료\")\n",
    "    \n",
    "    # 간단한 추론 테스트 (더미 데이터)\n",
    "    print(\"🔍 더미 데이터로 추론 테스트...\")\n",
    "    \n",
    "    try:\n",
    "        # 더미 오디오 입력 생성\n",
    "        import numpy as np\n",
    "        dummy_audio = np.random.randn(16000).astype(np.float32)  # 1초 더미 오디오\n",
    "        \n",
    "        # 특징 추출\n",
    "        input_features = processor(\n",
    "            dummy_audio,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "        \n",
    "        # 추론\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_features,\n",
    "                max_length=50,\n",
    "                num_beams=1,\n",
    "                do_sample=False\n",
    "            )\n",
    "        \n",
    "        result = processor.tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        print(f\"✅ 추론 테스트 성공!\")\n",
    "        print(f\"📝 더미 결과: {result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 추론 테스트 실패: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 함수\"\"\"\n",
    "    \n",
    "    print(\"🔧 Whisper 모델 .pt 변환 도구\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Transformers 모델을 .pt로 변환\n",
    "    pt_file = convert_transformers_to_pt()\n",
    "    \n",
    "    if pt_file:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        \n",
    "        # 2. 변환된 모델 테스트\n",
    "        test_pt_model()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🎉 변환 완료!\")\n",
    "        print(f\"📁 사용할 파일: {pt_file}\")\n",
    "        print(\"💡 이제 로컬에서 이 .pt 파일을 사용하세요!\")\n",
    "    else:\n",
    "        print(\"❌ 변환 실패!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96b8c581-276b-4292-a155-8b468a94f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 디코딩 파라미터 최적화 테스트\n",
      "================================================================================\n",
      "🔄 모델 로딩 중: whisper_pronunciation_finetuned\n",
      "🔥 GPU 사용 중\n",
      "✅ 모델 로드 완료!\n",
      "🎵 PCM 파일 처리 중: 000025.wav\n",
      "   📊 PCM 데이터 길이: 193515 샘플\n",
      "   📊 오디오 길이: 12.09초\n",
      "   📊 오디오 범위: [-0.246, 1.000]\n",
      "\n",
      "🧪 다양한 디코딩 전략 테스트\n",
      "================================================================================\n",
      "\n",
      "1️⃣ 반복 방지 강화\n",
      "------------------------------------------------------------\n",
      "📝 결과 (길이: 166):\n",
      "    거기서 아무런 그런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠. 그래서 그 아들께서 막 끄러지면서 아예 학꽈를 하는 게 아니라, 비행기구가 뭐 이런 거에 대해서 좀 더 조아하는 건지 모르겓떠라고 생닙니다라고 하더라고 해서 그때 한 번쯔메 안 핸는데\n",
      "🔍 분석:\n",
      "   - 반복 패턴: ✅ 없음\n",
      "   - 깨진 문자: ✅ 없음\n",
      "\n",
      "2️⃣ 빔 서치 + 반복 방지\n",
      "------------------------------------------------------------\n",
      "📝 결과 (길이: 60):\n",
      "    거기서 아무런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠.\n",
      "🔍 분석:\n",
      "   - 반복 패턴: ✅ 없음\n",
      "   - 깨진 문자: ✅ 없음\n",
      "\n",
      "3️⃣ 샘플링 기반\n",
      "------------------------------------------------------------\n",
      "📝 결과 (길이: 61):\n",
      "    거기서 아무런 게이블이 업써 관찰만 수천 명의 아이대를 쭉 키는 걸 보니까 부모한테 맨처매네 애착을 하지요?\n",
      "🔍 분석:\n",
      "   - 반복 패턴: ✅ 없음\n",
      "   - 깨진 문자: ✅ 없음\n",
      "\n",
      "4️⃣ 보수적 설정\n",
      "------------------------------------------------------------\n",
      "📝 결과 (길이: 154):\n",
      "    거기서 아무런 그런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠. 그래서 그 아들께서 막 끄러지면서 아예 학꽈를 하는 게 아니라, 비행기구가 뭐 이런 거에 대해서 좀 더 조아하는 건지 모르겓떠라고 생닙니다라고 하더라고 해서 그때\n",
      "🔍 분석:\n",
      "   - 반복 패턴: ✅ 없음\n",
      "   - 깨진 문자: ✅ 없음\n",
      "\n",
      "5️⃣ 길이 제한 + 조기 종료\n",
      "------------------------------------------------------------\n",
      "📝 결과 (길이: 60):\n",
      "    거기서 아무런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠.\n",
      "🔍 분석:\n",
      "   - 반복 패턴: ✅ 없음\n",
      "   - 깨진 문자: ✅ 없음\n",
      "\n",
      "🏆 최적 결과 선정\n",
      "================================================================================\n",
      "📊 결과 순위:\n",
      "   1. 1️⃣ 반복 방지 강화 (점수: 0)\n",
      "      📝  거기서 아무런 그런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠. 그래서 그 아들께서 막 끄러지면서 아예 학꽈를 하는 게 아니라, ...\n",
      "\n",
      "   2. 2️⃣ 빔 서치 + 반복 방지 (점수: 0)\n",
      "      📝  거기서 아무런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠....\n",
      "\n",
      "   3. 3️⃣ 샘플링 기반 (점수: 0)\n",
      "      📝  거기서 아무런 게이블이 업써 관찰만 수천 명의 아이대를 쭉 키는 걸 보니까 부모한테 맨처매네 애착을 하지요?...\n",
      "\n",
      "🥇 최고 결과: 1️⃣ 반복 방지 강화\n",
      "📝 전체 결과:  거기서 아무런 그런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠. 그래서 그 아들께서 막 끄러지면서 아예 학꽈를 하는 게 아니라, 비행기구가 뭐 이런 거에 대해서 좀 더 조아하는 건지 모르겓떠라고 생닙니다라고 하더라고 해서 그때 한 번쯔메 안 핸는데\n",
      "\n",
      "================================================================================\n",
      "🎉 테스트 완료! 최적 디코딩 설정을 찾았습니다.\n",
      "💡 이 설정을 실제 추론에서 사용하세요!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def load_trained_model(model_path=\"whisper_pronunciation_finetuned\"):\n",
    "    \"\"\"저장된 발음 모델 불러오기\"\"\"\n",
    "    print(f\"🔄 모델 로딩 중: {model_path}\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ 모델 폴더가 없습니다: {model_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "        processor = WhisperProcessor.from_pretrained(model_path)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            print(\"🔥 GPU 사용 중\")\n",
    "        else:\n",
    "            print(\"💻 CPU 사용 중\")\n",
    "            \n",
    "        print(\"✅ 모델 로드 완료!\")\n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로드 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_pcm_file(pcm_file_path, sample_rate=16000):\n",
    "    \"\"\"PCM 파일을 로드하여 numpy 배열로 변환\"\"\"\n",
    "    try:\n",
    "        with open(pcm_file_path, 'rb') as f:\n",
    "            pcm_data = f.read()\n",
    "        \n",
    "        np_pcm = np.frombuffer(pcm_data, dtype=np.int16)\n",
    "        \n",
    "        # 사용자 전처리 방식 적용\n",
    "        normalized = np_pcm / np.max(np.abs(np_pcm))\n",
    "        \n",
    "        print(f\"   📊 PCM 데이터 길이: {len(normalized)} 샘플\")\n",
    "        print(f\"   📊 오디오 길이: {len(normalized)/sample_rate:.2f}초\")\n",
    "        print(f\"   📊 오디오 범위: [{normalized.min():.3f}, {normalized.max():.3f}]\")\n",
    "        \n",
    "        return normalized.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ PCM 파일 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_different_decoding_strategies(model, processor, audio):\n",
    "    \"\"\"다양한 디코딩 전략 테스트\"\"\"\n",
    "    \n",
    "    print(\"🧪 다양한 디코딩 전략 테스트\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 입력 특징 추출\n",
    "    input_features = processor(\n",
    "        audio, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        input_features = input_features.cuda()\n",
    "    \n",
    "    strategies = [\n",
    "        {\n",
    "            \"name\": \"1️⃣ 반복 방지 강화\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 200,  # 길이 줄임\n",
    "                \"num_beams\": 1,\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 2.0,  # 🎯 반복 페널티 강화\n",
    "                \"no_repeat_ngram_size\": 3,  # 🎯 3-gram 반복 방지\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"2️⃣ 빔 서치 + 반복 방지\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 150,\n",
    "                \"num_beams\": 3,  # 🎯 빔 서치 사용\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 1.5,\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"early_stopping\": True,\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"3️⃣ 샘플링 기반\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 150,\n",
    "                \"num_beams\": 1,\n",
    "                \"do_sample\": True,  # 🎯 샘플링 활성화\n",
    "                \"temperature\": 0.7,  # 🎯 창의성 조절\n",
    "                \"top_p\": 0.9,       # 🎯 상위 90% 토큰만 사용\n",
    "                \"repetition_penalty\": 1.3,\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"4️⃣ 보수적 설정\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 100,  # 🎯 매우 짧게\n",
    "                \"num_beams\": 1,\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 3.0,  # 🎯 매우 강한 반복 페널티\n",
    "                \"no_repeat_ngram_size\": 2,\n",
    "                \"pad_token_id\": processor.tokenizer.eos_token_id,\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"5️⃣ 길이 제한 + 조기 종료\",\n",
    "            \"params\": {\n",
    "                \"max_new_tokens\": 50,  # 🎯 새 토큰 수 제한\n",
    "                \"num_beams\": 2,\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 2.5,\n",
    "                \"no_repeat_ngram_size\": 4,\n",
    "                \"early_stopping\": True,\n",
    "                \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for strategy in strategies:\n",
    "        print(f\"\\n{strategy['name']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(\n",
    "                    input_features,\n",
    "                    **strategy['params']\n",
    "                )\n",
    "            \n",
    "            transcription = processor.tokenizer.batch_decode(\n",
    "                predicted_ids, \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            # 결과 분석\n",
    "            result_length = len(transcription)\n",
    "            has_repetition = check_repetition(transcription)\n",
    "            has_broken_chars = check_broken_chars(transcription)\n",
    "            \n",
    "            print(f\"📝 결과 (길이: {result_length}):\")\n",
    "            print(f\"   {transcription[:200]}{'...' if len(transcription) > 200 else ''}\")\n",
    "            print(f\"🔍 분석:\")\n",
    "            print(f\"   - 반복 패턴: {'❌ 있음' if has_repetition else '✅ 없음'}\")\n",
    "            print(f\"   - 깨진 문자: {'❌ 있음' if has_broken_chars else '✅ 없음'}\")\n",
    "            \n",
    "            results.append({\n",
    "                'strategy': strategy['name'],\n",
    "                'result': transcription,\n",
    "                'length': result_length,\n",
    "                'has_repetition': has_repetition,\n",
    "                'has_broken_chars': has_broken_chars\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 전략 실패: {e}\")\n",
    "            results.append({\n",
    "                'strategy': strategy['name'],\n",
    "                'result': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_repetition(text):\n",
    "    \"\"\"반복 패턴 감지\"\"\"\n",
    "    words = text.split()\n",
    "    if len(words) < 6:\n",
    "        return False\n",
    "    \n",
    "    # 연속된 3개 단어가 반복되는지 확인\n",
    "    for i in range(len(words) - 5):\n",
    "        if words[i:i+3] == words[i+3:i+6]:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def check_broken_chars(text):\n",
    "    \"\"\"깨진 문자 감지\"\"\"\n",
    "    broken_patterns = ['�', '맨�', '깨�', '뀨�']\n",
    "    return any(pattern in text for pattern in broken_patterns)\n",
    "\n",
    "def find_best_result(results):\n",
    "    \"\"\"최적 결과 찾기\"\"\"\n",
    "    \n",
    "    print(\"\\n🏆 최적 결과 선정\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    valid_results = [r for r in results if r.get('result') and not r.get('error')]\n",
    "    \n",
    "    if not valid_results:\n",
    "        print(\"❌ 유효한 결과가 없습니다.\")\n",
    "        return None\n",
    "    \n",
    "    # 점수 계산 (낮을수록 좋음)\n",
    "    for result in valid_results:\n",
    "        score = 0\n",
    "        \n",
    "        # 반복 패턴 페널티\n",
    "        if result['has_repetition']:\n",
    "            score += 100\n",
    "            \n",
    "        # 깨진 문자 페널티  \n",
    "        if result['has_broken_chars']:\n",
    "            score += 50\n",
    "            \n",
    "        # 너무 길거나 짧으면 페널티\n",
    "        if result['length'] > 300:\n",
    "            score += 30\n",
    "        elif result['length'] < 10:\n",
    "            score += 20\n",
    "            \n",
    "        result['score'] = score\n",
    "    \n",
    "    # 점수순 정렬\n",
    "    valid_results.sort(key=lambda x: x['score'])\n",
    "    \n",
    "    print(\"📊 결과 순위:\")\n",
    "    for i, result in enumerate(valid_results[:3]):\n",
    "        print(f\"   {i+1}. {result['strategy']} (점수: {result['score']})\")\n",
    "        print(f\"      📝 {result['result'][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    best = valid_results[0]\n",
    "    print(f\"🥇 최고 결과: {best['strategy']}\")\n",
    "    print(f\"📝 전체 결과: {best['result']}\")\n",
    "    \n",
    "    return best\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 함수 - 디코딩 파라미터 최적화\"\"\"\n",
    "    \n",
    "    print(\"🎯 디코딩 파라미터 최적화 테스트\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # PCM 파일 경로\n",
    "    pcm_file = \"000025.wav\"\n",
    "    \n",
    "    # 파일 존재 확인\n",
    "    if not os.path.exists(pcm_file):\n",
    "        print(f\"❌ PCM 파일이 없습니다: {pcm_file}\")\n",
    "        return\n",
    "    \n",
    "    # 모델 로드\n",
    "    model, processor = load_trained_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # PCM 파일 로드\n",
    "    print(f\"🎵 PCM 파일 처리 중: {pcm_file}\")\n",
    "    audio = load_pcm_file(pcm_file)\n",
    "    if audio is None:\n",
    "        return\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # 다양한 디코딩 전략 테스트\n",
    "    results = test_different_decoding_strategies(model, processor, audio)\n",
    "    \n",
    "    # 최적 결과 선정\n",
    "    best_result = find_best_result(results)\n",
    "    \n",
    "    if best_result:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"🎉 테스트 완료! 최적 디코딩 설정을 찾았습니다.\")\n",
    "        print(\"💡 이 설정을 실제 추론에서 사용하세요!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68ee05df-b2d3-416e-8aea-4e10a0a387ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🐛 음성 앞부분 누락 문제 디버깅\n",
      "================================================================================\n",
      "🔍 PCM 파일 상세 분석\n",
      "============================================================\n",
      "📊 원본 PCM 정보:\n",
      "   - 파일 크기: 387030 bytes\n",
      "   - 샘플 수: 193515\n",
      "   - 길이: 12.09초\n",
      "   - 데이터 범위: [-7886, 32000]\n",
      "\n",
      "📊 첫 5초 분석:\n",
      "   - 샘플 수: 80000\n",
      "   - 평균 절댓값: 1062.83\n",
      "   - 최대 절댓값: 32000\n",
      "   - 0이 아닌 샘플 비율: 97.9%\n",
      "\n",
      "📊 정규화 후 첫 5초:\n",
      "   - 범위: [-0.246, 1.000]\n",
      "   - 평균 절댓값: 0.033\n",
      "\n",
      "📊 시간대별 음성 활동 (1초 단위):\n",
      "    0초: 에너지=0.0355 🔊\n",
      "    1초: 에너지=0.0400 🔊\n",
      "    2초: 에너지=0.0283 🔊\n",
      "    3초: 에너지=0.0299 🔊\n",
      "    4초: 에너지=0.0325 🔊\n",
      "    5초: 에너지=0.0355 🔊\n",
      "    6초: 에너지=0.0318 🔊\n",
      "    7초: 에너지=0.0284 🔊\n",
      "    8초: 에너지=0.0326 🔊\n",
      "    9초: 에너지=0.0256 🔊\n",
      "\n",
      "🔍 Whisper 입력 특성 분석\n",
      "============================================================\n",
      "📊 입력 특성 정보:\n",
      "   - 특성 크기: torch.Size([1, 80, 3000])\n",
      "   - 데이터 타입: torch.float32\n",
      "   - 값 범위: [-0.860, 1.140]\n",
      "   청크 1 (0~30초): torch.Size([1, 80, 3000])\n",
      "\n",
      "🔍 기본 Whisper vs 파인튜닝 모델 비교\n",
      "============================================================\n",
      "🔄 기본 Whisper 테스트...\n",
      "✅ 기본 Whisper 결과:\n",
      "   📝  아이들 크는 걸 이렇게 계절 연구소라고 또 있거든요. 미국에. 거기서 아무런 그런 개입을 없이 그냥 쭉 관찰만 수천 명의 아이들을 쭉 크는 걸 보니까 부모한테 맨 처음에는 애착을 하죠.\n",
      "\n",
      "🔄 파인튜닝 모델 테스트...\n",
      "✅ 파인튜닝 모델 결과:\n",
      "   📝  거기서 아무런 그런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠. 그래서 그 아들께서 막 끄러지면서 아예 학꽈를 하는 게 아니라, 비행기구가 뭐 이런 거에 대해서 좀 더 조아하는 건지 모르겓떠라고 생닙니다라고 하더라고 해서 그때 한 번쯔메 안 핸는데\n",
      "\n",
      "🔍 5초 구간별 분석\n",
      "============================================================\n",
      "\n",
      "📍 구간 0~5초:\n",
      "   에너지: 0.0332\n",
      "   결과:  미국에.\n",
      "\n",
      "📍 구간 5~10초:\n",
      "   에너지: 0.0308\n",
      "   결과:  그런 개입을 없이 그냥 쭉 관찰만 수천 명의 아이들을 쭉 키는 걸 보니까\n",
      "\n",
      "📍 구간 10~15초:\n",
      "   에너지: 0.0145\n",
      "   결과:  부모한테 맨 처음에는 애착을 하죠?\n",
      "\n",
      "================================================================================\n",
      "📋 디버깅 결과 요약\n",
      "================================================================================\n",
      "🔍 확인해야 할 포인트:\n",
      "1. 첫 5초 에너지가 충분한가?\n",
      "2. 기본 Whisper도 앞부분을 놓치는가?\n",
      "3. 구간별 분석에서 첫 구간 결과는?\n",
      "4. 파인튜닝 모델과 기본 모델의 차이는?\n",
      "\n",
      "💡 기본 Whisper 첫 부분 인식: ✅\n",
      "💡 파인튜닝 모델 첫 부분 인식: ❌\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def load_and_analyze_pcm(pcm_file_path):\n",
    "    \"\"\"PCM 파일 로드하고 상세 분석\"\"\"\n",
    "    \n",
    "    print(\"🔍 PCM 파일 상세 분석\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # PCM 파일 로드\n",
    "        with open(pcm_file_path, 'rb') as f:\n",
    "            pcm_data = f.read()\n",
    "        \n",
    "        # numpy 배열로 변환\n",
    "        np_pcm = np.frombuffer(pcm_data, dtype=np.int16)\n",
    "        \n",
    "        print(f\"📊 원본 PCM 정보:\")\n",
    "        print(f\"   - 파일 크기: {len(pcm_data)} bytes\")\n",
    "        print(f\"   - 샘플 수: {len(np_pcm)}\")\n",
    "        print(f\"   - 길이: {len(np_pcm)/16000:.2f}초\")\n",
    "        print(f\"   - 데이터 범위: [{np_pcm.min()}, {np_pcm.max()}]\")\n",
    "        \n",
    "        # 앞부분 분석 (첫 5초)\n",
    "        first_5_sec = np_pcm[:16000*5]  # 첫 5초\n",
    "        print(f\"\\n📊 첫 5초 분석:\")\n",
    "        print(f\"   - 샘플 수: {len(first_5_sec)}\")\n",
    "        print(f\"   - 평균 절댓값: {np.mean(np.abs(first_5_sec)):.2f}\")\n",
    "        print(f\"   - 최대 절댓값: {np.max(np.abs(first_5_sec))}\")\n",
    "        print(f\"   - 0이 아닌 샘플 비율: {np.count_nonzero(first_5_sec)/len(first_5_sec)*100:.1f}%\")\n",
    "        \n",
    "        # 정규화 (사용자 방식)\n",
    "        normalized = np_pcm / np.max(np.abs(np_pcm))\n",
    "        first_5_sec_norm = normalized[:16000*5]\n",
    "        \n",
    "        print(f\"\\n📊 정규화 후 첫 5초:\")\n",
    "        print(f\"   - 범위: [{first_5_sec_norm.min():.3f}, {first_5_sec_norm.max():.3f}]\")\n",
    "        print(f\"   - 평균 절댓값: {np.mean(np.abs(first_5_sec_norm)):.3f}\")\n",
    "        \n",
    "        # 시간대별 음성 활동 분석\n",
    "        print(f\"\\n📊 시간대별 음성 활동 (1초 단위):\")\n",
    "        for i in range(min(10, int(len(normalized)/16000))):  # 첫 10초\n",
    "            segment = normalized[i*16000:(i+1)*16000]\n",
    "            energy = np.mean(np.abs(segment))\n",
    "            print(f\"   {i:2d}초: 에너지={energy:.4f} {'🔊' if energy > 0.01 else '🔇'}\")\n",
    "        \n",
    "        return normalized, np_pcm\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ PCM 분석 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def test_whisper_input_features(audio_array):\n",
    "    \"\"\"Whisper 입력 특성 분석\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 Whisper 입력 특성 분석\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # 기본 Whisper processor로 특성 추출\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "        \n",
    "        # 전체 오디오\n",
    "        input_features = processor.feature_extractor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        print(f\"📊 입력 특성 정보:\")\n",
    "        print(f\"   - 특성 크기: {input_features.shape}\")\n",
    "        print(f\"   - 데이터 타입: {input_features.dtype}\")\n",
    "        print(f\"   - 값 범위: [{input_features.min():.3f}, {input_features.max():.3f}]\")\n",
    "        \n",
    "        # 30초씩 분할 처리 (Whisper 기본 방식)\n",
    "        chunk_size = 30 * 16000  # 30초\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(audio_array), chunk_size):\n",
    "            chunk = audio_array[i:i+chunk_size]\n",
    "            if len(chunk) < chunk_size:\n",
    "                # 마지막 청크는 패딩\n",
    "                padded_chunk = np.zeros(chunk_size, dtype=np.float32)\n",
    "                padded_chunk[:len(chunk)] = chunk\n",
    "                chunk = padded_chunk\n",
    "            \n",
    "            chunk_features = processor.feature_extractor(\n",
    "                chunk,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features\n",
    "            \n",
    "            chunks.append(chunk_features)\n",
    "            \n",
    "            # 첫 3개 청크만 분석\n",
    "            if len(chunks) <= 3:\n",
    "                print(f\"   청크 {len(chunks)} ({i//16000}~{(i+chunk_size)//16000}초): {chunk_features.shape}\")\n",
    "        \n",
    "        return input_features, chunks\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 입력 특성 분석 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def compare_basic_vs_finetuned(audio_array):\n",
    "    \"\"\"기본 Whisper vs 파인튜닝 모델 비교\"\"\"\n",
    "    \n",
    "    print(\"\\n🔍 기본 Whisper vs 파인튜닝 모델 비교\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. 기본 Whisper 테스트\n",
    "    try:\n",
    "        print(\"🔄 기본 Whisper 테스트...\")\n",
    "        basic_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "        basic_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            basic_model = basic_model.cuda()\n",
    "        \n",
    "        input_features = basic_processor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_ids = basic_model.generate(\n",
    "                input_features,\n",
    "                language=\"ko\",\n",
    "                task=\"transcribe\",\n",
    "                max_length=500  # 길게 설정\n",
    "            )\n",
    "        \n",
    "        basic_result = basic_processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        results['basic'] = basic_result\n",
    "        print(f\"✅ 기본 Whisper 결과:\")\n",
    "        print(f\"   📝 {basic_result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 기본 Whisper 테스트 실패: {e}\")\n",
    "        results['basic'] = None\n",
    "    \n",
    "    # 2. 파인튜닝 모델 테스트 (있다면)\n",
    "    finetuned_path = \"whisper_pronunciation_finetuned\"\n",
    "    if os.path.exists(finetuned_path):\n",
    "        try:\n",
    "            print(f\"\\n🔄 파인튜닝 모델 테스트...\")\n",
    "            finetuned_model = WhisperForConditionalGeneration.from_pretrained(finetuned_path)\n",
    "            finetuned_processor = WhisperProcessor.from_pretrained(finetuned_path)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                finetuned_model = finetuned_model.cuda()\n",
    "            \n",
    "            input_features = finetuned_processor(\n",
    "                audio_array,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                input_features = input_features.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predicted_ids = finetuned_model.generate(\n",
    "                    input_features,\n",
    "                    max_length=500,\n",
    "                    repetition_penalty=2.0,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "            \n",
    "            finetuned_result = finetuned_processor.tokenizer.batch_decode(\n",
    "                predicted_ids,\n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            results['finetuned'] = finetuned_result\n",
    "            print(f\"✅ 파인튜닝 모델 결과:\")\n",
    "            print(f\"   📝 {finetuned_result}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 파인튜닝 모델 테스트 실패: {e}\")\n",
    "            results['finetuned'] = None\n",
    "    else:\n",
    "        print(f\"\\n⚠️  파인튜닝 모델 없음: {finetuned_path}\")\n",
    "        results['finetuned'] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_audio_segments(audio_array, segment_length=5):\n",
    "    \"\"\"오디오를 구간별로 나누어 분석\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 {segment_length}초 구간별 분석\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        basic_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "        basic_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            basic_model = basic_model.cuda()\n",
    "        \n",
    "        segment_size = segment_length * 16000\n",
    "        \n",
    "        for i in range(0, min(len(audio_array), 30*16000), segment_size):  # 최대 30초까지만\n",
    "            segment = audio_array[i:i+segment_size]\n",
    "            \n",
    "            # 너무 짧은 구간은 패딩\n",
    "            if len(segment) < segment_size:\n",
    "                padded_segment = np.zeros(segment_size, dtype=np.float32)\n",
    "                padded_segment[:len(segment)] = segment\n",
    "                segment = padded_segment\n",
    "            \n",
    "            print(f\"\\n📍 구간 {i//16000}~{(i+segment_size)//16000}초:\")\n",
    "            print(f\"   에너지: {np.mean(np.abs(segment)):.4f}\")\n",
    "            \n",
    "            try:\n",
    "                input_features = basic_processor(\n",
    "                    segment,\n",
    "                    sampling_rate=16000,\n",
    "                    return_tensors=\"pt\"\n",
    "                ).input_features\n",
    "                \n",
    "                if torch.cuda.is_available():\n",
    "                    input_features = input_features.cuda()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    predicted_ids = basic_model.generate(\n",
    "                        input_features,\n",
    "                        language=\"ko\",\n",
    "                        task=\"transcribe\",\n",
    "                        max_length=100\n",
    "                    )\n",
    "                \n",
    "                result = basic_processor.tokenizer.batch_decode(\n",
    "                    predicted_ids,\n",
    "                    skip_special_tokens=True\n",
    "                )[0]\n",
    "                \n",
    "                print(f\"   결과: {result}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   ❌ 처리 실패: {e}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 구간별 분석 실패: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 디버깅 함수\"\"\"\n",
    "    \n",
    "    print(\"🐛 음성 앞부분 누락 문제 디버깅\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    pcm_file = \"000025.wav\"\n",
    "    \n",
    "    if not os.path.exists(pcm_file):\n",
    "        print(f\"❌ PCM 파일이 없습니다: {pcm_file}\")\n",
    "        return\n",
    "    \n",
    "    # 1. PCM 파일 상세 분석\n",
    "    audio_array, raw_pcm = load_and_analyze_pcm(pcm_file)\n",
    "    \n",
    "    if audio_array is None:\n",
    "        return\n",
    "    \n",
    "    # 2. Whisper 입력 특성 분석\n",
    "    input_features, chunks = test_whisper_input_features(audio_array)\n",
    "    \n",
    "    # 3. 기본 vs 파인튜닝 모델 비교\n",
    "    comparison_results = compare_basic_vs_finetuned(audio_array)\n",
    "    \n",
    "    # 4. 구간별 분석\n",
    "    analyze_audio_segments(audio_array, segment_length=5)\n",
    "    \n",
    "    # 5. 결과 요약\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📋 디버깅 결과 요약\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"🔍 확인해야 할 포인트:\")\n",
    "    print(\"1. 첫 5초 에너지가 충분한가?\")\n",
    "    print(\"2. 기본 Whisper도 앞부분을 놓치는가?\")\n",
    "    print(\"3. 구간별 분석에서 첫 구간 결과는?\")\n",
    "    print(\"4. 파인튜닝 모델과 기본 모델의 차이는?\")\n",
    "    \n",
    "    if comparison_results.get('basic'):\n",
    "        basic_starts_with_target = \"아이들\" in comparison_results['basic'][:20]\n",
    "        print(f\"\\n💡 기본 Whisper 첫 부분 인식: {'✅' if basic_starts_with_target else '❌'}\")\n",
    "    \n",
    "    if comparison_results.get('finetuned'):\n",
    "        finetuned_starts_with_target = \"아이들\" in comparison_results['finetuned'][:20]\n",
    "        print(f\"💡 파인튜닝 모델 첫 부분 인식: {'✅' if finetuned_starts_with_target else '❌'}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ece781f-f934-4210-8303-a1ccd1ba79b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 파인튜닝 모델 앞부분 누락 해결\n",
      "================================================================================\n",
      "🔥 GPU 사용 중\n",
      "✅ 파인튜닝 모델 로드 완료!\n",
      "\n",
      "🎵 PCM 파일 로드 중...\n",
      "   🔇 앞부분에 0.1초 무음 추가\n",
      "   🔊 앞부분 3.0초 볼륨 1.5배 증폭\n",
      "   📊 전처리 후 길이: 12.19초\n",
      "🧪 앞부분 누락 해결을 위한 디코딩 테스트\n",
      "================================================================================\n",
      "\n",
      "1️⃣ 언어/태스크 강제 지정\n",
      "------------------------------------------------------------\n",
      "📝 결과:  아이들 크는 거를 계젤 연구소라고 또 있거든요. 거기서 아무런 개입이 업쌔 그냥 쭉 관찰만 수천 명의 아이드를 주는 걸 보니까 부모한테 맨처 면허에 착을 하죠?\n",
      "🔍 앞부분 인식: ✅\n",
      "\n",
      "2️⃣ 시작 토큰 강제 지정\n",
      "------------------------------------------------------------\n",
      "📝 결과:  아이들 크는 거를 계젤 연구소라고 또 있거든요. 거기서 아무런 개입이 업쌔 그냥 쭉 관찰만 수천 명의 아이드를 주는 걸 보니까 부모한테 맨처 면허에 착을 하죠?\n",
      "🔍 앞부분 인식: ✅\n",
      "\n",
      "3️⃣ 프롬프트 기반 디코딩\n",
      "------------------------------------------------------------\n",
      "📝 결과: 아이들는 거를 계젤 연구소라고 또 있거든요 미국에 거기서 아무런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠\n",
      "🔍 앞부분 인식: ✅\n",
      "\n",
      "4️⃣ 빔 서치 + 길이 페널티\n",
      "------------------------------------------------------------\n",
      "📝 결과:  아이들 크는 거를 이렇게 계젤 연구소라고 또 있거든요 미국에 거기서 아무런 그런 뭐 개입이 업씨 그냥 쭉 관찰만 수천 명의 아이드를 쭉 키는 걸 보니까 부모한테 맨처 면허 애착을 하죠.\n",
      "🔍 앞부분 인식: ✅\n",
      "\n",
      "5️⃣ 온도 샘플링\n",
      "------------------------------------------------------------\n",
      "📝 결과:  아이들 크는 걸 계젤 연구소라고 또 있거든요. 거기서 아무런 개입이 업쌔 그냥 쭉 관찰만 수천 명의 아이드를 키우고 부모한테 맨처 면허 애착을 하죠?\n",
      "🔍 앞부분 인식: ✅\n",
      "\n",
      "🔄 청크 단위 처리 테스트\n",
      "============================================================\n",
      "📊 첫 청크: 10.0초\n",
      "📝 첫 청크 결과:  아이들 크는 거를 계젤 연구소라고 또 있거든요. 미국에 거기서 아무런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까.\n",
      "📊 나머지 청크: 2.2초\n",
      "📝 나머지 청크 결과:  부모한테 맨처맨에 애착을 하죠?\n",
      "📝 전체 결과:  아이들 크는 거를 계젤 연구소라고 또 있거든요. 미국에 거기서 아무런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까.  부모한테 맨처맨에 애착을 하죠?\n",
      "\n",
      "================================================================================\n",
      "📊 해결 방법 평가\n",
      "================================================================================\n",
      "✅ 앞부분 인식 성공한 방법들:\n",
      "   - 1️⃣ 언어/태스크 강제 지정\n",
      "     결과:  아이들 크는 거를 계젤 연구소라고 또 있거든요. 거기서 아무런 개입이 업쌔 그냥 쭉 관찰만 수천 명의 아이드를 주는 걸 보니까 부모한테 맨처 면허에 착을 하죠?...\n",
      "   - 2️⃣ 시작 토큰 강제 지정\n",
      "     결과:  아이들 크는 거를 계젤 연구소라고 또 있거든요. 거기서 아무런 개입이 업쌔 그냥 쭉 관찰만 수천 명의 아이드를 주는 걸 보니까 부모한테 맨처 면허에 착을 하죠?...\n",
      "   - 3️⃣ 프롬프트 기반 디코딩\n",
      "     결과: 아이들는 거를 계젤 연구소라고 또 있거든요 미국에 거기서 아무런 개입이 업써 관찰만 수천 명의 아이드를 쭉 크는 걸 보니까 부모한테 맨처 면허 애착을 하죠...\n",
      "   - 4️⃣ 빔 서치 + 길이 페널티\n",
      "     결과:  아이들 크는 거를 이렇게 계젤 연구소라고 또 있거든요 미국에 거기서 아무런 그런 뭐 개입이 업씨 그냥 쭉 관찰만 수천 명의 아이드를 쭉 키는 걸 보니까 부모한테 맨처 면허 애착을...\n",
      "   - 5️⃣ 온도 샘플링\n",
      "     결과:  아이들 크는 걸 계젤 연구소라고 또 있거든요. 거기서 아무런 개입이 업쌔 그냥 쭉 관찰만 수천 명의 아이드를 키우고 부모한테 맨처 면허 애착을 하죠?...\n",
      "\n",
      "✅ 청크 단위 처리로 앞부분 인식 성공!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "def load_finetuned_model():\n",
    "    \"\"\"파인튜닝된 모델 로드\"\"\"\n",
    "    model_path = \"whisper_pronunciation_finetuned\"\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"❌ 모델 폴더가 없습니다: {model_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "        processor = WhisperProcessor.from_pretrained(model_path)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "            print(\"🔥 GPU 사용 중\")\n",
    "        \n",
    "        print(\"✅ 파인튜닝 모델 로드 완료!\")\n",
    "        return model, processor\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 모델 로드 실패: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def load_pcm_with_preprocessing(pcm_file_path, add_silence=True, amplify_start=True):\n",
    "    \"\"\"PCM 파일 로드 + 전처리 옵션\"\"\"\n",
    "    \n",
    "    try:\n",
    "        with open(pcm_file_path, 'rb') as f:\n",
    "            pcm_data = f.read()\n",
    "        \n",
    "        np_pcm = np.frombuffer(pcm_data, dtype=np.int16)\n",
    "        normalized = np_pcm / np.max(np.abs(np_pcm))\n",
    "        \n",
    "        # 옵션 1: 앞부분에 짧은 무음 추가\n",
    "        if add_silence:\n",
    "            silence_duration = 0.1  # 0.1초\n",
    "            silence_samples = int(16000 * silence_duration)\n",
    "            silence = np.zeros(silence_samples, dtype=np.float32)\n",
    "            normalized = np.concatenate([silence, normalized])\n",
    "            print(f\"   🔇 앞부분에 {silence_duration}초 무음 추가\")\n",
    "        \n",
    "        # 옵션 2: 앞부분 볼륨 증폭\n",
    "        if amplify_start:\n",
    "            start_duration = 3.0  # 첫 3초\n",
    "            start_samples = int(16000 * start_duration)\n",
    "            if len(normalized) > start_samples:\n",
    "                amplification = 1.5  # 1.5배 증폭\n",
    "                normalized[:start_samples] *= amplification\n",
    "                # 클리핑 방지\n",
    "                normalized = np.clip(normalized, -1.0, 1.0)\n",
    "                print(f\"   🔊 앞부분 {start_duration}초 볼륨 {amplification}배 증폭\")\n",
    "        \n",
    "        return normalized.astype(np.float32)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ PCM 로드 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def test_different_decoding_approaches(model, processor, audio):\n",
    "    \"\"\"다양한 디코딩 접근법 테스트\"\"\"\n",
    "    \n",
    "    print(\"🧪 앞부분 누락 해결을 위한 디코딩 테스트\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    input_features = processor(\n",
    "        audio, \n",
    "        sampling_rate=16000, \n",
    "        return_tensors=\"pt\"\n",
    "    ).input_features\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        input_features = input_features.cuda()\n",
    "    \n",
    "    approaches = [\n",
    "        {\n",
    "            \"name\": \"1️⃣ 언어/태스크 강제 지정\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 500,\n",
    "                \"num_beams\": 1,\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 2.0,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                \"language\": \"ko\",\n",
    "                \"task\": \"transcribe\"\n",
    "            },\n",
    "            \"use_forced_ids\": True\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"2️⃣ 시작 토큰 강제 지정\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 500,\n",
    "                \"num_beams\": 1,\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 2.0,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                \"decoder_start_token_id\": processor.tokenizer.convert_tokens_to_ids(\"<|startoftranscript|>\")\n",
    "            },\n",
    "            \"use_forced_ids\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"3️⃣ 프롬프트 기반 디코딩\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 500,\n",
    "                \"num_beams\": 2,\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 2.0,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "            },\n",
    "            \"use_prompt\": True,\n",
    "            \"prompt_text\": \"아이들\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"4️⃣ 빔 서치 + 길이 페널티\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 500,\n",
    "                \"num_beams\": 3,\n",
    "                \"do_sample\": False,\n",
    "                \"repetition_penalty\": 1.8,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                \"length_penalty\": 0.8,  # 짧은 결과 선호도 낮춤\n",
    "                \"early_stopping\": False\n",
    "            },\n",
    "            \"use_forced_ids\": False\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"5️⃣ 온도 샘플링\",\n",
    "            \"params\": {\n",
    "                \"max_length\": 500,\n",
    "                \"num_beams\": 1,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.3,  # 낮은 온도로 보수적 생성\n",
    "                \"repetition_penalty\": 2.0,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "            },\n",
    "            \"use_forced_ids\": False\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for approach in approaches:\n",
    "        print(f\"\\n{approach['name']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        try:\n",
    "            params = approach['params'].copy()\n",
    "            \n",
    "            # 강제 디코더 ID 사용\n",
    "            if approach.get('use_forced_ids'):\n",
    "                forced_decoder_ids = processor.get_decoder_prompt_ids(\n",
    "                    language=\"ko\", \n",
    "                    task=\"transcribe\"\n",
    "                )\n",
    "                params['forced_decoder_ids'] = forced_decoder_ids\n",
    "            \n",
    "            # 프롬프트 기반 디코딩\n",
    "            elif approach.get('use_prompt'):\n",
    "                prompt_text = approach['prompt_text']\n",
    "                prompt_ids = processor.tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "                if torch.cuda.is_available():\n",
    "                    prompt_ids = prompt_ids.cuda()\n",
    "                params['decoder_input_ids'] = prompt_ids\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(\n",
    "                    input_features,\n",
    "                    **params\n",
    "                )\n",
    "            \n",
    "            transcription = processor.tokenizer.batch_decode(\n",
    "                predicted_ids, \n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            # 앞부분 인식 여부 확인\n",
    "            has_beginning = any(word in transcription[:50] for word in [\"아이들\", \"크는\", \"계절\"])\n",
    "            \n",
    "            print(f\"📝 결과: {transcription}\")\n",
    "            print(f\"🔍 앞부분 인식: {'✅' if has_beginning else '❌'}\")\n",
    "            \n",
    "            results.append({\n",
    "                'approach': approach['name'],\n",
    "                'result': transcription,\n",
    "                'has_beginning': has_beginning,\n",
    "                'length': len(transcription)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 실패: {e}\")\n",
    "            results.append({\n",
    "                'approach': approach['name'],\n",
    "                'result': None,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def test_chunked_processing(model, processor, audio):\n",
    "    \"\"\"청크 단위 처리로 앞부분 강제 인식\"\"\"\n",
    "    \n",
    "    print(\"\\n🔄 청크 단위 처리 테스트\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        # 첫 10초만 따로 처리\n",
    "        chunk_duration = 10  # 초\n",
    "        chunk_samples = chunk_duration * 16000\n",
    "        \n",
    "        if len(audio) > chunk_samples:\n",
    "            first_chunk = audio[:chunk_samples]\n",
    "            remaining_chunk = audio[chunk_samples:]\n",
    "        else:\n",
    "            first_chunk = audio\n",
    "            remaining_chunk = None\n",
    "        \n",
    "        print(f\"📊 첫 청크: {len(first_chunk)/16000:.1f}초\")\n",
    "        \n",
    "        # 첫 청크 처리\n",
    "        input_features = processor(\n",
    "            first_chunk,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_features = input_features.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_ids = model.generate(\n",
    "                input_features,\n",
    "                max_length=300,\n",
    "                num_beams=2,\n",
    "                repetition_penalty=2.0,\n",
    "                no_repeat_ngram_size=3,\n",
    "                language=\"ko\",\n",
    "                task=\"transcribe\"\n",
    "            )\n",
    "        \n",
    "        first_result = processor.tokenizer.batch_decode(\n",
    "            predicted_ids,\n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        print(f\"📝 첫 청크 결과: {first_result}\")\n",
    "        \n",
    "        # 나머지 청크도 처리 (있다면)\n",
    "        full_result = first_result\n",
    "        \n",
    "        if remaining_chunk is not None and len(remaining_chunk) > 16000:  # 1초 이상\n",
    "            print(f\"📊 나머지 청크: {len(remaining_chunk)/16000:.1f}초\")\n",
    "            \n",
    "            input_features = processor(\n",
    "                remaining_chunk,\n",
    "                sampling_rate=16000,\n",
    "                return_tensors=\"pt\"\n",
    "            ).input_features\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                input_features = input_features.cuda()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predicted_ids = model.generate(\n",
    "                    input_features,\n",
    "                    max_length=300,\n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.0,\n",
    "                    no_repeat_ngram_size=3\n",
    "                )\n",
    "            \n",
    "            remaining_result = processor.tokenizer.batch_decode(\n",
    "                predicted_ids,\n",
    "                skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            print(f\"📝 나머지 청크 결과: {remaining_result}\")\n",
    "            full_result = first_result + \" \" + remaining_result\n",
    "        \n",
    "        print(f\"📝 전체 결과: {full_result}\")\n",
    "        return full_result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 청크 처리 실패: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"메인 함수\"\"\"\n",
    "    \n",
    "    print(\"🔧 파인튜닝 모델 앞부분 누락 해결\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 모델 로드\n",
    "    model, processor = load_finetuned_model()\n",
    "    if model is None:\n",
    "        return\n",
    "    \n",
    "    # PCM 파일 로드 (전처리 옵션 적용)\n",
    "    pcm_file = \"000025.wav\"\n",
    "    \n",
    "    print(\"\\n🎵 PCM 파일 로드 중...\")\n",
    "    audio = load_pcm_with_preprocessing(\n",
    "        pcm_file, \n",
    "        add_silence=True,    # 앞부분 무음 추가\n",
    "        amplify_start=True   # 앞부분 볼륨 증폭\n",
    "    )\n",
    "    \n",
    "    if audio is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"   📊 전처리 후 길이: {len(audio)/16000:.2f}초\")\n",
    "    \n",
    "    # 다양한 디코딩 접근법 테스트\n",
    "    results = test_different_decoding_approaches(model, processor, audio)\n",
    "    \n",
    "    # 청크 단위 처리 테스트\n",
    "    chunked_result = test_chunked_processing(model, processor, audio)\n",
    "    \n",
    "    # 결과 요약\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"📊 해결 방법 평가\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    successful_approaches = [r for r in results if r.get('has_beginning')]\n",
    "    \n",
    "    if successful_approaches:\n",
    "        print(\"✅ 앞부분 인식 성공한 방법들:\")\n",
    "        for approach in successful_approaches:\n",
    "            print(f\"   - {approach['approach']}\")\n",
    "            print(f\"     결과: {approach['result'][:100]}...\")\n",
    "    else:\n",
    "        print(\"❌ 모든 방법이 앞부분 인식 실패\")\n",
    "        print(\"💡 추천: 청크 단위 처리나 모델 재학습 고려\")\n",
    "    \n",
    "    if chunked_result and \"아이들\" in chunked_result[:50]:\n",
    "        print(\"\\n✅ 청크 단위 처리로 앞부분 인식 성공!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacc37a-a1ab-42ee-8ff7-13f3897704e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
