import torch.nn as nn
import numpy as np
import torch
from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperConfig
from typing import Any, Text
import os
import time


class STTModelSingleton:
    """
    ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ëŠ” STT ëª¨ë¸
    """
    _instance = None
    _initialized = False

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(STTModelSingleton, cls).__new__(cls)
        return cls._instance

    def __init__(self):
        if not self._initialized:
            print("ğŸ¯ STT ëª¨ë¸ ì‹±ê¸€í†¤ ì´ˆê¸°í™” ì‹œì‘")
            self.model = None
            self.processor = None
            self.device = None
            self.sr = 16000
            self.is_finetuned_model = False

            # ëª¨ë¸ ì´ˆê¸°í™” (í•œ ë²ˆë§Œ ì‹¤í–‰)
            self._setup_model()
            STTModelSingleton._initialized = True
            print("âœ… STT ëª¨ë¸ ì‹±ê¸€í†¤ ì´ˆê¸°í™” ì™„ë£Œ")

    def _setup_model(self):
        """ëª¨ë¸ ì„¤ì • (í•œ ë²ˆë§Œ ì‹¤í–‰)"""
        print("ğŸš€ STT ëª¨ë¸ ì„¤ì • ì‹œì‘")
        self.check_cuda()
        self.initialize_model()
        self.find_pt()
        print("âœ… STT ëª¨ë¸ ì„¤ì • ì™„ë£Œ")

    def transcribe(self, audio):
        """í†µí•©ëœ ìŒì„± ì¸ì‹ í•¨ìˆ˜ (MFCC ì œê±°ëœ ë²„ì „)"""
        print(f"ğŸ” ì¶”ë¡  ì‹œì‘ - ì…ë ¥ íƒ€ì…: {type(audio)}, í˜•íƒœ: {audio.shape}")

        try:
            # ğŸ¯ MFCC ì²˜ë¦¬ ì™„ì „ ì œê±° - ì›ì‹œ ì˜¤ë””ì˜¤ë§Œ ì‚¬ìš©
            if len(audio.shape) == 2:
                print("âš ï¸ 2D ë°°ì—´ ê°ì§€ - ì›ì‹œ ì˜¤ë””ì˜¤ë¡œ ë³€í™˜")
                # 2D ë°°ì—´ì´ ë“¤ì–´ì˜¤ë©´ ì²« ë²ˆì§¸ ì¶•ë§Œ ì‚¬ìš© (MFCC ëŒ€ì‹ )
                audio = audio[0, :] if audio.shape[0] < audio.shape[1] else audio.flatten()
                print(f"ğŸ”„ 2D â†’ 1D ë³€í™˜ ì™„ë£Œ (ê¸¸ì´: {len(audio)})")
            else:
                print("âœ… 1D ë°°ì—´ ê°ì§€ - ì›ì‹œ ì˜¤ë””ì˜¤ ì§ì ‘ ì‚¬ìš©")

            # ì •ê·œí™”
            if np.max(np.abs(audio)) > 0:
                audio = audio / np.max(np.abs(audio))
                print("âœ… ì˜¤ë””ì˜¤ ì •ê·œí™” ì™„ë£Œ")

            # ìµœì†Œ ê¸¸ì´ í™•ë³´ (Whisper ìš”êµ¬ì‚¬í•­)
            if len(audio) < 1600:
                repeat_times = (1600 // len(audio)) + 1
                audio = np.tile(audio, repeat_times)[:1600]
                print(f"ğŸ”„ ìµœì†Œ ê¸¸ì´ í™•ë³´: {len(audio)}")

            # Whisper ì „ì²˜ë¦¬
            input_features = self.processor.feature_extractor(
                audio,
                sampling_rate=self.sr,
                return_tensors="pt"
            ).input_features

            # GPU ì‚¬ìš© ì‹œ ì´ë™
            if torch.cuda.is_available():
                input_features = input_features.cuda()
                self.model = self.model.cuda()

            input_features = input_features.to(self.model.dtype)

            # ì¶”ë¡  ìˆ˜í–‰
            start_time = time.time()

            with torch.no_grad():
                if self.is_finetuned_model:
                    # ğŸ¯ íŒŒì¸íŠœë‹ ëª¨ë¸ìš© ê°œì„ ëœ íŒŒë¼ë¯¸í„°
                    predicted_ids = self.model.generate(
                        input_features,
                        max_length=448,
                        num_beams=3,  # ë¹” ì„œì¹˜ë¡œ ì •í™•ë„ í–¥ìƒ
                        repetition_penalty=1.1,  # ë‚®ê²Œ ì¡°ì •
                        no_repeat_ngram_size=2,  # ë‚®ê²Œ ì¡°ì •
                        do_sample=False,
                        early_stopping=True,
                        forced_decoder_ids=[[1, 50259], [2, 50264], [3, 50359]]
                    )
                else:
                    # ğŸ¯ ì›ë³¸ ëª¨ë¸ìš© íŒŒë¼ë¯¸í„°
                    predicted_ids = self.model.generate(
                        input_features,
                        max_length=448,
                        num_beams=5,  # ë” ì •í™•í•œ íƒìƒ‰
                        repetition_penalty=1.1,
                        no_repeat_ngram_size=2,
                        do_sample=False,
                        early_stopping=True,
                        forced_decoder_ids=[[1, 50259], [2, 50264], [3, 50359]]
                    )

            inference_time = time.time() - start_time
            print(f"ğŸ ì „ì²´ ì¶”ë¡  ì‹œê°„: {inference_time:.3f}ì´ˆ")

            # í…ìŠ¤íŠ¸ ë””ì½”ë”©
            transcription = self.processor.tokenizer.batch_decode(
                predicted_ids,
                skip_special_tokens=True
            )[0]

            print(f"âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ: {transcription}")
            return transcription

        except Exception as e:
            print(f"âŒ ìŒì„± ì¸ì‹ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            return "ìŒì„± ì¸ì‹ ì‹¤íŒ¨"

    # ğŸ—‘ï¸ ê¸°ì¡´ inference_stt í•¨ìˆ˜ ì œê±° (transcribeë¡œ í†µí•©)

    def initialize_model(self):
        try:
            print("ğŸ”„ ê¸°ë³¸ ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë”© ì¤‘...")
            self.processor = WhisperProcessor.from_pretrained("openai/whisper-small")
            self.model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

            if hasattr(self.model, 'generation_config'):
                if hasattr(self.model.generation_config, 'forced_decoder_ids'):
                    self.model.generation_config.forced_decoder_ids = None
                if hasattr(self.model.generation_config, 'suppress_tokens'):
                    self.model.generation_config.suppress_tokens = None
                print("ğŸ”§ ê¸°ë³¸ ëª¨ë¸ generation config ì •ë¦¬ ì™„ë£Œ")

            if self.device is not None:
                self.model.to(self.device)

            print("âœ… ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            print(f"âŒ ê¸°ë³¸ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ìˆ˜ë™ìœ¼ë¡œ ì»´í¬ë„ŒíŠ¸ ìƒì„±
            try:
                from transformers import WhisperFeatureExtractor, WhisperTokenizer
                print("ğŸ”„ ìˆ˜ë™ ì»´í¬ë„ŒíŠ¸ ìƒì„± ì‹œë„...")

                feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
                tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small")
                self.processor = WhisperProcessor(
                    feature_extractor=feature_extractor,
                    tokenizer=tokenizer
                )
                self.model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")

                if self.device is not None:
                    self.model.to(self.device)

                print("âœ… ìˆ˜ë™ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

            except Exception as fallback_error:
                print(f"âŒ ìˆ˜ë™ ì´ˆê¸°í™”ë„ ì‹¤íŒ¨: {str(fallback_error)}")
                raise RuntimeError("ëª¨ë¸ ì´ˆê¸°í™”ê°€ ì™„ì „íˆ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    def check_cuda(self):
        if torch.cuda.is_available():
            self.device = torch.device("cuda:0")
            print("GPU ì‚¬ìš©")
        else:
            self.device = torch.device("cpu")
            print("CPU ì‚¬ìš©")

    def _safe_load_checkpoint(self, model_file):
        try:
            print("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(model_file, map_location=self.device, weights_only=False)
            return checkpoint
        except TypeError:
            print("ğŸ”„ ì´ì „ PyTorch ë²„ì „ìœ¼ë¡œ ë¡œë”© ì¤‘...")
            checkpoint = torch.load(model_file, map_location=self.device)
            return checkpoint
        except Exception as e:
            print(f"âš ï¸ ì²« ë²ˆì§¸ ë¡œë”© ì‹œë„ ì‹¤íŒ¨: {str(e)}")
            try:
                from transformers.models.whisper.tokenization_whisper import WhisperTokenizer
                from transformers.models.whisper.feature_extraction_whisper import WhisperFeatureExtractor
                safe_globals = [WhisperTokenizer, WhisperFeatureExtractor]

                if hasattr(torch.serialization, 'add_safe_globals'):
                    torch.serialization.add_safe_globals(safe_globals)
                    checkpoint = torch.load(model_file, map_location=self.device)
                else:
                    with torch.serialization.safe_globals(safe_globals):
                        checkpoint = torch.load(model_file, map_location=self.device)

                print("âœ… ì•ˆì „í•œ ê¸€ë¡œë²Œ ì„¤ì •ìœ¼ë¡œ ë¡œë”© ì„±ê³µ")
                return checkpoint
            except Exception as retry_error:
                print(f"âŒ ì¬ì‹œë„ ì‹¤íŒ¨: {str(retry_error)}")
                return None

    def create_compatible_processor(self, checkpoint):
        try:
            stored_tokenizer = checkpoint['tokenizer']
            if not hasattr(stored_tokenizer, 'all_special_ids'):
                print("âš ï¸ ì €ì¥ëœ í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± ë¬¸ì œ - ìƒˆë¡œ ìƒì„±")
                raise AttributeError("Missing all_special_ids attribute")

            stored_feature_extractor = checkpoint['feature_extractor']
            required_attrs = ['dither', 'chunk_length', 'feature_size']
            missing_attrs = [attr for attr in required_attrs if not hasattr(stored_feature_extractor, attr)]

            if missing_attrs:
                print(f"âš ï¸ FeatureExtractor í˜¸í™˜ì„± ë¬¸ì œ - ëˆ„ë½ëœ ì†ì„±: {missing_attrs}")
                raise AttributeError(f"Missing attributes: {missing_attrs}")

            # WhisperProcessor ì˜¬ë°”ë¥¸ ìƒì„± ë°©ë²•
            processor = WhisperProcessor(
                feature_extractor=stored_feature_extractor,
                tokenizer=stored_tokenizer
            )
            print("âœ… ì €ì¥ëœ í”„ë¡œì„¸ì„œ ì‚¬ìš©")
            return processor

        except (AttributeError, KeyError, Exception) as e:
            print(f"âš ï¸ ì €ì¥ëœ í”„ë¡œì„¸ì„œ ì‚¬ìš© ë¶ˆê°€: {str(e)}")
            print("ğŸ”„ ìƒˆë¡œìš´ í”„ë¡œì„¸ì„œë¡œ ëŒ€ì²´")

            # ì•ˆì „í•œ ìƒˆ í”„ë¡œì„¸ì„œ ìƒì„±
            try:
                new_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
                print("âœ… ìƒˆë¡œìš´ í”„ë¡œì„¸ì„œ ìƒì„± ì™„ë£Œ (í•œêµ­ì–´ ì„¤ì •)")
                return new_processor
            except Exception as processor_error:
                print(f"âŒ ìƒˆ í”„ë¡œì„¸ì„œ ìƒì„± ì‹¤íŒ¨: {str(processor_error)}")
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ê¸°ë³¸ í”„ë¡œì„¸ì„œ ìƒì„±
                from transformers import WhisperFeatureExtractor, WhisperTokenizer
                feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
                tokenizer = WhisperTokenizer.from_pretrained("openai/whisper-small")
                fallback_processor = WhisperProcessor(
                    feature_extractor=feature_extractor,
                    tokenizer=tokenizer
                )
                print("âœ… Fallback í”„ë¡œì„¸ì„œ ìƒì„± ì™„ë£Œ")
                return fallback_processor

    def load_pt(self, pt_file: Any):
        try:
            model_file = pt_file
            print(f"ëª¨ë¸ íŒŒì¼ ë¡œë”© ì‹œë„: {model_file}")

            if not os.path.exists(model_file):
                print(f"âŒ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_file}")
                return False

            checkpoint = self._safe_load_checkpoint(model_file)
            if checkpoint is None:
                return False

            if 'model_state_dict' in checkpoint:
                config = WhisperConfig.from_dict(checkpoint['model_config'])
                self.model = WhisperForConditionalGeneration(config)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.processor = self.create_compatible_processor(checkpoint)

                if hasattr(self.model, 'generation_config'):
                    if hasattr(self.model.generation_config, 'forced_decoder_ids'):
                        self.model.generation_config.forced_decoder_ids = None
                        print("ğŸ”§ forced_decoder_ids ì œê±°ë¨")
                    if hasattr(self.model.generation_config, 'suppress_tokens'):
                        self.model.generation_config.suppress_tokens = None
                        print("ğŸ”§ suppress_tokens ì œê±°ë¨")
                    print("âœ… Generation config í˜¸í™˜ì„± ìˆ˜ì • ì™„ë£Œ")

                self.is_finetuned_model = True
                print("âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                state_dict = checkpoint
                self.model.load_state_dict(state_dict)
                self.is_finetuned_model = True
                print("âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")

            self.model.to(self.device)
            return True

        except Exception as e:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ ì—ëŸ¬ : {str(e)}")
            return False

    def get_model_path(self):
        if 'MODEL_PATH' in os.environ:
            return os.environ['MODEL_PATH']

        current_file_dir = os.path.dirname(os.path.abspath(__file__))
        possible_paths = [
            os.path.join(current_file_dir, "model"),
            os.path.join(os.path.dirname(current_file_dir), "model"),
            os.path.join(current_file_dir, "service", "model"),
            "/Users/yongmin/Desktop/voicelearning/VoiceRecognitionAIAPI/app/connectAPI/service/model",
            "/app/app/connectAPI/service/model"
        ]

        for path in possible_paths:
            if os.path.exists(path):
                print(f"ğŸ“ ëª¨ë¸ ë””ë ‰í† ë¦¬ ë°œê²¬: {path}")
                return path

        print("âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    def find_pt(self, model_dir=None):
        """íŒŒì¸íŠœë‹ ëª¨ë¸ íŒŒì¼ íƒì§€ ë° ë¡œë“œ"""
        if model_dir is None:
            model_dir = self.get_model_path()

        if model_dir is None:
            print("âš ï¸ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            return False

        found = False
        print(f"ğŸ” ëª¨ë¸ íŒŒì¼ íƒìƒ‰ ì¤‘: {model_dir}")

        if os.path.exists(model_dir):
            # íŠ¹ì • íŒŒì¼ëª… ìš°ì„  ê²€ìƒ‰
            specific_file = os.path.join(model_dir, "whisper_pronunciation_model.pt")
            if os.path.exists(specific_file):
                print(f"ğŸ¯ íŠ¹ì • ëª¨ë¸ íŒŒì¼ ë°œê²¬: {specific_file}")
                if self.load_pt(specific_file):
                    return True

            # ì¼ë°˜ .pt íŒŒì¼ ê²€ìƒ‰
            for root, _, files in os.walk(model_dir):
                for file in files:
                    if file.endswith(".pt") or file.endswith(".pth"):
                        full_path = os.path.join(root, file)
                        print(f"ğŸ“„ ëª¨ë¸ íŒŒì¼ ë°œê²¬: {full_path}")
                        if self.load_pt(full_path):
                            found = True
                            break
                if found:
                    break
        else:
            print(f"âŒ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_dir}")

        if not found:
            print("âš ï¸ .pt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

        return found


# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ìƒì„±)
stt_model_instance = None


def get_stt_model():
    """STT ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    global stt_model_instance
    if stt_model_instance is None:
        stt_model_instance = STTModelSingleton()
    return stt_model_instance


def quick_transcribe(audio_data: np.ndarray) -> str:
    """ë¹ ë¥¸ ìŒì„± ì¸ì‹ í•¨ìˆ˜ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œìš©)"""
    model = get_stt_model()
    return model.transcribe(audio_data)