from app.connectAPI.service.preprocessing.wav_to_pcm import convert_wav_to_pcm
from app.connectAPI.service.preprocessing.pcm_to_numpy import convert_pcm_to_numpy
from app.connectAPI.service.preprocessing.processing import (
    convert_preprocessing,
    convert_preprocessing_for_microphone_stable,
    convert_preprocessing_for_microphone_advanced,
    convert_preprocessing_for_microphone_chunked,
    convert_preprocessing_simple,
    intelligent_feature_processing,
    detect_feature_type
)
import numpy as np
from app.connectAPI.service.model.models import get_stt_model
from typing import Union, Dict, Any
from fastapi import UploadFile
import wave
import io
import librosa


async def control_all(wav_data: Union[bytes, UploadFile]) -> Dict[str, Any]:
    """
    í†µí•© ì˜¤ë””ì˜¤ ì²˜ë¦¬ ë° ìŒì„± ì¸ì‹ í•¨ìˆ˜
    stt.pyì™€ ì™„ì „ í˜¸í™˜ - ìë™ìœ¼ë¡œ ìµœì  ì²˜ë¦¬ ë°©ë²• ì„ íƒ
    """
    model = get_stt_model()

    try:
        # WAV ë°ì´í„° ì¤€ë¹„
        if isinstance(wav_data, UploadFile):
            wav_content = await wav_data.read()
            await wav_data.seek(0)
        else:
            wav_content = wav_data

        # ì›ë³¸ WAV íŒŒì¼ ì •ë³´ ë¶„ì„
        audio_info = await _analyze_wav_info(wav_content)
        print(f"ğŸ“Š ì›ë³¸ WAV ì •ë³´: {audio_info}")

        # ìƒ˜í”Œë§ ë ˆì´íŠ¸ì— ë”°ë¥¸ ì²˜ë¦¬ ë¶„ê¸°
        if audio_info['sample_rate'] != 16000:
            # 16kHzê°€ ì•„ë‹Œ ê²½ìš°: LibROSAë¡œ ë³€í™˜ í›„ ê°„ë‹¨ ì²˜ë¦¬
            return await _handle_non_16khz_audio(wav_content, model, audio_info)
        else:
            # 16kHzì¸ ê²½ìš°: ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
            return await _handle_16khz_audio(wav_content, model, audio_info)

    except Exception as e:
        print(f"âŒ ì „ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # í´ë°±: LibROSAë¡œ ìµœì†Œ ì²˜ë¦¬
        return await _fallback_processing(wav_data, model)


async def _handle_non_16khz_audio(wav_content: bytes, model, audio_info: Dict[str, Any]) -> Dict[str, Any]:
    """16kHzê°€ ì•„ë‹Œ ì˜¤ë””ì˜¤ ì²˜ë¦¬"""
    try:
        print(f"ğŸ”„ ìƒ˜í”Œë§ ë ˆì´íŠ¸ ë³€í™˜: {audio_info['sample_rate']}Hz â†’ 16000Hz")

        # LibROSAë¡œ 16kHz ë³€í™˜ ë° ìµœì†Œ ì „ì²˜ë¦¬
        audio_data, _ = librosa.load(io.BytesIO(wav_content), sr=16000)
        processed_audio = await _optimized_preprocessing(audio_data)

        # ìŒì„± ì¸ì‹
        text = model.transcribe(processed_audio)

        return {
            "text": text,
            "processing_method": f"resampled_from_{audio_info['sample_rate']}Hz"
        }

    except Exception as e:
        print(f"âŒ ë¦¬ìƒ˜í”Œë§ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


async def _handle_16khz_audio(wav_content: bytes, model, audio_info: Dict[str, Any]) -> Dict[str, Any]:
    """16kHz ì˜¤ë””ì˜¤ ì²˜ë¦¬ - ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ ìš°ì„ """
    try:
        print("ğŸ¯ 16kHz ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œì‘")

        # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
        pcm_data, sample_width = await _convert_wav_to_pcm_bytes(wav_content)
        numpy_data = await convert_pcm_to_numpy(pcm_data, sample_width)

        # ì˜¤ë””ì˜¤ ë¶„ì„
        duration = len(numpy_data) / 16000
        audio_max = np.max(np.abs(numpy_data))
        audio_rms = np.sqrt(np.mean(numpy_data ** 2))

        print(f"ğŸ” ì˜¤ë””ì˜¤ ë¶„ì„: ê¸¸ì´ {duration:.1f}ì´ˆ, ìµœëŒ€ê°’ {audio_max:.4f}, RMS {audio_rms:.4f}")

        # ê¸¸ì´ì™€ í’ˆì§ˆì— ë”°ë¥¸ ì „ì²˜ë¦¬ ì„ íƒ
        processed_audio = await _select_preprocessing_method(numpy_data, duration, audio_max, audio_rms)

        # ìŒì„± ì¸ì‹
        text = model.transcribe(processed_audio)

        return {"text": text, "processing_method": "legacy_pipeline"}

    except Exception as e:
        print(f"âŒ 16kHz ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise


async def _select_preprocessing_method(
        numpy_data: np.ndarray,
        duration: float,
        max_amplitude: float,
        rms: float
) -> np.ndarray:
    """ì˜¤ë””ì˜¤ íŠ¹ì„±ì— ë”°ë¥¸ ìµœì  ì „ì²˜ë¦¬ ë°©ë²• ì„ íƒ"""

    try:
        print(f"ğŸ›ï¸ ì „ì²˜ë¦¬ ë°©ë²• ì„ íƒ ì¤‘...")

        # 1. ë§¤ìš° ì§§ì€ ì˜¤ë””ì˜¤
        if duration < 1.0:
            print("âš¡ ì§§ì€ ì˜¤ë””ì˜¤ - ê°„ë‹¨ ì „ì²˜ë¦¬")
            return await convert_preprocessing_simple(numpy_data)

        # 2. ë§¤ìš° ê¸´ ì˜¤ë””ì˜¤
        elif duration > 30.0:
            print("ğŸ“Š ê¸´ ì˜¤ë””ì˜¤ - ì²­í‚¹ ì „ì²˜ë¦¬")
            return await convert_preprocessing_for_microphone_chunked(numpy_data)

        # 3. ì¤‘ê°„ ê¸¸ì´ ì˜¤ë””ì˜¤ (ëŒ€ë¶€ë¶„ì˜ ì¼€ì´ìŠ¤)
        elif duration < 8.0:
            print("ğŸ¤ ì¤‘ê°„ ê¸¸ì´ - ì•ˆì •í™” ì „ì²˜ë¦¬")
            return await convert_preprocessing_for_microphone_stable(numpy_data)

        # 4. í’ˆì§ˆì— ë”°ë¥¸ ì„ íƒ (8ì´ˆ ì´ìƒ)
        else:
            # ì •ê·œí™” ìƒíƒœ í™•ì¸
            is_normalized = max_amplitude <= 1.0

            # ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤
            if is_normalized and 0.03 <= rms <= 0.5:
                print("ğŸµ ê³ í’ˆì§ˆ ì˜¤ë””ì˜¤ - ê¸°ë³¸ ì „ì²˜ë¦¬")
                return await convert_preprocessing(numpy_data)

            # ì €í’ˆì§ˆ ë˜ëŠ” ë¹„ì •ê·œí™” ì˜¤ë””ì˜¤
            elif not is_normalized or rms < 0.03 or max_amplitude > 0.98:
                print("ğŸ”§ ì €í’ˆì§ˆ/ë¹„ì •ê·œí™” ì˜¤ë””ì˜¤ - ê³ ê¸‰ ì „ì²˜ë¦¬")
                return await convert_preprocessing_for_microphone_advanced(numpy_data)

            # ê¸°ë³¸ê°’
            else:
                print("ğŸ¤ ì¼ë°˜ ì˜¤ë””ì˜¤ - ì•ˆì •í™” ì „ì²˜ë¦¬")
                return await convert_preprocessing_for_microphone_stable(numpy_data)

    except Exception as e:
        print(f"âš ï¸ ì „ì²˜ë¦¬ ì„ íƒ ì‹¤íŒ¨: {e} - ê°„ë‹¨ ì „ì²˜ë¦¬ë¡œ í´ë°±")
        return await convert_preprocessing_simple(numpy_data)


async def _optimized_preprocessing(audio_data: np.ndarray) -> np.ndarray:
    """
    ë¦¬ìƒ˜í”Œë§ëœ ì˜¤ë””ì˜¤ìš© ìµœì í™”ëœ ì „ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ê²Œì´íŠ¸ ê°œì„ )
    """
    try:
        print("ğŸµ ìµœì í™”ëœ ì „ì²˜ë¦¬ ì ìš©")

        # 1. DC ì„±ë¶„ ì œê±°
        audio_data = audio_data - np.mean(audio_data)

        # 2. ì •ê·œí™”
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            audio_data = audio_data / max_val

        # 3. ë§¤ìš° ë‚®ì€ ë…¸ì´ì¦ˆ ê²Œì´íŠ¸ (ê¸°ì¡´ 0.005 â†’ 0.0005ë¡œ ê°œì„ )
        noise_threshold = 0.0005
        audio_data = np.where(np.abs(audio_data) < noise_threshold, 0, audio_data)

        # 4. ìµœì†Œí•œì˜ í˜ì´ë“œ (ê¸°ì¡´ //20 â†’ //50ìœ¼ë¡œ ì¤„ì„)
        window_size = min(256, len(audio_data) // 50)
        if window_size > 0:
            fade_in = np.linspace(0, 1, window_size)
            fade_out = np.linspace(1, 0, window_size)
            audio_data[:window_size] *= fade_in
            audio_data[-window_size:] *= fade_out

        print(f"âœ… ìµœì í™”ëœ ì „ì²˜ë¦¬ ì™„ë£Œ: ìµœëŒ€ê°’ {np.max(np.abs(audio_data)):.4f}")
        return audio_data

    except Exception as e:
        print(f"âŒ ìµœì í™”ëœ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return audio_data


async def _convert_wav_to_pcm_bytes(wav_content: bytes) -> tuple:
    """WAV ë°”ì´íŠ¸ë¥¼ PCM ë°ì´í„°ë¡œ ë³€í™˜"""
    try:
        print("ğŸ”„ WAV â†’ PCM ë³€í™˜")

        with wave.open(io.BytesIO(wav_content), 'rb') as wav_file:
            frames = wav_file.readframes(wav_file.getnframes())
            sample_width = wav_file.getsampwidth()
            sample_rate = wav_file.getframerate()

            print(f"ğŸ“Š ë³€í™˜ ì™„ë£Œ: SR={sample_rate}Hz, width={sample_width}bytes")
            return frames, sample_width

    except Exception as direct_error:
        print(f"âš ï¸ ì§ì ‘ ë³€í™˜ ì‹¤íŒ¨: {direct_error}")

        # í´ë°±: ê¸°ì¡´ í•¨ìˆ˜ í˜¸í™˜ ë˜í¼
        try:
            print("ğŸ”„ í˜¸í™˜ ë˜í¼ë¡œ ì¬ì‹œë„")

            class BytesUploadFile:
                def __init__(self, content: bytes):
                    self._content = content
                    self._position = 0

                async def read(self, size: int = -1):
                    if size == -1:
                        result = self._content[self._position:]
                        self._position = len(self._content)
                    else:
                        result = self._content[self._position:self._position + size]
                        self._position += len(result)
                    return result

                async def seek(self, position: int):
                    self._position = position

                @property
                def filename(self):
                    return "audio.wav"

                @property
                def content_type(self):
                    return "audio/wav"

            fake_file = BytesUploadFile(wav_content)
            return await convert_wav_to_pcm(fake_file)

        except Exception as wrapper_error:
            print(f"âŒ ë˜í¼ ë³€í™˜ë„ ì‹¤íŒ¨: {wrapper_error}")
            raise Exception(f"WAVâ†’PCM ë³€í™˜ ì‹¤íŒ¨: {direct_error}")


async def _analyze_wav_info(wav_content: bytes) -> Dict[str, Any]:
    """WAV íŒŒì¼ ì •ë³´ ë¶„ì„"""
    try:
        with wave.open(io.BytesIO(wav_content), 'rb') as wav_file:
            info = {
                'sample_rate': wav_file.getframerate(),
                'channels': wav_file.getnchannels(),
                'sample_width': wav_file.getsampwidth(),
                'frames': wav_file.getnframes(),
                'duration': wav_file.getnframes() / wav_file.getframerate()
            }
            return info
    except Exception as e:
        print(f"âš ï¸ WAV ì •ë³´ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return {
            'sample_rate': 16000,
            'channels': 1,
            'sample_width': 2,
            'frames': 0,
            'duration': 0
        }


async def _fallback_processing(wav_data: Union[bytes, UploadFile], model) -> Dict[str, Any]:
    """ìµœí›„ì˜ í´ë°± ì²˜ë¦¬"""
    try:
        print("ğŸ†˜ í´ë°± ì²˜ë¦¬ ì‹œì‘")

        # WAV ë°ì´í„° ì¤€ë¹„
        if isinstance(wav_data, UploadFile):
            wav_content = await wav_data.read()
        else:
            wav_content = wav_data

        # LibROSAë¡œ ìµœì†Œ ì²˜ë¦¬
        audio_data, _ = librosa.load(io.BytesIO(wav_content), sr=16000)

        # ê¸°ë³¸ ì •ê·œí™”ë§Œ
        if np.max(np.abs(audio_data)) > 0:
            audio_data = audio_data / np.max(np.abs(audio_data))

        print(f"âœ… í´ë°± ì²˜ë¦¬ ì™„ë£Œ: {len(audio_data)} samples")

        # ìŒì„± ì¸ì‹
        text = model.transcribe(audio_data)
        return {"text": text, "processing_method": "fallback"}

    except Exception as e:
        print(f"âŒ í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"text": "ìŒì„± ì¸ì‹ ì‹¤íŒ¨"}


# =============================================================================
# ì¶”ê°€ ì²˜ë¦¬ ëª¨ë“œë“¤ (ì„ íƒì  ì‚¬ìš©)
# =============================================================================

async def simple_control_all(wav_data: Union[bytes, UploadFile]) -> Dict[str, Any]:
    """ê°„ë‹¨í•œ ì²˜ë¦¬ ëª¨ë“œ - ë¬´ì¡°ê±´ convert_preprocessing_simple ì‚¬ìš©"""
    try:
        print("âš¡ ê°„ë‹¨ ì²˜ë¦¬ ëª¨ë“œ")

        # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì²˜ë¦¬
        pcm_data, sample_width = await convert_wav_to_pcm(wav_data)
        numpy_data = await convert_pcm_to_numpy(pcm_data, sample_width)

        # ê°„ë‹¨ ì „ì²˜ë¦¬ë§Œ
        processed_audio = await convert_preprocessing_simple(numpy_data)

        # ìŒì„± ì¸ì‹
        model = get_stt_model()
        text = model.transcribe(processed_audio)

        return {"text": text, "processing_method": "simple"}

    except Exception as e:
        print(f"âŒ ê°„ë‹¨ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"text": "ê°„ë‹¨ ì²˜ë¦¬ ì‹¤íŒ¨"}


async def mfcc_control_all(wav_data: Union[bytes, UploadFile]) -> Dict[str, Any]:
    """MFCC íŠ¹ì„± ê¸°ë°˜ ì²˜ë¦¬ ëª¨ë“œ"""
    try:
        print("ğŸ“ MFCC íŠ¹ì„± ì²˜ë¦¬ ëª¨ë“œ")

        # WAV ë°ì´í„° ì¤€ë¹„
        if isinstance(wav_data, UploadFile):
            wav_content = await wav_data.read()
        else:
            wav_content = wav_data

        # LibROSAë¡œ ë¡œë“œ
        audio_data, _ = librosa.load(io.BytesIO(wav_content), sr=16000)

        # 2D ë°°ì—´ì¸ì§€ í™•ì¸ (MFCC íŠ¹ì„± ë°ì´í„°)
        if len(audio_data.shape) == 2:
            feature_info = detect_feature_type(audio_data)
            if feature_info['confidence'] > 0.7:
                processed_audio = await intelligent_feature_processing(audio_data, 16000)
                model = get_stt_model()
                text = model.transcribe(processed_audio)
                return {"text": text, "processing_method": "mfcc_features"}

        # ì¼ë°˜ ì˜¤ë””ì˜¤ì¸ ê²½ìš° ê°„ë‹¨ ì²˜ë¦¬
        processed_audio = await convert_preprocessing_simple(audio_data)
        model = get_stt_model()
        text = model.transcribe(processed_audio)
        return {"text": text, "processing_method": "mfcc_simple"}

    except Exception as e:
        print(f"âŒ MFCC ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return {"text": "MFCC ì²˜ë¦¬ ì‹¤íŒ¨"}


# =============================================================================
# í•¨ìˆ˜ ë³„ì¹­ (í•˜ìœ„ í˜¸í™˜ì„±)
# =============================================================================

# ë©”ì¸ í•¨ìˆ˜ (stt.pyì—ì„œ ì‚¬ìš©)
process_audio = control_all

# ì¶”ê°€ ëª¨ë“œë“¤
simple_process_audio = simple_control_all
mfcc_process_audio = mfcc_control_all