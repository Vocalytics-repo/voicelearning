{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "SqHgNVVhfFy-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "#import jiwer : label 텍스트가 있을 경우 활용\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from typing import Any, Text\n",
        "\n",
        "class sttmodel:\n",
        "    def __init__(self):\n",
        "        self.model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "        self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "        self.device = None\n",
        "        self.transcription = None\n",
        "        self.sr = 16000\n",
        "\n",
        "    def inference_stt(self, data):\n",
        "        audio_array = data\n",
        "        input_features = self.processor.feature_extractor(\n",
        "            audio_array,\n",
        "            sampling_rate = self.sr,\n",
        "            return_tensors = \"pt\"\n",
        "        ).input_features\n",
        "        # 입력 특성을 모델과 동일한 장치로 이동\n",
        "        input_features = input_features.to(self.device)\n",
        "\n",
        "        # 모델을 통한 예측\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = self.model.generate(input_features)\n",
        "\n",
        "        # 예측된 토큰을 텍스트로 디코딩\n",
        "        transcription = self.processor.tokenizer.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    def check_cuda(self,):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda:0\")\n",
        "            print(\"GPU 사용\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            print(\"CPU 사용\")\n",
        "\n",
        "    def load_pt(self, pt_file: str):\n",
        "        try:\n",
        "            print(f\"모델 파일 로드 시도: {pt_file}\")\n",
        "            state_dict = torch.load(pt_file, map_location=self.device)\n",
        "            self.model.load_state_dict(state_dict)\n",
        "            print(f\"모델 파일 성공적으로 로드됨: {pt_file}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"모델 파일 에러: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def find_pt(self, model_dir):\n",
        "        found = False\n",
        "        if os.path.exists(model_dir):\n",
        "            print(f\"{model_dir} 디렉토리 존재함\")\n",
        "            # 먼저 디렉토리 내용 출력\n",
        "            print(f\"디렉토리 내용: {os.listdir(model_dir)}\")\n",
        "\n",
        "            for root, _, files in os.walk(model_dir):\n",
        "                for file in files:\n",
        "                    if file.endswith(\".pt\"):\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        print(f\"모델 파일 발견: {full_path}\")\n",
        "                        if self.load_pt(full_path):\n",
        "                            found = True\n",
        "                            break\n",
        "                if found:\n",
        "                    break\n",
        "        else:\n",
        "            print(f\"{model_dir} 디렉토리가 존재하지 않습니다.\")\n",
        "\n",
        "        return found\n",
        "\n",
        "    def stt_model(self, test_file):\n",
        "        self.model.eval()\n",
        "        self.transcription = self.inference_stt(test_file)\n",
        "\n",
        "    def start_stt(self, test_file: np.ndarray) -> Any:\n",
        "        self.check_cuda()\n",
        "        self.find_pt(\"/content/drive/MyDrive/model\")   # pt 확장자로 된 pt파일 이름을 넣는 것이 아닌 pt 파일이 존재하는 디렉토리 이름을 넣는다.\n",
        "        self.stt_model(test_file)\n",
        "        return self.transcription\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stt = sttmodel()"
      ],
      "metadata": {
        "id": "erRl-RWyfgQ-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from typing import Any, Text\n",
        "\n",
        "class sttmodel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(sttmodel, self).__init__()\n",
        "        # 기본 모델 구조만 초기화하고, 가중치는 나중에 로드\n",
        "        self.model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "        self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "        self.device = None\n",
        "        self.transcription = None\n",
        "        self.sr = 16000\n",
        "\n",
        "    def inference_stt(self, data):\n",
        "        audio_array = data\n",
        "        input_features = self.processor.feature_extractor(\n",
        "            audio_array,\n",
        "            sampling_rate = self.sr,\n",
        "            return_tensors = \"pt\"\n",
        "        ).input_features\n",
        "        input_features = input_features.to(self.device)\n",
        "\n",
        "        forced_decoder_ids = self.processor.get_decoder_prompt_ids(language=\"korean\", task=\"transcribe\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted_ids = self.model.generate(\n",
        "                input_features,\n",
        "                forced_decoder_ids=forced_decoder_ids,\n",
        "                language=\"ko\",  # 한국어 지정\n",
        "                task=\"transcribe\"\n",
        "            )\n",
        "        transcription = self.processor.tokenizer.batch_decode(\n",
        "            predicted_ids,\n",
        "            skip_special_tokens=True\n",
        "        )[0]\n",
        "\n",
        "        return transcription\n",
        "\n",
        "    def check_cuda(self,):\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = torch.device(\"cuda:0\")\n",
        "            print(\"GPU 사용\")\n",
        "        else:\n",
        "            self.device = torch.device(\"cpu\")\n",
        "            print(\"CPU 사용\")\n",
        "\n",
        "    def load_pt(self, pt_file: str):\n",
        "        try:\n",
        "            print(f\"모델 파일 로드 시도: {pt_file}\")\n",
        "            state_dict = torch.load(pt_file, map_location=self.device)\n",
        "            self.model.load_state_dict(state_dict)\n",
        "            print(f\"모델 파일 성공적으로 로드됨: {pt_file}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"모델 파일 에러: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def find_pt(self, model_dir):\n",
        "        found = False\n",
        "        if os.path.exists(model_dir):\n",
        "            print(f\"{model_dir} 디렉토리 존재함\")\n",
        "            # 먼저 디렉토리 내용 출력\n",
        "            print(f\"디렉토리 내용: {os.listdir(model_dir)}\")\n",
        "\n",
        "            for root, _, files in os.walk(model_dir):\n",
        "                for file in files:\n",
        "                    if file.endswith(\".pt\"):\n",
        "                        full_path = os.path.join(root, file)\n",
        "                        print(f\"모델 파일 발견: {full_path}\")\n",
        "                        if self.load_pt(full_path):\n",
        "                            found = True\n",
        "                            break\n",
        "                if found:\n",
        "                    break\n",
        "        else:\n",
        "            print(f\"{model_dir} 디렉토리가 존재하지 않습니다.\")\n",
        "\n",
        "        return found\n",
        "\n",
        "    def stt_model(self, test_file):\n",
        "        self.model.eval()\n",
        "        self.transcription = self.inference_stt(test_file)\n",
        "\n",
        "    def start_stt(self, test_file: np.ndarray) -> Any:\n",
        "        self.check_cuda()\n",
        "        # 절대 경로로 명확히 지정\n",
        "        model_path = \"/content/drive/MyDrive/model\"\n",
        "        # 모델 로드 시도\n",
        "        model_loaded = self.find_pt(model_path)\n",
        "\n",
        "        if not model_loaded:\n",
        "            raise ValueError(\"학습된 모델 파일을 찾거나 로드할 수 없습니다.\")\n",
        "\n",
        "        self.stt_model(test_file)\n",
        "        return self.transcription"
      ],
      "metadata": {
        "id": "Hhuulx6arLh7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnTL5VLOf_vz",
        "outputId": "15ac1c9e-fbfe-4de4-bd15-4052ac3125d4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_path = '/content/drive/MyDrive/KsponSpeech_data/10.한국어음성/KsponSpeech_data_unzip_data/KsponSpeech_01/KsponSpeech_0001/KsponSpeech_000002.pcm'\n",
        "\n",
        "with open(file_path, 'rb') as f:\n",
        "    pcm_data = f.read()"
      ],
      "metadata": {
        "id": "trB6OXHhoQtg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# PCM 파일을 NumPy 배열로 변환\n",
        "pcm_data_np = np.frombuffer(pcm_data, dtype=np.int16).astype(np.float32) / 32768.0\n",
        "stt = sttmodel().to('cuda')\n",
        "stt.start_stt(pcm_data_np)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "o0F5FWc8peqP",
        "outputId": "9bca200a-0d55-42f9-e500-70a780e5d1ec"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 사용\n",
            "/content/drive/MyDrive/model 디렉토리 존재함\n",
            "디렉토리 내용: ['trainedModel.pt', 'example.pcm', '.ipynb_checkpoints']\n",
            "모델 파일 발견: /content/drive/MyDrive/model/trainedModel.pt\n",
            "모델 파일 로드 시도: /content/drive/MyDrive/model/trainedModel.pt\n",
            "모델 파일 성공적으로 로드됨: /content/drive/MyDrive/model/trainedModel.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'나는 악습은 원래 없어진다, 없어져야 된다고 생각하긴 했는데 근데 그에 약간 필요악으로 하나 정도쯤은 있어야 되거든. 물 뜨러 가고. bd'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# 현재 작업 디렉토리 확인\n",
        "print(\"현재 디렉토리:\", os.getcwd())\n",
        "# 'model' 디렉토리가 있는지 확인\n",
        "print(\"model 디렉토리 존재 여부:\", os.path.exists(\"model\"))\n",
        "# 'model' 디렉토리에 무슨 파일이 있는지 확인\n",
        "if os.path.exists(\"model\"):\n",
        "    print(\"model 디렉토리 내용:\", os.listdir(\"model\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2S4dUJRfs1o",
        "outputId": "8377b4f6-f835-447b-f4bd-c24e17fce491"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "현재 디렉토리: /content/drive/MyDrive\n",
            "model 디렉토리 존재 여부: True\n",
            "model 디렉토리 내용: ['trainedModel.pt', 'example.pcm', '.ipynb_checkpoints']\n"
          ]
        }
      ]
    }
  ]
}