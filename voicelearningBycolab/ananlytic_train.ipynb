{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "793d446c-926e-4b0b-9c60-6ae52463f96b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ KsponSpeech_01 5:5 í˜¼í•© í•™ìŠµ\n",
      "ğŸš€ KsponSpeech_01 5:5 í˜¼í•© í•™ìŠµ ì‹œì‘\n",
      "ğŸ¯ KsponSpeech_01 5:5 í˜¼í•© ë°ì´í„°ì…‹ ìƒì„±\n",
      "ğŸ” KsponSpeech_01 íŒŒì¼ ìˆ˜ì§‘ ì¤‘...\n",
      "  ğŸ“ KsponSpeech_0001 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 997ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0002 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 160ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0003 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0004 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0005 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0006 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 37ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0007 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0008 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 937ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0009 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 834ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0010 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 376ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0011 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0012 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 961ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0013 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0014 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 861ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0015 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 65ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0016 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0017 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0018 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 484ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "ğŸ“Š ì´ 12713ê°œ íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ\n",
      "ğŸ“‹ ì›ë³¸ í…ìŠ¤íŠ¸: 6356ê°œ, ë°œìŒ í…ìŠ¤íŠ¸: 6357ê°œ\n",
      "ğŸ“Š ì´ 12713ê°œ - ì›ë³¸: 6356ê°œ, ë°œìŒ: 6357ê°œ\n",
      "ğŸ”„ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12713/12713 [00:06<00:00, 2074.32 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12713/12713 [00:39<00:00, 321.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: 12713ê°œ\n",
      "ğŸ“Š í•™ìŠµ: 10170ê°œ, ê²€ì¦: 1271ê°œ\n",
      "ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                      | 3004/10170 [10:20<24:39,  4.84 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 320\u001b[0m\n\u001b[1;32m    317\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# í•™ìŠµ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_mixed_5_5_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ í•™ìŠµ ì‹¤íŒ¨\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 255\u001b[0m, in \u001b[0;36mtrain_mixed_5_5_whisper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m     labels \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    244\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m448\u001b[39m,\n\u001b[1;32m    247\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     )\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_features,\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels\n\u001b[1;32m    253\u001b[0m     }\n\u001b[0;32m--> 255\u001b[0m processed_datasets \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[1;32m    256\u001b[0m     split: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m    257\u001b[0m         prepare_dataset,\n\u001b[1;32m    258\u001b[0m         remove_columns\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split, dataset \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    261\u001b[0m })\n\u001b[1;32m    263\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorSpeechSeq2SeqWithPadding(processor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# 5. í›ˆë ¨ ì„¤ì •\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 256\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    243\u001b[0m     labels \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    244\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    245\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    246\u001b[0m         max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m448\u001b[39m,\n\u001b[1;32m    247\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    248\u001b[0m     )\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_features,\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels\n\u001b[1;32m    253\u001b[0m     }\n\u001b[1;32m    255\u001b[0m processed_datasets \u001b[38;5;241m=\u001b[39m DatasetDict({\n\u001b[0;32m--> 256\u001b[0m     split: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprepare_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split, dataset \u001b[38;5;129;01min\u001b[39;00m datasets\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    261\u001b[0m })\n\u001b[1;32m    263\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorSpeechSeq2SeqWithPadding(processor\u001b[38;5;241m=\u001b[39mprocessor)\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# 5. í›ˆë ¨ ì„¤ì •\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3428\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3426\u001b[0m _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m-> 3428\u001b[0m     example \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m update_data:\n\u001b[1;32m   3430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3324\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[22], line 233\u001b[0m, in \u001b[0;36mtrain_mixed_5_5_whisper.<locals>.prepare_dataset\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m audio \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 233\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39minput_features[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_features, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    239\u001b[0m     input_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(input_features)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/feature_extraction_whisper.py:306\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor.__call__\u001b[0;34m(self, raw_speech, truncation, pad_to_multiple_of, return_tensors, return_attention_mask, padding, max_length, sampling_rate, do_normalize, device, return_token_timestamps, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m input_features \u001b[38;5;241m=\u001b[39m padded_inputs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    303\u001b[0m extract_fbank_features \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_torch_extract_fbank_features \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_np_extract_fbank_features\n\u001b[1;32m    305\u001b[0m )\n\u001b[0;32m--> 306\u001b[0m input_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_fbank_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_features[\u001b[38;5;241m0\u001b[39m], List):\n\u001b[1;32m    309\u001b[0m     padded_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_features\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39masarray(feature, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m input_features]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/whisper/feature_extraction_whisper.py:139\u001b[0m, in \u001b[0;36mWhisperFeatureExtractor._torch_extract_fbank_features\u001b[0;34m(self, waveform, device)\u001b[0m\n\u001b[1;32m    137\u001b[0m     window \u001b[38;5;241m=\u001b[39m window\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    138\u001b[0m stft \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstft(waveform, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_fft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhop_length, window\u001b[38;5;241m=\u001b[39mwindow, return_complex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 139\u001b[0m magnitudes \u001b[38;5;241m=\u001b[39m \u001b[43mstft\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m    141\u001b[0m mel_filters \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_filters)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ë°ì´í„° ì ˆë°˜ì”©í•˜ì—¬ (ë³€í™˜ í…ìŠ¤íŠ¸, no ë³€í™˜ í…ìŠ¤íŠ¸) -> ê²°ê³¼ ì²´í¬\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import json\n",
    "import random\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì›)\"\"\"\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return \"í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨\"\n",
    "\n",
    "def get_kspon01_files():\n",
    "    \"\"\"KsponSpeech_01 ëª¨ë“  íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    print(\"ğŸ” KsponSpeech_01 íŒŒì¼ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    \n",
    "    # ê²½ë¡œ ì„¤ì • - ìˆ˜ì •ëœ ê²½ë¡œ\n",
    "    base_audio_dir = \"PreprocessData/KsponSpeech_01\"      # NPY íŒŒì¼ë“¤\n",
    "    base_text_dir = \"TrainData/unzipped_Speech/KsponSpeech_01\"  # ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "    base_g2p_dir = \"PreprocessData/KsponSpeech_01\"        # G2P í…ìŠ¤íŠ¸ (ì—¬ê¸°ì— ìˆì—ˆìŒ!)\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    # KsponSpeech_0001 ~ KsponSpeech_0018 í´ë”ë“¤ ì²˜ë¦¬\n",
    "    for i in range(1, 19):\n",
    "        if i < 10:\n",
    "            folder_name = f\"KsponSpeech_000{i}\"\n",
    "        else:\n",
    "            folder_name = f\"KsponSpeech_00{i}\"\n",
    "        \n",
    "        print(f\"  ğŸ“ {folder_name} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        # í´ë” ê²½ë¡œë“¤\n",
    "        audio_folder = os.path.join(base_audio_dir, folder_name)\n",
    "        original_folder = os.path.join(base_text_dir, folder_name)\n",
    "        g2p_folder = os.path.join(base_g2p_dir, f\"{folder_name}_g2p\")  # PreprocessDataì—ì„œ ì°¾ê¸°\n",
    "        \n",
    "        # í´ë” ì¡´ì¬ í™•ì¸\n",
    "        if not all([os.path.exists(audio_folder), os.path.exists(original_folder), os.path.exists(g2p_folder)]):\n",
    "            print(f\"    âš ï¸ í´ë” ì—†ìŒ: ì˜¤ë””ì˜¤={os.path.exists(audio_folder)}, ì›ë³¸={os.path.exists(original_folder)}, G2P={os.path.exists(g2p_folder)}\")\n",
    "            continue\n",
    "        \n",
    "        # NPY íŒŒì¼ë“¤ ì°¾ê¸°\n",
    "        npy_files = glob.glob(os.path.join(audio_folder, \"*_combined_features.npy\"))\n",
    "        \n",
    "        folder_files = []\n",
    "        for npy_file in npy_files:\n",
    "            # íŒŒì¼ëª…ì—ì„œ ê¸°ë³¸ ì´ë¦„ ì¶”ì¶œ\n",
    "            base_name = os.path.basename(npy_file).replace('_combined_features.npy', '')\n",
    "            \n",
    "            # ëŒ€ì‘ë˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼ë“¤\n",
    "            original_txt = os.path.join(original_folder, f\"{base_name}.txt\")\n",
    "            g2p_txt = os.path.join(g2p_folder, f\"{base_name}.txt\")\n",
    "            \n",
    "            # ëª¨ë“  íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸\n",
    "            if all([os.path.exists(npy_file), os.path.exists(original_txt), os.path.exists(g2p_txt)]):\n",
    "                folder_files.append({\n",
    "                    'audio': npy_file,\n",
    "                    'original_txt': original_txt,\n",
    "                    'g2p_txt': g2p_txt,\n",
    "                    'base_name': base_name,\n",
    "                    'folder': folder_name\n",
    "                })\n",
    "        \n",
    "        print(f\"    âœ… {len(folder_files)}ê°œ íŒŒì¼ ë§¤ì¹­ë¨\")\n",
    "        all_files.extend(folder_files)\n",
    "    \n",
    "    print(f\"ğŸ“Š ì´ {len(all_files)}ê°œ íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "    return all_files\n",
    "\n",
    "def create_5_5_mixed_dataset():\n",
    "    \"\"\"5:5 ë¹„ìœ¨ë¡œ í˜¼í•© ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    print(\"ğŸ¯ KsponSpeech_01 5:5 í˜¼í•© ë°ì´í„°ì…‹ ìƒì„±\")\n",
    "    \n",
    "    # ëª¨ë“  íŒŒì¼ ìˆ˜ì§‘\n",
    "    all_files = get_kspon01_files()\n",
    "    \n",
    "    if len(all_files) == 0:\n",
    "        print(\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "    \n",
    "    # íŒŒì¼ë“¤ì„ ì…”í”Œë§\n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    # 5:5ë¡œ ë¶„í• \n",
    "    split_point = len(all_files) // 2\n",
    "    original_files = all_files[:split_point]\n",
    "    g2p_files = all_files[split_point:]\n",
    "    \n",
    "    print(f\"ğŸ“‹ ì›ë³¸ í…ìŠ¤íŠ¸: {len(original_files)}ê°œ, ë°œìŒ í…ìŠ¤íŠ¸: {len(g2p_files)}ê°œ\")\n",
    "    \n",
    "    # ë°ì´í„°ì…‹ êµ¬ì„±\n",
    "    data = {\"audio\": [], \"text\": [], \"text_type\": []}\n",
    "    \n",
    "    # ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€\n",
    "    for file_info in original_files:\n",
    "        try:\n",
    "            original_text = load_text(file_info['original_txt'])\n",
    "            if original_text != \"í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨\":\n",
    "                data[\"audio\"].append(file_info['audio'])\n",
    "                data[\"text\"].append(original_text)\n",
    "                data[\"text_type\"].append(\"original\")\n",
    "        except Exception as e:\n",
    "            print(f\"ì›ë³¸ í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # ë°œìŒ í…ìŠ¤íŠ¸ ë°ì´í„° ì¶”ê°€\n",
    "    for file_info in g2p_files:\n",
    "        try:\n",
    "            g2p_text = load_text(file_info['g2p_txt'])\n",
    "            if g2p_text != \"í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨\":\n",
    "                data[\"audio\"].append(file_info['audio'])\n",
    "                data[\"text\"].append(g2p_text)\n",
    "                data[\"text_type\"].append(\"g2p\")\n",
    "        except Exception as e:\n",
    "            print(f\"ë°œìŒ í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # ìµœì¢… í†µê³„\n",
    "    total_count = len(data['audio'])\n",
    "    original_count = data['text_type'].count('original')\n",
    "    g2p_count = data['text_type'].count('g2p')\n",
    "    \n",
    "    print(f\"ğŸ“Š ì´ {total_count}ê°œ - ì›ë³¸: {original_count}ê°œ, ë°œìŒ: {g2p_count}ê°œ\")\n",
    "    \n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def map_to_array(batch):\n",
    "    \"\"\"NPY íŒŒì¼ì„ ì˜¤ë””ì˜¤ ë°°ì—´ë¡œ ë³€í™˜\"\"\"\n",
    "    arrays = []\n",
    "    rates = []\n",
    "\n",
    "    for audio_path in batch[\"audio\"]:\n",
    "        try:\n",
    "            audio_array = np.load(audio_path)\n",
    "            \n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(16000)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë””ì˜¤ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "            continue\n",
    "\n",
    "    batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "    return batch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def train_mixed_5_5_whisper():\n",
    "    \"\"\"5:5 í˜¼í•© ë°ì´í„°ë¡œ Whisper ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    output_dir = \"whisper_mixed_5_5_finetuned\"\n",
    "    \n",
    "    print(\"ğŸš€ KsponSpeech_01 5:5 í˜¼í•© í•™ìŠµ ì‹œì‘\")\n",
    "    \n",
    "    # 1. í˜¼í•© ë°ì´í„°ì…‹ ìƒì„±\n",
    "    dataset = create_5_5_mixed_dataset()\n",
    "    \n",
    "    if dataset is None or len(dataset) == 0:\n",
    "        print(\"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    print(\"ğŸ”„ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"audio\"]) > 0)\n",
    "    \n",
    "    print(f\"âœ… ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ\")\n",
    "\n",
    "    # 3. ë°ì´í„°ì…‹ ë¶„í• \n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    print(f\"ğŸ“Š í•™ìŠµ: {len(datasets['train'])}ê°œ, ê²€ì¦: {len(datasets['validation'])}ê°œ\")\n",
    "\n",
    "    # 4. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "    print(\"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        audio = batch[\"audio\"]\n",
    "        \n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        \n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "        \n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=448,\n",
    "            padding=False\n",
    "        ).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    # 5. í›ˆë ¨ ì„¤ì •\n",
    "    device_capability = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    supports_bf16 = device_capability >= (8, 0)\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=1e-6,\n",
    "        warmup_steps=300,\n",
    "        max_steps=4000,\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=supports_bf16,\n",
    "        fp16=False if supports_bf16 else True,\n",
    "        dataloader_pin_memory=False,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=400,\n",
    "        save_steps=400,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=3,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        eval_dataset=processed_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"ğŸš€ í•™ìŠµ ì‹œì‘...\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ¯ KsponSpeech_01 5:5 í˜¼í•© í•™ìŠµ\")\n",
    "    \n",
    "    # ì‹œë“œ ì„¤ì •\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    model, processor = train_mixed_5_5_whisper()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"âŒ í•™ìŠµ ì‹¤íŒ¨\")\n",
    "    else:\n",
    "        print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "034ff3dd-697b-406c-ba58-a838b9030146",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData G2P í´ë” ìƒì„¸ í™•ì¸:\n",
      "G2P í´ë” 18ê°œ:\n",
      "  KsponSpeech_0001_g2p: 1000ê°œ íŒŒì¼\n",
      "  KsponSpeech_0002_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0003_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0004_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0005_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0006_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0007_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0008_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0009_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0010_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0011_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0012_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0013_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0014_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0015_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0016_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0017_g2p: 0ê°œ íŒŒì¼\n",
      "  KsponSpeech_0018_g2p: 0ê°œ íŒŒì¼\n",
      "\n",
      "ì´ G2P í…ìŠ¤íŠ¸ íŒŒì¼: 1000ê°œ\n",
      "\n",
      "ë§¤ì¹­ ê°€ëŠ¥í•œ ë°ì´í„° í™•ì¸:\n",
      "  KsponSpeech_0001: ì˜¤ë””ì˜¤=997, ì›ë³¸=1000, G2P=1000 â†’ ë§¤ì¹­=997\n",
      "\n",
      "ì‹¤ì œ ë§¤ì¹­ ê°€ëŠ¥í•œ ì „ì²´ ë°ì´í„°: 997ê°œ\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0c957f-cd22-4a4f-a223-a7aa70d9c646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ 100% ë°œìŒ í…ìŠ¤íŠ¸ Whisper í•™ìŠµ (êµ¬ì¡° ë³´ì¡´ + CER/WER)\n",
      "ğŸš€ 100% ë°œìŒ í…ìŠ¤íŠ¸ Whisper í•™ìŠµ ì‹œì‘\n",
      "ğŸ¯ ëª©í‘œ: êµ¬ì¡° ë³´ì¡´ + ë°œìŒ íŒ¨í„´ í•™ìŠµ\n",
      "ğŸ¯ 100% ë°œìŒ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± (êµ¬ì¡° ë³´ì¡´ ê°•í™”)\n",
      "ğŸ” KsponSpeech_01 íŒŒì¼ ìˆ˜ì§‘ ì¤‘...\n",
      "  ğŸ“ KsponSpeech_0001 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 997ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0002 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 160ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0003 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0004 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0005 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0006 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 37ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0007 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0008 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 937ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0009 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 834ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0010 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 376ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0011 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0012 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 961ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0013 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0014 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 861ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0015 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 65ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0016 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0017 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 1000ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "  ğŸ“ KsponSpeech_0018 ì²˜ë¦¬ ì¤‘...\n",
      "    âœ… 484ê°œ íŒŒì¼ ë§¤ì¹­ë¨\n",
      "ğŸ“Š ì´ 12713ê°œ íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ\n",
      "ğŸ“Š ì´ 12713ê°œ ë°œìŒ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\n",
      "ğŸ”„ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12713/12713 [00:05<00:00, 2127.90 examples/s]\n",
      "Filter:  16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                 | 2000/12713 [00:06<00:35, 301.80 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 485\u001b[0m\n\u001b[1;32m    482\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# í•™ìŠµ ì‹¤í–‰\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m model, processor \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_phonetic_whisper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâŒ í•™ìŠµ ì‹¤íŒ¨\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 282\u001b[0m, in \u001b[0;36mtrain_phonetic_whisper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ”„ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    281\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(map_to_array, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[0;32m--> 282\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mê°œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# 3. ë°ì´í„°ì…‹ ë¶„í• \u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3670\u001b[0m, in \u001b[0;36mDataset.filter\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3668\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m-> 3670\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_indices_from_mask_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3677\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3678\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mindices\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mValue\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muint64\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3692\u001b[0m \u001b[43m    \u001b[49m\u001b[43msuffix_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffix_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3694\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFilter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3697\u001b[0m new_dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   3698\u001b[0m new_dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    558\u001b[0m }\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3055\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3050\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3051\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3052\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3053\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3054\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3055\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3056\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3057\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3458\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3454\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3455\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3456\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3457\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3458\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3462\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3465\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3466\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3467\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:3320\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3319\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3320\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3322\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3323\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3324\u001b[0m     }\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/arrow_dataset.py:6301\u001b[0m, in \u001b[0;36mget_indices_from_mask_function\u001b[0;34m(function, batched, with_indices, with_rank, input_columns, indices_mapping, *args, **fn_kwargs)\u001b[0m\n\u001b[1;32m   6298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   6299\u001b[0m     \u001b[38;5;66;03m# inputs only contains a batch of examples\u001b[39;00m\n\u001b[1;32m   6300\u001b[0m     batch: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 6301\u001b[0m     num_examples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m   6302\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_examples):\n\u001b[1;32m   6303\u001b[0m         example \u001b[38;5;241m=\u001b[39m {key: batch[key][i] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch}\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:279\u001b[0m, in \u001b[0;36mLazyDict.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    277\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key]\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format:\n\u001b[0;32m--> 279\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys_to_format\u001b[38;5;241m.\u001b[39mremove(key)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:382\u001b[0m, in \u001b[0;36mLazyBatch.format\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_column\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:448\u001b[0m, in \u001b[0;36mPythonFormatter.format_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 448\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m     column \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_column(column, pa_table\u001b[38;5;241m.\u001b[39mcolumn_names[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m column\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/datasets/formatting/formatting.py:148\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_column\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mextract_column\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pylist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/table.pxi:1335\u001b[0m, in \u001b[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/array.pxi:1650\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyarrow/scalar.pxi:793\u001b[0m, in \u001b[0;36mpyarrow.lib.StructScalar.as_py\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/lib/python3.8/_collections_abc.py:720\u001b[0m, in \u001b[0;36mKeysView.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 720\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mapping\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "ì‹¤ì‹œê°„ CER/WER 300ìŠ¤í…ë§ˆë‹¤ ì¶œë ¥\n",
    "ìµœì¢… í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìë™ ì €ì¥\n",
    "PT íŒŒì¼ ì¬ì‚¬ìš©ì„ ìœ„í•œ ì €ì¥\n",
    "êµ¬ì¡° ë³´ì¡´ ê°•í™”ëœ ìƒì„± ì„¤ì •\n",
    "'''\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from jiwer import wer, cer\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def load_text(file_path):\n",
    "    \"\"\"í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì§€ì›)\"\"\"\n",
    "    encodings = ['utf-8', 'cp949', 'euc-kr']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                text = f.read().strip()\n",
    "            return text\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return \"í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨\"\n",
    "\n",
    "def preprocess_g2p_text(text):\n",
    "    \"\"\"êµ¬ì¡° ë³´ì¡´ì„ ìœ„í•œ G2P í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\"\"\"\n",
    "    # ê³µë°± ì •ë¦¬\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # ë¬¸ì¥ ê²½ê³„ ëª…ì‹œí™” (ì„ íƒì )\n",
    "    # text = re.sub(r'([.!?])', r'\\1 /', text)\n",
    "    \n",
    "    # ë°˜ë³µë˜ëŠ” íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬\n",
    "    text = re.sub(r'([+*/])\\1+', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def get_kspon01_files():\n",
    "    \"\"\"KsponSpeech_01 ëª¨ë“  íŒŒì¼ë“¤ ê°€ì ¸ì˜¤ê¸°\"\"\"\n",
    "    print(\"ğŸ” KsponSpeech_01 íŒŒì¼ ìˆ˜ì§‘ ì¤‘...\")\n",
    "    \n",
    "    base_audio_dir = \"PreprocessData/KsponSpeech_01\"\n",
    "    base_text_dir = \"TrainData/unzipped_Speech/KsponSpeech_01\"\n",
    "    base_g2p_dir = \"PreprocessData/KsponSpeech_01\"\n",
    "    \n",
    "    all_files = []\n",
    "    \n",
    "    for i in range(1, 19):\n",
    "        folder_name = f\"KsponSpeech_000{i}\" if i < 10 else f\"KsponSpeech_00{i}\"\n",
    "        print(f\"  ğŸ“ {folder_name} ì²˜ë¦¬ ì¤‘...\")\n",
    "        \n",
    "        audio_folder = os.path.join(base_audio_dir, folder_name)\n",
    "        original_folder = os.path.join(base_text_dir, folder_name)\n",
    "        g2p_folder = os.path.join(base_g2p_dir, f\"{folder_name}_g2p\")\n",
    "        \n",
    "        if not all([os.path.exists(audio_folder), os.path.exists(original_folder), os.path.exists(g2p_folder)]):\n",
    "            print(f\"    âš ï¸ í´ë” ì—†ìŒ\")\n",
    "            continue\n",
    "        \n",
    "        npy_files = glob.glob(os.path.join(audio_folder, \"*_combined_features.npy\"))\n",
    "        \n",
    "        folder_files = []\n",
    "        for npy_file in npy_files:\n",
    "            base_name = os.path.basename(npy_file).replace('_combined_features.npy', '')\n",
    "            original_txt = os.path.join(original_folder, f\"{base_name}.txt\")\n",
    "            g2p_txt = os.path.join(g2p_folder, f\"{base_name}.txt\")\n",
    "            \n",
    "            if all([os.path.exists(npy_file), os.path.exists(original_txt), os.path.exists(g2p_txt)]):\n",
    "                folder_files.append({\n",
    "                    'audio': npy_file,\n",
    "                    'original_txt': original_txt,\n",
    "                    'g2p_txt': g2p_txt,\n",
    "                    'base_name': base_name,\n",
    "                    'folder': folder_name\n",
    "                })\n",
    "        \n",
    "        print(f\"    âœ… {len(folder_files)}ê°œ íŒŒì¼ ë§¤ì¹­ë¨\")\n",
    "        all_files.extend(folder_files)\n",
    "    \n",
    "    print(f\"ğŸ“Š ì´ {len(all_files)}ê°œ íŒŒì¼ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
    "    return all_files\n",
    "\n",
    "def create_phonetic_only_dataset():\n",
    "    \"\"\"100% ë°œìŒ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±\"\"\"\n",
    "    print(\"ğŸ¯ 100% ë°œìŒ í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± (êµ¬ì¡° ë³´ì¡´ ê°•í™”)\")\n",
    "    \n",
    "    all_files = get_kspon01_files()\n",
    "    if len(all_files) == 0:\n",
    "        return None\n",
    "    \n",
    "    random.shuffle(all_files)\n",
    "    \n",
    "    data = {\"audio\": [], \"text\": [], \"original_text\": []}\n",
    "    \n",
    "    # 100% G2P í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš©\n",
    "    for file_info in all_files:\n",
    "        try:\n",
    "            g2p_text = load_text(file_info['g2p_txt'])\n",
    "            original_text = load_text(file_info['original_txt'])\n",
    "            \n",
    "            if g2p_text != \"í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨\" and original_text != \"í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨\":\n",
    "                # G2P í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (êµ¬ì¡° ë³´ì¡´)\n",
    "                processed_g2p = preprocess_g2p_text(g2p_text)\n",
    "                \n",
    "                data[\"audio\"].append(file_info['audio'])\n",
    "                data[\"text\"].append(processed_g2p)  # í•™ìŠµìš© (ë°œìŒ í…ìŠ¤íŠ¸)\n",
    "                data[\"original_text\"].append(original_text)  # í‰ê°€ìš© (ì›ë³¸ í…ìŠ¤íŠ¸)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    total_count = len(data['audio'])\n",
    "    print(f\"ğŸ“Š ì´ {total_count}ê°œ ë°œìŒ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±\")\n",
    "    \n",
    "    return Dataset.from_dict(data)\n",
    "\n",
    "def map_to_array(batch):\n",
    "    \"\"\"NPY íŒŒì¼ì„ ì˜¤ë””ì˜¤ ë°°ì—´ë¡œ ë³€í™˜\"\"\"\n",
    "    arrays = []\n",
    "    rates = []\n",
    "\n",
    "    for audio_path in batch[\"audio\"]:\n",
    "        try:\n",
    "            audio_array = np.load(audio_path)\n",
    "            \n",
    "            if audio_array.dtype in [np.int16, np.int8]:\n",
    "                max_value = float(2 ** (15 if audio_array.dtype == np.int16 else 7))\n",
    "                audio_array = audio_array.astype(np.float32) / max_value\n",
    "            elif audio_array.dtype != np.float32:\n",
    "                audio_array = audio_array.astype(np.float32)\n",
    "            \n",
    "            arrays.append(audio_array)\n",
    "            rates.append(16000)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ì˜¤ë””ì˜¤ ë¡œë“œ ì˜¤ë¥˜: {e}\")\n",
    "            continue\n",
    "\n",
    "    batch[\"audio\"] = [{\"array\": arr, \"sampling_rate\": sr} for arr, sr in zip(arrays, rates)]\n",
    "    return batch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: WhisperProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            {\"input_features\": [feature[\"input_features\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": [feature[\"labels\"] for feature in features]},\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
    "            labels_batch[\"input_ids\"] == self.processor.tokenizer.pad_token_id,\n",
    "            -100\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "def compute_metrics(eval_pred, processor, original_texts):\n",
    "    \"\"\"CER/WER ê³„ì‚°\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # ë¼ë²¨ì—ì„œ -100ì„ ì œê±°\n",
    "    labels = np.where(labels != -100, labels, processor.tokenizer.pad_token_id)\n",
    "    \n",
    "    # ë””ì½”ë”©\n",
    "    decoded_preds = processor.tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = processor.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # CER/WER ê³„ì‚° (ì˜¤ë¥˜ ì²˜ë¦¬ í¬í•¨)\n",
    "    try:\n",
    "        # ë¹ˆ ì˜ˆì¸¡ ì²˜ë¦¬\n",
    "        filtered_preds = []\n",
    "        filtered_labels = []\n",
    "        \n",
    "        for pred, label in zip(decoded_preds, decoded_labels):\n",
    "            if pred.strip() and label.strip():\n",
    "                filtered_preds.append(pred.strip())\n",
    "                filtered_labels.append(label.strip())\n",
    "        \n",
    "        if len(filtered_preds) > 0:\n",
    "            cer_score = cer(filtered_labels, filtered_preds) * 100\n",
    "            wer_score = wer(filtered_labels, filtered_preds) * 100\n",
    "        else:\n",
    "            cer_score = 100.0\n",
    "            wer_score = 100.0\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ë©”íŠ¸ë¦­ ê³„ì‚° ì˜¤ë¥˜: {e}\")\n",
    "        cer_score = 100.0\n",
    "        wer_score = 100.0\n",
    "    \n",
    "    return {\n",
    "        \"cer\": cer_score,\n",
    "        \"wer\": wer_score\n",
    "    }\n",
    "\n",
    "class PhoneticWhisperTrainer(Seq2SeqTrainer):\n",
    "    \"\"\"CER/WER í‰ê°€ê°€ í¬í•¨ëœ ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ\"\"\"\n",
    "    \n",
    "    def __init__(self, original_texts=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.original_texts = original_texts\n",
    "        \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        \"\"\"í‰ê°€ ì‹œ CER/WER ê³„ì‚°\"\"\"\n",
    "        print(\"ğŸ“Š CER/WER í‰ê°€ ì¤‘...\")\n",
    "        \n",
    "        eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
    "        \n",
    "        # ê¸°ë³¸ í‰ê°€\n",
    "        output = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # CER/WER ê³„ì‚°ì„ ìœ„í•œ ìƒ˜í”Œ ì˜ˆì¸¡\n",
    "        try:\n",
    "            eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "            sample_batch = next(iter(eval_dataloader))\n",
    "            \n",
    "            # GPUë¡œ ì´ë™\n",
    "            if torch.cuda.is_available():\n",
    "                sample_batch = {k: v.cuda() if isinstance(v, torch.Tensor) else v \n",
    "                              for k, v in sample_batch.items()}\n",
    "            \n",
    "            # ì˜ˆì¸¡ ìƒì„±\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                generated_ids = self.model.generate(\n",
    "                    sample_batch[\"input_features\"],\n",
    "                    max_length=448,\n",
    "                    num_beams=2,\n",
    "                    repetition_penalty=2.5,  # êµ¬ì¡° ë³´ì¡´ ê°•í™”\n",
    "                    no_repeat_ngram_size=4,\n",
    "                    length_penalty=1.2,\n",
    "                    early_stopping=True,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "            \n",
    "            # ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "            metrics = compute_metrics(\n",
    "                (generated_ids.cpu().numpy(), sample_batch[\"labels\"].cpu().numpy()),\n",
    "                self.tokenizer,\n",
    "                self.original_texts\n",
    "            )\n",
    "            \n",
    "            # ê²°ê³¼ ì¶”ê°€\n",
    "            output.update({f\"{metric_key_prefix}_{k}\": v for k, v in metrics.items()})\n",
    "            \n",
    "            print(f\"ğŸ“ˆ CER: {metrics['cer']:.2f}%, WER: {metrics['wer']:.2f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ CER/WER ê³„ì‚° ì˜¤ë¥˜: {e}\")\n",
    "        \n",
    "        return output\n",
    "\n",
    "def train_phonetic_whisper():\n",
    "    \"\"\"100% ë°œìŒ í…ìŠ¤íŠ¸ë¡œ Whisper ëª¨ë¸ í•™ìŠµ\"\"\"\n",
    "    output_dir = \"whisper_phonetic_only_finetuned\"\n",
    "    pt_save_path = \"whisper_phonetic_model.pt\"\n",
    "    \n",
    "    print(\"ğŸš€ 100% ë°œìŒ í…ìŠ¤íŠ¸ Whisper í•™ìŠµ ì‹œì‘\")\n",
    "    print(\"ğŸ¯ ëª©í‘œ: êµ¬ì¡° ë³´ì¡´ + ë°œìŒ íŒ¨í„´ í•™ìŠµ\")\n",
    "    \n",
    "    # 1. ë°œìŒ ì „ìš© ë°ì´í„°ì…‹ ìƒì„±\n",
    "    dataset = create_phonetic_only_dataset()\n",
    "    \n",
    "    if dataset is None or len(dataset) == 0:\n",
    "        print(\"âŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\")\n",
    "        return None, None\n",
    "    \n",
    "    # 2. ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬\n",
    "    print(\"ğŸ”„ ì˜¤ë””ì˜¤ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...\")\n",
    "    dataset = dataset.map(map_to_array, batched=True, batch_size=8)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"audio\"]) > 0)\n",
    "    \n",
    "    print(f\"âœ… ìµœì¢… ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}ê°œ\")\n",
    "\n",
    "    # 3. ë°ì´í„°ì…‹ ë¶„í• \n",
    "    train_test_valid = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    test_valid = train_test_valid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_test_valid[\"train\"],\n",
    "        \"test\": test_valid[\"test\"],\n",
    "        \"validation\": test_valid[\"train\"]\n",
    "    })\n",
    "\n",
    "    print(f\"ğŸ“Š í•™ìŠµ: {len(datasets['train'])}ê°œ, ê²€ì¦: {len(datasets['validation'])}ê°œ, í…ŒìŠ¤íŠ¸: {len(datasets['test'])}ê°œ\")\n",
    "\n",
    "    # 4. ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ\n",
    "    print(\"ğŸ¤– ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        \"openai/whisper-small\", \n",
    "        use_cache=False,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "    \n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "\n",
    "    def prepare_dataset(batch):\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        audio = batch[\"audio\"]\n",
    "        \n",
    "        input_features = processor.feature_extractor(\n",
    "            audio[\"array\"],\n",
    "            sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        \n",
    "        if isinstance(input_features, np.ndarray):\n",
    "            input_features = torch.from_numpy(input_features)\n",
    "        \n",
    "        input_features = input_features.to(model.dtype)\n",
    "        \n",
    "        # ë°œìŒ í…ìŠ¤íŠ¸ë¡œ ë¼ë²¨ ìƒì„±\n",
    "        labels = processor.tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=448,\n",
    "            padding=False\n",
    "        ).input_ids\n",
    "        \n",
    "        return {\n",
    "            \"input_features\": input_features,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "    processed_datasets = DatasetDict({\n",
    "        split: dataset.map(\n",
    "            prepare_dataset,\n",
    "            remove_columns=dataset.column_names\n",
    "        )\n",
    "        for split, dataset in datasets.items()\n",
    "    })\n",
    "\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "    # 5. í›ˆë ¨ ì„¤ì • (êµ¬ì¡° ë³´ì¡´ ê°•í™”)\n",
    "    device_capability = torch.cuda.get_device_capability() if torch.cuda.is_available() else (0, 0)\n",
    "    supports_bf16 = device_capability >= (8, 0)\n",
    "    \n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=5e-7,              # ë§¤ìš° ë‚®ì€ í•™ìŠµë¥  (êµ¬ì¡° ë³´ì¡´)\n",
    "        warmup_steps=500,                # ê¸´ ì›Œë°ì—…\n",
    "        max_steps=3000,                  # ì ì ˆí•œ ìŠ¤í… ìˆ˜\n",
    "        gradient_checkpointing=True,\n",
    "        bf16=supports_bf16,\n",
    "        fp16=False if supports_bf16 else True,\n",
    "        dataloader_pin_memory=False,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=300,                  # 300ìŠ¤í…ë§ˆë‹¤ í‰ê°€ (CER/WER ê³„ì‚°)\n",
    "        save_steps=300,\n",
    "        logging_steps=50,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_cer\", # CER ê¸°ì¤€ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ\n",
    "        greater_is_better=False,\n",
    "        save_total_limit=3,\n",
    "        report_to=[\"tensorboard\"],\n",
    "        push_to_hub=False,\n",
    "        remove_unused_columns=False,\n",
    "        # êµ¬ì¡° ë³´ì¡´ì„ ìœ„í•œ ìƒì„± ì„¤ì •\n",
    "        generation_max_length=448,\n",
    "        generation_num_beams=2,\n",
    "        predict_with_generate=True,\n",
    "    )\n",
    "\n",
    "    # 6. ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ (CER/WER í¬í•¨)\n",
    "    trainer = PhoneticWhisperTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=processed_datasets[\"train\"],\n",
    "        eval_dataset=processed_datasets[\"validation\"],\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        original_texts=[item[\"original_text\"] for item in datasets[\"validation\"]],\n",
    "    )\n",
    "\n",
    "    print(\"ğŸš€ ë°œìŒ ì „ìš© í•™ìŠµ ì‹œì‘...\")\n",
    "    print(\"ğŸ“Š í‰ê°€ ì£¼ê¸°: 300ìŠ¤í…ë§ˆë‹¤ CER/WER ê³„ì‚°\")\n",
    "    trainer.train()\n",
    "\n",
    "    print(\"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...\")\n",
    "    \n",
    "    # 1. Hugging Face í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
    "    trainer.save_model(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    # 2. PT íŒŒì¼ë¡œ ì €ì¥ (ì¬ì‚¬ìš© ìš©ì´)\n",
    "    print(f\"ğŸ’¾ PT íŒŒì¼ ì €ì¥ ì¤‘: {pt_save_path}\")\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'processor_config': processor.to_dict() if hasattr(processor, 'to_dict') else None,\n",
    "        'model_config': model.config.to_dict(),\n",
    "        'training_args': training_args.to_dict(),\n",
    "        'vocab_size': len(processor.tokenizer),\n",
    "        'training_type': 'phonetic_only_structure_preserved'\n",
    "    }, pt_save_path)\n",
    "    \n",
    "    # 3. ìµœì¢… í…ŒìŠ¤íŠ¸ (CER/WER)\n",
    "    print(\"ğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ ì¤‘...\")\n",
    "    test_results = trainer.evaluate(processed_datasets[\"test\"], metric_key_prefix=\"test\")\n",
    "    \n",
    "    # 4. ê²°ê³¼ ì €ì¥\n",
    "    final_stats = {\n",
    "        \"training_type\": \"phonetic_only_structure_preserved\",\n",
    "        \"total_samples\": len(dataset),\n",
    "        \"learning_rate\": training_args.learning_rate,\n",
    "        \"max_steps\": training_args.max_steps,\n",
    "        \"final_test_cer\": test_results.get(\"test_cer\", \"N/A\"),\n",
    "        \"final_test_wer\": test_results.get(\"test_wer\", \"N/A\"),\n",
    "        \"model_path\": output_dir,\n",
    "        \"pt_path\": pt_save_path\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"phonetic_training_results.json\"), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(final_stats, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"âœ… ë°œìŒ ì „ìš© í•™ìŠµ ì™„ë£Œ!\")\n",
    "    print(f\"ğŸ“Š ìµœì¢… ê²°ê³¼:\")\n",
    "    print(f\"   CER: {final_stats['final_test_cer']}\")\n",
    "    print(f\"   WER: {final_stats['final_test_wer']}\")\n",
    "    print(f\"   HF ëª¨ë¸: {output_dir}\")\n",
    "    print(f\"   PT íŒŒì¼: {pt_save_path}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "def load_phonetic_model(pt_path=\"whisper_phonetic_model.pt\"):\n",
    "    \"\"\"ì €ì¥ëœ PT íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ\"\"\"\n",
    "    print(f\"ğŸ“ PT íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ: {pt_path}\")\n",
    "    \n",
    "    if not os.path.exists(pt_path):\n",
    "        print(f\"âŒ PT íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pt_path}\")\n",
    "        return None, None\n",
    "    \n",
    "    # PT íŒŒì¼ ë¡œë“œ\n",
    "    checkpoint = torch.load(pt_path, map_location='cpu')\n",
    "    \n",
    "    # ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
    "    processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "    \n",
    "    # ìƒíƒœ ë¡œë“œ\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    print(\"âœ… PT íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    print(f\"   í›ˆë ¨ íƒ€ì…: {checkpoint.get('training_type', 'unknown')}\")\n",
    "    print(f\"   í•™ìŠµë¥ : {checkpoint.get('training_args', {}).get('learning_rate', 'unknown')}\")\n",
    "    \n",
    "    return model, processor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ğŸ¯ 100% ë°œìŒ í…ìŠ¤íŠ¸ Whisper í•™ìŠµ (êµ¬ì¡° ë³´ì¡´ + CER/WER)\")\n",
    "    \n",
    "    # ì‹œë“œ ì„¤ì •\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # í•™ìŠµ ì‹¤í–‰\n",
    "    model, processor = train_phonetic_whisper()\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"âŒ í•™ìŠµ ì‹¤íŒ¨\")\n",
    "    else:\n",
    "        print(\"âœ… í•™ìŠµ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13d43d-b30f-4aa2-a0b4-a4c328465f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
